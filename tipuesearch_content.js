var tipuesearch = {"pages":[{"title":"Docker Python Mysql","text":"docker pull mysql:5.7 docker run -itd -p 8070:3306 -v /hadoop-data/work/sl/project/mysql/data:/var/lib/mysql -v /hadoop-data/work/sl/project/mysql/conf.d:/etc/mysql/conf.d --name=qa_mysql -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 mysql -h host_ip -P port -u root -p123456 show databases ; show tables from database_name ; # use database_name; show tables; show columns from table_name ; # desc table_name; seletc * from table_name limit n ; # head n row data select * from table_name where id = 0 ; # conditional search select count ( id ) from table_name ; # data total delete from table_name ; # delete table data 1. pymysql.err.DataError: (1366, \"Incorrect string value: ‘\\xE5\\xA4\\xA9\\xE6\\xB9\\x96…' for column ‘question' at row 1\") 原因：由于建表的时候没有指定数据库字符集, 保存中文的时候就会报错：pymysql.err.InternalError: (1366, …) mysql>ALTER TABLE your_table CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; or mysql>alter table 表名 convert to character set utf8mb4; 2.(2006, \"MySQL server has gone away (BrokenPipeError(32, ‘Broken pipe'))\"); 默认超8小时，连接自动断开 def _is_alive ( self ) : \"\"\"检查连接是否失活\"\"\" try : self . conn . ping ( reconnect = True ) except : self . conn = pymysql . connect ( host = MYSQL_HOST , user = MYSQL_USER , port = MYSQL_PORT , password = MYSQL_PWD , database = MYSQL_DB , charset = 'utf8mb4' , local_infile = True ) self . cursor = self . conn . cursor ()","tags":"Programming","url":"articles/Docker-Python-Mysql.html","loc":"articles/Docker-Python-Mysql.html"},{"title":"Docker build","text":"问题描述： >>> docker build - t image_name : v1 . Sending build context to Docker daemon 2.229 MB Step 1 / 4 : FROM nvidia / cuda : 10.0 - cudnn7 - runtime - centos7 ---> Running in 7 d171c998c6a Removing intermediate container 7 d171c998c6a ---> 494 b7b9cbb47 Step 3 / 4 : RUN yum - y install vim ---> Running in 1129 f4d0c210 Loaded plugins : fastestmirror , ovl , versionlock Determining fastest mirrors * base : mirrors . cn99 . com * extras : mirrors . cn99 . com * updates : mirrors . cn99 . com http : // mirrors . cn99 . com / centos / 7.9 . 2009 / os / x86_64 / repodata / repomd . xml : [ Errno 12 ] Timeout on http : // mirrors . cn99 . com / centos / 7.9 . 2009 / os / x86_64 / repodata / repomd . xml : ( 28 , 'Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds' ) Trying other mirror . https : // developer . download . nvidia . com / compute / cuda / repos / rhel7 / x86_64 / repodata / repomd . xml : [ Errno 12 ] Timeout on https : // developer . download . nvidia . com / compute / cuda / repos / rhel7 / x86_64 / repodata / repomd . xml : ( 28 , 'Operation timed out after 30001 milliseconds with 0 out of 0 bytes received' ) Trying other mirror . https : // developer . download . nvidia . com / compute / cuda / repos / rhel7 / x86_64 / repodata / repomd . xml : [ Errno 12 ] Timeout on https : // developer . download . nvidia . com / compute / cuda / repos / rhel7 / x86_64 / repodata / repomd . xml : ( 28 , 'Operation timed out after 30000 milliseconds with 0 out of 0 bytes received' ) Trying other mirror . 在容器中yum安装工具一切正常，在Dockerfile中构建镜像时，出现网络超时问题。 由于在起容器是 docker run --net=host 解决方案：在制作镜像时设置与主机同一网络 docker build --network=host -t image_name:v1 .","tags":"Tools","url":"articles/Docker-build.html","loc":"articles/Docker-build.html"},{"title":"Docker export vs commit","text":"问题描述： 2021 - 09 - 04 14 : 12 : 08.686281 : I tensorflow / stream_executor / platform / default / dso_loader . cc : 53 ] Could not dlopen library 'libcuda.so.1' ; dlerror : / lib64 / libcuda . so . 1 : file too short 2021 - 09 - 04 14 : 12 : 08.686315 : E tensorflow / stream_executor / cuda / cuda_driver . cc : 318 ] failed call to cuInit : UNKNOWN ERROR ( 303 ) 2021 - 09 - 04 14 : 12 : 08.686354 : I tensorflow / stream_executor / cuda / cuda_diagnostics . cc : 169 ] retrieving CUDA diagnostic information for host : jerry 2021 - 09 - 04 14 : 12 : 08.686366 : I tensorflow / stream_executor / cuda / cuda_diagnostics . cc : 176 ] hostname : jerry 2021 - 09 - 04 14 : 12 : 08.686403 : I tensorflow / stream_executor / cuda / cuda_diagnostics . cc : 200 ] libcuda reported version is : Not found : was unable to find libcuda . so DSO loaded into this program 2021 - 09 - 04 14 : 12 : 08.686568 : I tensorflow / stream_executor / cuda / cuda_diagnostics . cc : 204 ] kernel reported version is : 418.126 . 2 2021 - 09 - 04 14 : 12 : 08.702025 : I tensorflow / core / platform / profile_utils / cpu_utils . cc : 94 ] CPU Frequency : 2400000000 Hz 2021 - 09 - 04 14 : 12 : 08.717327 : I tensorflow / compiler / xla / service / service . cc : 168 ] XLA service 0x3aade70 executing computations on platform Host . Devices : 2021 - 09 - 04 14 : 12 : 08.717385 : I tensorflow / compiler / xla / service / service . cc : 175 ] StreamExecutor device ( 0 ): < undefined > , < undefined > False 基础镜像 nvidia/cuda:10.0-cudnn7-runtime-centos7 起容器配置后，通过命令 docker export 制作镜像，分发使用其容器不能驱动gpu，报错 Could not dlopen library 'libcuda.so.1'; dlerror: /lib64/libcuda.so.1: file too short 解决方案： docker commit . 知识点： docker export vs docker commit","tags":"Tools","url":"articles/Docker-export-commit.html","loc":"articles/Docker-export-commit.html"},{"title":"Docker Build GPU Base Image","text":"https://docs.docker.com/engine/reference/commandline/docker/ 1.基础镜像选择 https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/user-guide.html 选择Nvidia cuda支持gpu： https://hub.docker.com/r/nvidia/cuda/tags?page=1&ordering=last_updated&name=10.0 runtime版：用于服务部署， 大小1.7G；devel版：用于开发，功能完整，大小3G。 docker pull nvidia/cuda:10.0-cudnn7-runtime-centos7 docker pull nvidia/cuda:10.0-cudnn7-devel-centos7 2.yum换源 启动容器：docker run -itd —gpus=all —net=host —name=temp -v /docker_tmp:/server_api 1ba4e7500fa3 /bin/bash 进入容器：docker exec -it temp /bin/bash 备份yum原始的源：mv /etc/yum.repos.d /etc/yum.repos.d.bk 创建yum源目录：mkdir /etc/yum.repos.d 下载阿里云yum源：wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 重建缓存：yum clean all && yum makecache 3.安装python3 安装python环境编译工具 yum -y install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel yum -y install gcc automake autoconf libtool make wget vim 移除centos自带python软连接（软链到python2的） cd /usr/bin && mv python python.bk 下载python3源码 mkdir -p /usr/local/python/python3 cd /usr/local wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tgz tar - xzvf Python-3.6.8.tgz 编译python3源码 cd /usr/local/Python-3.6.8 ./configure —prefix=/usr/local/python/python3 make && make install 直接make && make install会报错：zipimport.ZipImportError: can't decompress data; zlib not available，先如下解决： vim Modules/Setup # 把#zlib zlibmodule.c -I \\((prefix)/include -L\\) (exec_prefix)/lib -lz 注释去掉 cd /usr/local/Python-3.6.8/Modules/zlib && ./configure && make install cd /usr/local/Python-3.6.8 设置软链接 ln -s /usr/local/python/python3/bin/python3 /usr/bin/python 验证安装 python -V 修改yum 注意:由于centos7的yum要使用到python2.7.5的环境，我们要指定yum使用的python的版本，不然使用了3.6.8可能会导致yum命令无法使用，修改头部设置为使用python2.7 vim /usr/bin/yum #!/usr/bin/python -→ #!/usr/bin/python2.7 4.安装pip工具 cd /usr/local wget http://bootstrap.pypa.io/get-pip.py python get-pip.py ln -s /usr/local/python/python3/bin/pip3 /usr/bin/pip pip -V 5.中文支持 echo \"export LC_ALL=en_US. UTF -8\" >> /etc/profile source /etc/profile echo \"export LC_ALL=en_US. UTF -8\" >> ~/.bashrc source ~/.bashrc 6.制作镜像 docker export -o gpu_python3.tar 8020a9753211(容器号) docker import gpu_python3.tar nvidia/cuda:1.9.0-cuda10.2-cudnn7-runtime-python3 docker tag nvidia/cuda:1.9.0-cuda10.2-cudnn7-runtime-python3 image:tag","tags":"Tools","url":"articles/Docker-Build-GPU-Base-Image.html","loc":"articles/Docker-Build-GPU-Base-Image.html"},{"title":"Sliding Window","text":"from typing import List 滑动窗口均值 class Solution : def meanSlidingWindow ( self , nums : List [ int ], k : int ) -> List [ int ]: \"\"\"暴力解法O(N*K) 时间复杂度分析：由于每一个输入数组的元素，我们都需要计算它下K个元素的均值，所以时间复杂度为O(N*K)，N是输入数组元素个数。 这种算法效率低下的原因是，窗口每移动一步，总是有K-1个元素被重复计算了两次。overlapping elements: K-1 \"\"\" result = [] for i in range ( len ( nums ) - k + 1 ): window_sum = 0. for j in range ( i , i + k ): window_sum += nums [ j ] result . append ( window_sum / k ) return result def meanSlidingWindow1 ( self , nums : List [ int ], k : int ) -> List [ int ]: \"\"\"暴力解法O(N*K) \"\"\" result = [] window_sum = 0. for i in range ( k ): window_sum += nums [ i ] result . append ( window_sum / k ) for i in range ( 1 , len ( nums ) - k + 1 ): window_sum = window_sum - nums [ i - 1 ] + nums [ i + k - 1 ] result . append ( window_sum / k ) return result solution = Solution () print ( solution . meanSlidingWindow ( nums = [ 1 , 3 , 2 , 6 , - 1 , 4 , 1 , 8 , 2 ], k = 5 )) print ( solution . meanSlidingWindow1 ( nums = [ 1 , 3 , 2 , 6 , - 1 , 4 , 1 , 8 , 2 ], k = 5 )) [2.2, 2.8, 2.4, 3.6, 2.8] [2.2, 2.8, 2.4, 3.6, 2.8] 239. 滑动窗口最大值 class Solution : def maxSlidingWindow ( self , nums : List [ int ], k : int ) -> List [ int ]: \"\"\"暴力解法O(N*K) 时间复杂度分析：由于每一个输入数组的元素，我们都需要计算它下K个元素中的最大值，所以时间复杂度为O(N*K)，N是输入数组元素个数。 这种算法效率低下的原因是，窗口每移动一步，总是有K-1个元素被重复计算了两次。overlapping elements: K-1 \"\"\" result = [] for i in range ( len ( nums ) - k + 1 ): max_ = - float ( 'inf' ) for j in range ( i , i + k ): max_ = max ( nums [ j ], max_ ) result . append ( max_ ) return result solution = Solution () solution . maxSlidingWindow ( nums = [ 1 , 3 , - 1 , - 3 , 5 , 3 , 6 , 7 ], k = 3 ) [3, 3, 5, 5, 6, 7] ! jupyter nbconvert -- to markdown 2021 - 02 - 24 - Sliding - Window . ipynb","tags":"Algorithms","url":"articles/Sliding-Window.html","loc":"articles/Sliding-Window.html"},{"title":"Viterbi Algorithm","text":"矩阵法numpy 注：转移矩阵和发射矩阵以列维度上形式表示概率分布的，即列维度axis=1上元素和为1。 注意：把隐状态概率分布表示为向量后，向量乘以状态转移矩阵的结果为新的隐状态。 同理，对发射矩阵做向量矩阵乘法会得到新的观测状态。 import numpy as np from typing import List , Optional , Tuple def step ( score_prev : np . ndarray , emission_probs : np . ndarray , transition_probs : np . ndarray , observed_state : int ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"运行维特比算法一个时间步。 Args: score_prev: probability distribution with shape (num_hidden), the previous score emission_probs: the emission probability matrix (num_hidden, num_observed) transition_probs: the transition probability matrix, with shape (num_hidden, num_hidden) observed_state: the observed state at the current step Returns: - the score for the next step - the maximizing previous state, before the current state, as an int array with shape (num_hidden) \"\"\" pre_max = score_prev * transition_probs . T # 乘以各个隐状态转移概率下的分数值（矩阵乘法并非点积，向量广播成矩阵后对应元素相乘） max_prev_states = np . argmax ( pre_max , axis = 1 ) # 来自前一时间步的哪个隐状态。即对应前一时间步隐状态位置 max_vals = pre_max [ np . arange ( len ( max_prev_states )), max_prev_states ] # 根据最大值索引取出最大值 score_new = max_vals * emission_probs [:, observed_state ] # 发射概率列向量：该观测状态对应的隐状态列 return score_new , max_prev_states def viterbi ( emission_probs : np . ndarray , transition_probs : np . array , start_probs : np . ndarray , observed_states : List [ int ]) -> Tuple [ List [ int ], float ]: \"\"\"运行维特比算法获得最有可能的状态序列。 Args: emission_probs: transition_probs: start_probs: observed_states: Returns: - 最有可能的状态序列。 - 状态和观测序列的联合概率值。 \"\"\" # 运行正向传递，存储最有可能的先前状态。 score = start_probs * emission_probs [:, observed_states [ 0 ]] # 第一个时间步的最优解向量（隐含状态向量维度） all_pre_states = [] for observed_state in observed_states [ 1 :]: score , prevs = step ( score , emission_probs , transition_probs , observed_state ) all_pre_states . append ( prevs ) # 回溯 state = np . argmax ( score ) # 最后一个时间步最大分数出现的位置，该值表示来自前一个时间步的第几个状态 sequence_score = score [ state ] # 获取最大分数，即累计最大分数，最大隐状态序列 sequence_state = [ state ] # 最后一个时间步的最优隐状态加入结果序列 for pre_states in all_pre_states [:: - 1 ]: state = pre_states [ state ] # 回溯获取前一个最优隐状态位置 sequence_state . append ( state ) # 加入结果序列 return sequence_state [:: - 1 ], sequence_score num_hidden_states = 3 # vocab_size num_observed_states = 2 # vocab_size num_time_steps = 4 # sequence_length # 初始化转移概率矩阵 transition_probs = np . array ([ [ 0.1 , 0.2 , 0.7 ], [ 0.1 , 0.1 , 0.8 ], [ 0.5 , 0.4 , 0.1 ], ]) assert transition_probs . shape == ( num_hidden_states , num_hidden_states ) assert transition_probs . sum ( axis = 1 ) . mean () == 1.0 # 初始化发射概率矩阵 （行 -> 隐含状态：0，1，2，列 -> 观测状态：0，1） emission_probs = np . array ([ [ 0.1 , 0.9 ], [ 0.3 , 0.7 ], [ 0.5 , 0.5 ], ]) assert emission_probs . shape == ( num_hidden_states , num_observed_states ) assert emission_probs . sum ( axis = 1 ) . mean () == 1.0 # 初始隐状态概率 init_hidden_state_probs = np . array ([ 0.1 , 0.3 , 0.6 ]) assert init_hidden_state_probs . shape == ( num_hidden_states ,) # 定义观测序列 observed_states = [ 1 , 1 , 0 , 1 ] assert len ( observed_states ) == num_time_steps max_seq , seq_prob = viterbi ( emission_probs , transition_probs , init_hidden_state_probs , observed_states , ) max_seq , seq_prob ([2, 0, 2, 0], 0.0212625) init_hidden_state_probs array([0.1, 0.3, 0.6]) emission_probs array([[0.1, 0.9], [0.3, 0.7], [0.5, 0.5]]) score = init_hidden_state_probs * emission_probs [:, 0 ] score array([0.01, 0.09, 0.3 ]) transition_probs . T array([[0.1, 0.1, 0.5], [0.2, 0.1, 0.4], [0.7, 0.8, 0.1]]) pre_max = score * transition_probs . T pre_max array([[0.001, 0.009, 0.15 ], [0.002, 0.009, 0.12 ], [0.007, 0.072, 0.03 ]]) max_prev_states = np . argmax ( pre_max , axis = 1 ) max_prev_states array([2, 2, 1]) np . arange ( len ( max_prev_states )) array([0, 1, 2]) pre_max [ np . arange ( len ( max_prev_states )), max_prev_states ] array([0.15 , 0.12 , 0.072]) step ( score , emission_probs , transition_probs , 1 ) (array([0.135, 0.084, 0.036]), array([2, 2, 1])) 矩阵法pytorch import torch # shape: (1, 5, 3) logits = torch . rand (( 1 , 5 , 3 )) mask = torch . tensor ([[ 1 , 1 , 1 , 0 , 0 ]]) transisition_probs = torch . rand (( 3 , 3 )) batch_size , max_len , n_tags = logits . size () seq_len = mask . long () . sum ( dim = 1 ) seq_len tensor([3]) logits = logits . transpose ( 0 , 1 ) . data print ( logits . shape ) mask = mask . transpose ( 0 , 1 ) . data . eq ( True ) print ( mask . shape ) flip_mask = mask . eq ( False ) torch.Size([5, 1, 3]) torch.Size([5, 1]) vpath = torch . zeros_like ( logits , dtype = torch . int ) print ( vpath . shape ) vscore = logits [ 0 ] # 初始化 print ( vscore ) torch.Size([5, 1, 3]) tensor([[0.3556, 0.4721, 0.6929]]) end_transition_prob = torch . zeros ( 3 ) . view ( 1 , 3 ) . repeat ( batch_size , 1 , 1 ) end_transition_prob tensor([[[0., 0., 0.]]]) def viterbi_decode ( self , logits , mask , unpad = False ): r \"\"\"给定一个特征矩阵以及转移分数矩阵，计算出最佳的路径以及对应的分数 :param torch.FloatTensor logits: batch_size x max_len x num_tags，特征矩阵。 :param torch.ByteTensor mask: batch_size x max_len, 为0的位置认为是pad；如果为None，则认为没有padding。 :param bool unpad: 是否将结果删去padding。False, 返回的是batch_size x max_len的tensor; True，返回的是 List[List[int]], 内部的List[int]为每个sequence的label，已经除去pad部分，即每个List[int]的长度是这 个sample的有效长度。 :return: 返回 (paths, scores)。 paths: 是解码后的路径, 其值参照unpad参数. scores: torch.FloatTensor, size为(batch_size,), 对应每个最优路径的分数。 \"\"\" batch_size , max_len , n_tags = logits . size () seq_len = mask . long () . sum ( 1 ) logits = logits . transpose ( 0 , 1 ) . data # L, B, H mask = mask . transpose ( 0 , 1 ) . data . eq ( True ) # L, B flip_mask = mask . eq ( False ) # dp vpath = logits . new_zeros (( max_len , batch_size , n_tags ), dtype = torch . long ) # torch.zeros_like(logits, dtype=torch.int) vscore = logits [ 0 ] # bsz x n_tags # 初始化 transitions = self . _constrain . data . clone () transitions [: n_tags , : n_tags ] += self . trans_m . data if self . include_start_end_trans : transitions [ n_tags , : n_tags ] += self . start_scores . data transitions [: n_tags , n_tags + 1 ] += self . end_scores . data vscore += transitions [ n_tags , : n_tags ] # add all 0 trans_score = transitions [: n_tags , : n_tags ] . view ( 1 , n_tags , n_tags ) . data # 转移概率矩阵 end_trans_score = transitions [: n_tags , n_tags + 1 ] . view ( 1 , 1 , n_tags ) . repeat ( batch_size , 1 , 1 ) # bsz, 1, n_tags # 针对长度为1的句子, 长度非1的句子则作为初始值 vscore += transitions [: n_tags , n_tags + 1 ] . view ( 1 , n_tags ) . repeat ( batch_size , 1 ) . masked_fill ( seq_len . ne ( 1 ) . view ( - 1 , 1 ), 0 ) for i in range ( 1 , max_len ): prev_score = vscore . view ( batch_size , n_tags , 1 ) cur_score = logits [ i ] . view ( batch_size , 1 , n_tags ) + trans_score # emission_prob: logits (1,1,8) + (1,8,8) score = prev_score + cur_score . masked_fill ( flip_mask [ i ] . view ( batch_size , 1 , 1 ), 0 ) # bsz x n_tag x n_tag # 需要考虑当前位置是该序列的最后一个 score += end_trans_score . masked_fill ( seq_len . ne ( i + 1 ) . view ( - 1 , 1 , 1 ), 0 ) best_score , best_dst = score . max ( 1 ) vpath [ i ] = best_dst # 由于最终是通过last_tags回溯，需要保持每个位置的vscore情况 vscore = best_score . masked_fill ( flip_mask [ i ] . view ( batch_size , 1 ), 0 ) + vscore . masked_fill ( mask [ i ] . view ( batch_size , 1 ), 0 ) # 上面masked_fill均是解决mask为0情况 # backtrace batch_idx = torch . arange ( batch_size , dtype = torch . long , device = logits . device ) seq_idx = torch . arange ( max_len , dtype = torch . long , device = logits . device ) lens = ( seq_len - 1 ) # idxes [L, B], batched idx from seq_len-1 to 0 idxes = ( lens . view ( 1 , - 1 ) - seq_idx . view ( - 1 , 1 )) % max_len ans = logits . new_empty (( max_len , batch_size ), dtype = torch . long ) ans_score , last_tags = vscore . max ( 1 ) # 最优路径分数，和最后一个时间步的最优隐状态 ans [ idxes [ 0 ], batch_idx ] = last_tags for i in range ( max_len - 1 ): last_tags = vpath [ idxes [ i ], batch_idx , last_tags ] ans [ idxes [ i + 1 ], batch_idx ] = last_tags ans = ans . transpose ( 0 , 1 ) if unpad : paths = [] for idx , max_len in enumerate ( lens ): paths . append ( ans [ idx , : max_len + 1 ] . tolist ()) else : paths = ans return paths , ans_scor 迭代法 pre_max array([[0.001, 0.002, 0.007, 0.009, 0.009, 0.072, 0.15 , 0.12 , 0.03 ]]) a = pre_max . reshape ( 3 , 3 ) a array([[0.001, 0.002, 0.007], [0.009, 0.009, 0.072], [0.15 , 0.12 , 0.03 ]]) a [ 0 , 0 ] = 99 b = pre_max . reshape ( 3 , 3 ) b array([[0.001, 0.002, 0.007], [0.009, 0.009, 0.072], [0.15 , 0.12 , 0.03 ]]) a = b . resize ( 1 , 9 ) a a b array([[0.001, 0.002, 0.007, 0.009, 0.009, 0.072, 0.15 , 0.12 , 0.03 ]])","tags":"Algorithms","url":"articles/Viterbi-Algorithm.html","loc":"articles/Viterbi-Algorithm.html"},{"title":"Google-android","text":"ADB Go谷歌安装器 GMS安装器 不建议采用以上一键安装 疑难问题 ADB adb version adb devices 查看所有应用列表：adb shell pm list packages 查看系统应用列表：adb shell pm list packages -s # com.google.android.gms，com.google.android.gsf，com.google.android.partnersetup， # com.google.android.backuptransport，com.google.android.onetimeinitializer 删除应用：adb shell pm uninstall --user 0 com.baidu.input_huawei Go谷歌安装器 注意Go谷歌安装器4.8.4只支持安卓7.0、7.1、8.0、9.0，尝试过安卓8.1总是失败！ GMS安装器 不建议采用以上一键安装 优点方便，缺点默认装的谷歌三件套：谷歌框架，谷歌play服务，谷歌play商城存在与手机安卓系统不匹配，建议单独下载安装。 https://www.apkmirror.com/ apkpure.com https://www.olocat.cn/?p=45 疑难问题 google play登陆核验信息黑屏退出 @ 所有 google 内建包 @ adb shell pm list packages @ adb shell pm list packages - s adb shell pm uninstall -- user 0 com . google . android . gms adb shell pm uninstall -- user 0 com . google . android . gsf adb shell pm uninstall -- user 0 com . google . android . backuptransport adb shell pm uninstall -- user 0 com . google . android . onetimeinitializer adb shell pm uninstall -- user 0 com . android . vending @ google play adb shell pm uninstall -- user 0 com . google . android . gms . policy_sidecar_aps adb shell pm uninstall -- user 0 com . google . android . printservice . recommendation adb shell pm uninstall -- user 0 com . google . android . overlay . contactproviderconfig adb shell pm uninstall -- user 0 com . google . android . partnersetup adb shell pm uninstall -- user 0 com . google . android . webview adb shell pm uninstall -- user 0 com . google . android . overlay . settingsConfig adb shell pm uninstall -- user 0 com . google . android . syncadapters . contacts adb shell pm uninstall -- user 0 com . google . android . ext . services adb shell pm uninstall -- user 0 com . google . android . overlay . gmsconfig adb shell pm uninstall -- user 0 com . google . android . configupdater adb shell pm uninstall -- user 0 com . google . android . marvin . talkback adb shell pm uninstall -- user 0 com . google . android . ext . shared adb shell pm uninstall -- user 0 com . google . ar . core adb shell pm uninstall -- user 0 com . google . android . gsf . login adb shell pm uninstall -- user 0 com . google . android . overlay . settingsProvider adb shell pm list packages pause","tags":"Tools","url":"articles/Google-Android.html","loc":"articles/Google-Android.html"},{"title":"Convert pytorch checkpoint to tensorflow2","text":"https://github.com/huggingface/transformers/issues/6124 from transformers import TFBertModel model = TFBertModel . from_pretrained ( \"./rubert-base-cased-pt\" , from_pt = True ) model . save ( \"./rubert-base-cased\" ) # this adds a TF model file (tf_model.h5) to your directory","tags":"NLP","url":"articles/Convert-pytorch-model-to-tensorflow2.html","loc":"articles/Convert-pytorch-model-to-tensorflow2.html"},{"title":"CMRC Metric","text":"F1 # find longest common string def find_lcs ( s1 , s2 ): m = [[ 0 for i in range ( len ( s2 ) + 1 )] for j in range ( len ( s1 ) + 1 )] mmax = 0 p = 0 for i in range ( len ( s1 )): for j in range ( len ( s2 )): if s1 [ i ] == s2 [ j ]: m [ i + 1 ][ j + 1 ] = m [ i ][ j ] + 1 if m [ i + 1 ][ j + 1 ] > mmax : mmax = m [ i + 1 ][ j + 1 ] p = i + 1 return s1 [ p - mmax : p ], mmax def calc_f1_score ( answers , prediction ): f1_scores = [] for ans in answers : ans_segs = mixed_segmentation ( ans , rm_punc = True ) prediction_segs = mixed_segmentation ( prediction , rm_punc = True ) lcs , lcs_len = find_lcs ( ans_segs , prediction_segs ) if lcs_len == 0 : f1_scores . append ( 0 ) continue precision = 1.0 * lcs_len / len ( prediction_segs ) recall = 1.0 * lcs_len / len ( ans_segs ) f1 = ( 2 * precision * recall ) / ( precision + recall ) f1_scores . append ( f1 ) return max ( f1_scores ) EM # remove punctuation def remove_punctuation ( in_str ): in_str = in_str . lower () . strip () sp_char = [ '-' , ':' , '_' , '*' , '&#94;' , '/' , ' \\\\ ' , '~' , '`' , '+' , '=' , '，' , '。' , '：' , '？' , '！' , '\"' , '\"' , '；' , ''' , '《' , '》' , '……' , '·' , '、' , '「' , '」' , '（' , '）' , '－' , '～' , '『' , '』' ] out_segs = [] for char in in_str : if char in sp_char : continue else : out_segs . append ( char ) return '' . join ( out_segs ) def calc_em_score ( answers , prediction ): em = 0 for ans in answers : ans_ = remove_punctuation ( ans ) prediction_ = remove_punctuation ( prediction ) if ans_ == prediction_ : em = 1 break return em","tags":"NLP","url":"articles/CMRC-Metric.html","loc":"articles/CMRC-Metric.html"},{"title":"JDMDC2020","text":"2020京东多模态对话JDMDC2020第二名解决方案 数据工作 数据集探索 数据预处理 训练数据构造 融合多模态与知识库 技术方案 基线模型MHRED 最佳模型VLGPT 总结与引用 2020京东多模态对话JDMDC2020第二名解决方案 CCL2020 : 中国计算语言学大会（ CCL 2020）技术评测结果 JDMDC2020 RANK ： https://jddc.jd.com/rank Code: JDMDC2020 -Solution-2nd Hibot团队成绩：初赛第三，决赛第二 Hibot队员：林克，茸茸，阿布，杰瑞 数据工作 数据集探索 数据分为五个字段：会话ID, 店铺类别， 商品ID， 对话文本（包含图片ID）， 对话角色。通过商品ID与图片ID，来引用商品知识库数据和图片模态数据。 统计原始会话（连续QQAA未合并）轮数长度以及单句文本长度（按字）。 数据预处理 训练数据构造 融合多模态与知识库 技术方案 基线模型MHRED 官方提供基线生成模型MHRED：https://github.com/jd-aig/nlp_baai/tree/master/jddc2020_baseline/mhred/pytorch 基线模型MHRED复现BLEU分为：3.3853，在基线的基础上，我们加入了注意力机制BLEU提分到：5.6237。 最佳模型VLGPT 总结与引用 What did work 去除上下文为空的数据 去除无意义回复，例如好的，嗯嗯，哦… 根据数据集自定义字典，OOV问题 带掩码的Loss，masked answer labels 增加token type embedding区分角色 Base基础GPT模型 各种注意力机制Self-Attention, Context-Attention, Masked SeLf-Attention[5][12] What didn't work 删除通用模板回复 Label Smoothing提升微弱 [13] 多头任务学习DoubleHead GPT (生成任务+是否为下一句) [14] BERT中文wiki预训练模型 [15] GPT中文wiki[16]与清华开源预训练模型 [17][18] Medium中型GPT模型 最大互信息Maximum Mutual Information( MMI ) [19] [1] Kishore Papineni, Salim Roukos, et al. BLEU : a Method for Automatic Evaluation of Machine Translation [2] Boxing Chen, Colin Cherry et al. A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU [3] Amrita Saha, Mitesh Khapra, et al. Towards Building Large Scale Multimodal Domain-Aware Conversation Systems [4] Minh-Thang Luong, Hieu Pham, et al. Effective Approaches to Attention-based Neural Machine Translation [5] Ashish Vaswani, Noam Shazeer, et al. Attention Is All You Need [6] Jacob Devlin, Ming-Wei Chang, et al. BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding [7] Alec Radford, Karthik Narasimhan, et al. Improving Language Understanding by Generative Pre-Training [8] Alec Radford, Jeffrey Wu, et al. Language Models are Unsupervised Multitask Learners [9] https://huggingface.co - transformers [10] Thomas Wolf, Victor Sanh, et al. TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents [11] https://github.com/huggingface/transfer-learning-conv-ai [12] https://jalammar.github.io/illustrated-transformer [13] Rafael Müller, Simon Kornblith, st al. when does label smoothing help? [14] https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel [15] https://huggingface.co/bert-base-chinese [16] https://github.com/qingkongzhiqian/ GPT2 -Summary [17] https://cloud.tsinghua.edu.cn/f/4dfb8c6c22ae47fbbe98 [18] Yida Wang, Pei Ke, et al. A Large-Scale Chinese Short-Text Conversation Dataset [19] Yizhe Zhang, Siqi Sun, et al. DialoGPT:Large-Scale Generative Pre-training for Conversational Response Generation","tags":"NLP","url":"articles/JDMDC2020.html","loc":"articles/JDMDC2020.html"},{"title":"Deploy model in production","text":"Install wsgi using code Install wsgi using pip Load wsgi module in Ubuntu How do I use a conda environment with mod_wsgi? Install wsgi using code git clone https://github.com/GrahamDumpleton/mod_wsgi apt-get install apache2-dev apt-get install python-dev cd mod_wsgi/ ./configure make make install Install wsgi using pip apt-get install libapache2-mod-wsgi-py3 instead of libapache2-mod-wsgi for python3 mod_wsgi install Load wsgi module in Ubuntu How do I use a conda environment with mod_wsgi? How do I use a conda environment with mod_wsgi? ubuntu+apache+mod_wsgi+flask mod_wsgi deploy https://github.com/ahkarami/Deep-Learning-in-Production https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html","tags":"Tools","url":"articles/Deploy-model-in-production.html","loc":"articles/Deploy-model-in-production.html"},{"title":"NLP Resources","text":"Paper Project Paper Effective Approaches to Attention-based Neural Machine Translation Attention is All You Need BART : Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension RASA : Dialogue Transformers Project Transformers Microsoft NLP best practices: Best practices and examples on NLP . RASA","tags":"NLP","url":"articles/NLP-Resources.html","loc":"articles/NLP-Resources.html"},{"title":"Pytorch Transformer","text":"pytorch transformer illustrated transformer import math import torch import torch.nn as nn from torch.nn import TransformerEncoder , TransformerEncoderLayer , TransformerDecoder , TransformerDecoderLayer from layers.encoder import CustomEmbedding import layers class PositionalEncoding ( nn . Module ): \"\"\" Args: d_model: the number of expected features in the input (required). dropout: the dropout value (default=0.1). \"\"\" def __init__ ( self , d_model , dropout = 0.1 , max_len = 512 ): super ( PositionalEncoding , self ) . __init__ () self . dropout = nn . Dropout ( p = dropout ) pe = torch . zeros ( max_len , d_model ) position = torch . arange ( 0 , max_len , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) . float () * ( - math . log ( 10000.0 ) / d_model )) pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) pe = pe . unsqueeze ( 0 ) . transpose ( 0 , 1 ) self . register_buffer ( 'pe' , pe ) def forward ( self , x ): x = x + self . pe [: x . size ( 0 ), :] return self . dropout ( x ) class TransformerModel ( nn . Module ): \"\"\" Args: vocab_size: # the size of vocabulary. embed_size: embedding dimension, the number of expected features in the input nhead: the number of heads in the multiheadattention models. dim_feedforward: the dimension of the feedforward network model in nn.TransformerEncoder. nlayers: the number of sub-decoder-layers in the decoder (default=6). dropout: the dropout value (default=0.1). \"\"\" def __init__ ( self , config ): super ( TransformerModel , self ) . __init__ () self . config = config self . pos_encoder = PositionalEncoding ( config . embed_size , config . dropout , config . max_len ) self . pos_decoder = PositionalEncoding ( config . embed_size , config . dropout , config . max_len ) self . src_embedding = CustomEmbedding ( config . vocab_size , config . embed_size ) self . tgt_embedding = nn . Embedding ( config . vocab_size , config . embed_size ) # encode images self . image_encoder = layers . ImageEncoder ( config . embedding_size ) # encoder encoder_layers = TransformerEncoderLayer ( config . embed_size , config . nhead , config . dim_feedforward , config . dropout ) self . transformer_encoder = TransformerEncoder ( encoder_layers , config . nlayers ) # decoder decoder_layers = TransformerDecoderLayer ( config . embed_size , config . nhead , config . dim_feedforward , config . dropout ) self . transformer_decoder = TransformerDecoder ( decoder_layers , config . nlayers ) self . linear = nn . Linear ( config . embed_size , config . vocab_size ) self . init_weights () def _attn_padding_mask ( self , seq ): \"\"\" seq_q: [batch_size, seq_len] seq_k: [batch_size, seq_len] seq_len could be src_len or it could be tgt_len seq_len in seq_q and seq_len in seq_k maybe not equal \"\"\" # eq(zero) is PAD token return seq . data . eq ( 0 ) # [batch_size, 1, len_k], True is masked #return pad_attn_mask.expand(batch_size, len_q, len_k) # [batch_size, len_q, len_k] def _sequence_mask ( self , seq ): \"\"\" Along with the input sequence, a square attention mask is required because the self-attention layers in nn. TransformerEncoder are only allowed to attend the earlier positions in the sequence. For the language modeling task, any tokens on the future positions should be masked. To have the actual words, the output of nn. TransformerEncoder model is sent to the final Linear layer, which is followed by a log-Softmax function. \"\"\" batch_size , seq_len = seq . size () mask = ( torch . triu ( torch . ones ( seq_len , seq_len )) == 1 ) . transpose ( 0 , 1 ) . to ( seq . device ) mask = mask . float () . masked_fill ( mask == 0 , float ( '-inf' )) . masked_fill ( mask == 1 , float ( 0.0 )) return mask def init_weights ( self ): initrange = 0.1 self . src_embedding . embedding . weight . data . uniform_ ( - initrange , initrange ) self . tgt_embedding . weight . data . uniform_ ( - initrange , initrange ) def forward ( self , src , input_sentence_length , input_conversation_length , tgt , input_images , input_images_length , input_image_indexes ): # encode images img_encoder_outputs = self . image_encoder ( input_images ) # encoder src_padding_mask = self . _attn_padding_mask ( src ) src_embed = self . src_embedding ( src , img_encoder_outputs , input_images_length , input_image_indexes ) * math . sqrt ( self . config . embed_size ) src_embed = self . pos_encoder ( src_embed ) . transpose ( 0 , 1 ) # Shape must be [Len, Batch, Embed] for nn.TransformerEncoderLayer. memory = self . transformer_encoder ( src = src_embed , src_key_padding_mask = src_padding_mask ) # decoder tgt_padding_mask = self . _attn_padding_mask ( tgt ) tgt_sequence_mask = self . _sequence_mask ( tgt ) tgt_embed = self . tgt_embedding ( tgt ) * math . sqrt ( self . config . embed_size ) tgt_embed = self . pos_decoder ( tgt_embed ) . transpose ( 0 , 1 ) output = self . transformer_decoder ( tgt = tgt_embed , memory = memory , tgt_mask = tgt_sequence_mask , tgt_key_padding_mask = tgt_padding_mask , memory_key_padding_mask = src_padding_mask ) output = self . linear ( output ) return output . transpose ( 0 , 1 ) . contiguous ()","tags":"Programming","url":"articles/Pytorch-Transformer.html","loc":"articles/Pytorch-Transformer.html"},{"title":"Pytorch contiguous","text":"Pytorch contiguous","tags":"Programming","url":"articles/Pytorch-contiguous.html","loc":"articles/Pytorch-contiguous.html"},{"title":"【 NLP 】Teacher Forcing","text":"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks Teacher-Forcing 技术在训练前期的确是能够很大的加速模型收敛的： 模型在训练过程中的每一个时间步steps，有p的概率选择使用target，有1-p的概率选择使用predict。 模型在训练前期，p应该尽可能的大，这样能够加速收敛；而在快要结束训练的时候，p尽可能的小，让模型在 Autoregressive 的方案中尽可能的修复自身生成的错误。 更确切的，这个p概率可以随着训练的Steps or Epoch 进行衰减，而衰减的方式也可以分为：Exponential Decay, Inverse Sigmoid decay 和 Linear decay 三种方式。 基于pytorch实现Linear decay： parser . add_argument ( '--ss_used' , type = str2bool , default = True ) parser . add_argument ( '--ss_start' , type = float , default = 1.0 ) parser . add_argument ( '--ss_decay' , type = float , default = 0.005 ) parser . add_argument ( '--ss_min' , type = float , default = 0.9 ) # train for epoch_i in range ( self . epoch_i , self . config . n_epoch ): if self . config . ss_used and self . config . ss_start > self . config . ss_min : self . config . ss_start = self . config . ss_start - self . config . ss_decay * epoch_i # decode def decode ( self , out ): # Sample next word from multinomial word distribution if self . sample : x = torch . multinomial ( self . softmax ( out / self . temperature ), 1 ) . view ( - 1 ) # Greedy sampling else : _ , x = out . max ( dim = 1 ) return x for i in range ( seq_len ): out , h = self . forward_step ( x , h ) out_list . append ( out ) if config . ss_used and random . random () > config . ss_start : x = self . decode ( out ) # predict val else : x = inputs [:, i ] # ground true val","tags":"NLP","url":"articles/【NLP】Teacher-Forcing.html","loc":"articles/【NLP】Teacher-Forcing.html"},{"title":"Pytorch distributed train","text":"主卡线程暴涨 共享内存问题 lamda对象不能序列化问题 加载分布式模型到单卡 主卡线程暴涨 异常： 正常： def to_var ( x , on_cpu = False , gpu_id = None ): \"\"\"Tensor => Variable\"\"\" if torch . cuda . is_available () and not on_cpu : x = x . cuda ( gpu_id , non_blocking = True ) # x = Variable(x) return x def normal_kl_div ( mu1 , var1 , mu2 = to_var ( torch . FloatTensor ([ 0.0 ])), var2 = to_var ( torch . FloatTensor ([ 1.0 ]))): one = to_var ( torch . FloatTensor ([ 1.0 ])) return torch . sum ( 0.5 * ( torch . log ( var2 ) - torch . log ( var1 ) 多线程脚本导入时，函数参数总是执行to_var()。当线程num_workers越多，数据无效装入cuda就越多。 修改 def normal_kl_div ( mu1 , var1 , mu2 , var2 ): mu2 = to_var ( torch . FloatTensor ([ 0.0 ])) var2 = to_var ( torch . FloatTensor ([ 1.0 ])) one = to_var ( torch . FloatTensor ([ 1.0 ])) return torch . sum ( 0.5 * ( torch . log ( var2 ) - torch . log ( var1 ) 共享内存问题 Training Start ! 0 %| | 0 / 40937 [ 00 : 00 <? , ? it / s ] ERROR : Unexpected bus error encountered in worker . This might be caused by insufficient shared memory ( shm ) . Traceback ( most recent call last ) : File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\" , line 236 , in _feed obj = _ForkingPickler . dumps ( obj ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/reduction.py\" , line 51 , in dumps cls ( buf , protocol ) . dump ( obj ) fd , size = storage . _share_fd_ () RuntimeError : unable to write to file </ torch_3531_3962650647 > RuntimeError : DataLoader worker ( pid 3527 ) is killed by signal : Bus error . Please note that PyTorch uses shared memory to share data between processes , so if torch multiprocessing is used ( e . g . for multithreaded data loaders ) the default shared memory segment size that container runs with is not enough , and you should increase shared memory size either with -- ipc = host or -- shm - size command line options to nvidia - docker run . 多进程数据加载Dataloader，Docker容器的共享内存/dev/shm不足 修改当前Docker的shm-size。挂载点/dev/shm - docker ps - a - docker inspect [ container id ] | grep Id - systemctl stop docker - cd [ container directory ] - 修改hostconfig . json和config . v2 . json - systemctl start docker - docker start [ container id ] num_workers设置0 dataloader = torch . utils . data . DataLoader ( dataset , batch_size = 16 , shuffle = True , num_workers = 0 , pin_memory = True , collate_fn = dataset . collate_fn ) lamda对象不能序列化问题 -- Process 0 terminated with the following error : Traceback ( most recent call last ): File \"/root/anaconda3/envs/torch/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\" , line 19 , in _wrap fn ( i , * args ) File \"/home/yckj2939/project/yckj_project/mhred/pytorch/train.py\" , line 107 , in main_worker model . train ( train_sampler , train_data_loader , eval_data_loader ) File \"/home/yckj2939/project/yckj_project/mhred/pytorch/utils/time_track.py\" , line 18 , in timed result = method ( * args , ** kwargs ) File \"/home/yckj2939/project/yckj_project/mhred/pytorch/solver.py\" , line 160 , in train for batch_i , ( conversations , conversation_length , sentence_length , images , images_length ) in enumerate ( tqdm ( train_data_loader , ncols = 80 )): File \"/root/anaconda3/envs/torch/lib/python3.7/site-packages/tqdm/_tqdm.py\" , line 979 , in __iter__ for obj in iterable : File \"/root/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\" , line 278 , in __iter__ return _MultiProcessingDataLoaderIter ( self ) File \"/root/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\" , line 682 , in __init__ w . start () File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/process.py\" , line 112 , in start self . _popen = self . _Popen ( self ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/context.py\" , line 223 , in _Popen return _default_context . get_context () . Process . _Popen ( process_obj ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/context.py\" , line 284 , in _Popen return Popen ( process_obj ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/popen_spawn_posix.py\" , line 32 , in __init__ super () . __init__ ( process_obj ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/popen_fork.py\" , line 20 , in __init__ self . _launch ( process_obj ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/popen_spawn_posix.py\" , line 47 , in _launch reduction . dump ( process_obj , fp ) File \"/root/anaconda3/envs/torch/lib/python3.7/multiprocessing/reduction.py\" , line 60 , in dump ForkingPickler ( file , protocol ) . dump ( obj ) AttributeError : Can 't pickle local object ' Vocab . load .< locals >.< lambda > ' def load ( self , word2id_path = None , id2word_path = None ): if word2id_path : with open ( word2id_path , 'rb' ) as f : word2id = pickle . load ( f ) # Can't pickle lambda function self . word2id = defaultdict ( lambda : UNK_ID ) self . word2id . update ( word2id ) self . vocab_size = len ( self . word2id ) pickle模块不能序列化lambda，需要自定义函数 from collections import defaultdict UNK = 1 dic = defaultdict ( lambda : UNK ) print ( dic [ 'Jerry' ]) # res: 1 ---> UNK ################################################## class Test : def default_unk ( self ): return UNK def update ( self ): self . w2i = defaultdict ( self . default_unk ) return self . w2i test = Test () dic = test . update () print ( dic [ 'Annie' ]) # res: 1 ---> UNK luly.lamost.org/blog/python_multiprocessing 加载分布式模型到单卡 RuntimeError : Error ( s ) in loading state_dict for MHRED : Missing key ( s ) in state_dict : \"encoder.embedding.weight\" , ... Unexpected key ( s ) in state_dict : \"module.encoder.embedding.weight\" , ... Distributed包装的模型在保存时，权值参数前面会带有module字符，然而自己在单卡环境下，没有用Distributed包装的模型权值参数不带module。 方案一：保存模型时把module去掉 if len ( gpu_ids ) > 1 : t . save ( net . module . state_dict (), \"model.pth\" ) else : t . save ( net . state_dict (), \"model.pth\" ) 方案二： 创建新的模型OrderedDict不包含module loc = 'cuda: {} ' . format ( self . config . gpu ) checkpoint = torch . load ( checkpoint_path , map_location = loc ) checkpoint_new = OrderedDict () for key , value in checkpoint . items (): key = key [ 7 :] # remove `module.` checkpoint_new [ key ] = value self . model . load_state_dict ( checkpoint_new ) when loading the module, you need to provide an appropriate map_location argument to prevent a process to step into others' devices. If map_location is missing, torch.load will first load the module to CPU and then copy each parameter to where it was saved, which would result in all processes on the same machine using the same set of devices https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints","tags":"Programming","url":"articles/Pytorch-distributed-train.html","loc":"articles/Pytorch-distributed-train.html"},{"title":"Iterator","text":"Iterator Generator Coroutine Iterator iter(): 接收的是可迭代对象，返回的是迭代器。 next(): 接收的是迭代器，调用的是迭代器对象中的next函数，返回数据元素。 由于生成器是一种特殊的迭代器，故而用next()而不是iter()。 class Classmate ( object ): def __init__ ( self ): self . names = list () def add ( self , name ): self . names . append ( name ) def __iter__ ( self ): # 返回迭代器对象 return ClassIterator ( self ) class ClassIterator ( object ): def __init__ ( self , obj ): self . obj = obj self . cur = 0 # 包含__iter__的方法对象成可迭代iterable def __iter__ ( self ): pass # 包含__next__的方法对象成迭代器iterator def __next__ ( self ): if self . cur < len ( self . obj . names ): res = self . obj . names [ self . cur ] self . cur += 1 return res else : raise StopIteration 优化：去除迭代器ClassIterator，将Classmate写成迭代器，返回自身self即可。 from collections import Iterable import time class Classmate ( object ): def __init__ ( self ): self . names = list () self . cur = 0 def add ( self , name ): self . names . append ( name ) def __iter__ ( self ): return self def __next__ ( self ): if self . cur < len ( self . names ): res = self . names [ self . cur ] self . cur += 1 return res else : raise StopIteration if __name__ == '__main__' : classmate = Classmate () classmate . add ( 'Jerry' ) classmate . add ( 'Annie' ) classmate . add ( 'Sophie' ) print ( 'Iterable: {} ' . format ( isinstance ( classmate , Iterable ))) for temp in classmate : print ( temp ) time . sleep ( 1 ) Fibonacci数列，迭代器定义生成数据的方法，在访问的时候产生数据，节省内存。 class Fibonacci ( object ): def __init__ ( self , num ): self . num = num self . cur = 0 self . a = 0 self . b = 1 def __iter__ ( self ): return self def __next__ ( self ): if self . cur < self . num : res = self . a self . a , self . b = self . b , self . a + self . b self . cur += 1 return res else : raise StopIteration if __name__ == '__main__' : fibo = Fibonacci ( 10 ) for num in fibo : print ( num ) Generator 生成器是特殊的迭代器。含有yield关键字的函数，不再是函数而是生成器。调用时不再是函数调用而是创建生成器对象。 yield关键字，将函数暂停，当下次访问时，接着yield后面继续执行。 生成器两种启动方式：next(generator)和generator.send()，后者可以传参。 生成器传参：generator.send(‘pass args') 生成器return：在迭代结束抛出异常StopIterator时，返回return的内容。 def generator ( n ): cur = 0 a , b = 0 , 1 while cur < n : ret = yield a print ( 'ret>>>' , ret ) a , b = b , a + b cur += 1 # 并非调用函数，而是创建生成器对象，使用next()函数执行生成器代码。 obj = generator ( 10 ) # next()函数传递的是迭代器，而生成器是一种特殊的迭代器 ret = next ( obj ) print ( ret ) # 启动生成器时，参数传给yield右边ret ret = obj . send ( 'pass arg' ) print ( ret ) ret = next ( obj ) print ( ret ) 结论 ：迭代器能节省内存空间，能实现循环；生成器能暂定类函数的运行，用send或者next继续执行。他们都是保存生成数据的代码。 生成器的另一个重要应用协程：实现多任务。 import time def task1 (): while True : print ( '---1---' ) time . sleep ( 0.1 ) yield def task2 (): while True : print ( '---2---' ) time . sleep ( 0.1 ) yield def main (): t1 = task1 () t2 = task2 () while True : next ( t1 ) next ( t2 ) if __name__ == '__main__' : main () Coroutine 采用同步的方式编写异步代码，线程的切换是操作系统执行的。单线程内的协程是程序员自己调度切换的。不需要锁的机制，函数的切换资源消耗更少，并发性更高。 协程：可以暂停的函数（生成器），且可以向暂停处传参。python通过生成器实现协程。","tags":"Programming","url":"articles/Iterator.html","loc":"articles/Iterator.html"},{"title":"【 RL 】Reinforcement Learning","text":"马尔可夫决策过程 MDP组件 MDP符号表示 转换概率 期望回报 期望奖励 折扣期望奖励 策略与值函数 策略 Q值函数 最优策略 最优Q值函数 贝尔曼最优方程 马尔可夫决策过程 构建强化学习问题：马尔可夫决策过程 Markov Decision Processes (MDPs) MDP组件 马尔可夫决策过程为我们提供了一种形式化顺序决策的方法。 这种形式化是构建通过强化学习解决的问题的基础。 在马尔可夫决策过程中，智能体 \\(Agent\\) 与其所在的环境进行交互。这些交互随着时间的流逝依次发生。 在每个时间步 \\(t\\) 中， \\(Agent\\) 都会获得环境状态 \\(s_t\\) 的编码。给定 \\(s_t\\) ， \\(Agent\\) 将选择要采取的行动 \\(a_t\\) 。 然后，环境通过 \\(a_t\\) 将更新新的状态 \\(s_{t+1}\\) ，并且由于先前的操作， \\(Agent\\) 会获得奖励 \\(r_{t+1}\\) 。 一个马尔可夫决策过程的组建包括： Agent Environment State Action Reward 从给定状态 \\(State\\) 中选择动作 \\(Action\\) ，过渡到新状态并获得奖励 \\(Reward\\) 的过程是一遍又一遍地依次进行的，这创建的一系列成为轨迹 \\(Trajectory\\) ，该轨迹显示状态，动作和奖励的顺序。 在此过程中， \\(Agent\\) 的目标是最大限度地提高在给定这些状态下采取的这些行动所获得的奖励总额。 这意味着 \\(Agent\\) 不仅希望最大化即时奖励，还希望 最大化其随着时间的推移所获得的累积奖励 。 MDP符号表示 在一个马尔可夫决策过程中，有一个状态集合 \\(S\\) ，动作集合 \\(A\\) ，奖励集合 \\(R\\) ，且这些集合的元素是有限的。 在每一个时间步 \\(t = 0, 1, 2, ...\\) , \\(Agent\\) 接收环境的状态 \\(S_t \\in S\\) ，基于当前状态 \\(S_t\\) ， \\(Agent\\) 选择一个动作 \\(A_t \\in A\\) 。形成一个state-action对( \\(S_t, A_t\\) )。 时间推移到下一个时间步 \\(t + 1\\) ，环境受到 \\(Agent\\) 选择的动作 \\(A_t\\) 影响，将状态从 \\(S_t\\) 转为 \\(S_{t+1} \\in S\\) ，此时Agent获得一个数值奖励 \\(R_{t+1} \\in R\\) ，该奖励是关于Agent从状态 \\(S_t\\) 选择动作 \\(A_t\\) 的奖励。即奖励函数： \\(f(S_t, A_t) = R_{t+1}\\) \\(Trajectory: S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...\\) 转换概率 因为集合 \\(S\\) 和 \\(R\\) 是有限集，随机变量 \\(S_t\\) 和 \\(R_t\\) 具有明确定义的概率分布，即 \\begin{equation*} p\\left( s&#94;{\\prime },r\\mid s,a\\right) =\\Pr \\left\\{ S_{t}=s&#94;{\\prime },R_{t}=r\\mid S_{t-1}=s,A_{t-1}=a\\right\\} \\text{.} \\end{equation*} 期望回报 期望奖励 在一个MDP过程中，是什么驱动强化学习智能体？ 期望回报 Expected Return 在一个MDP过程中，智能体的目标是最大化它的期望奖励Reward 。数学表示方式： 定义在在 \\(t\\) 时刻所获得的期望奖励 \\(G\\) 是： \\begin{equation*} G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\\cdots +R_{T}\\text{,} \\end{equation*} \\(T\\) 是最后一个时间步。 期望奖励的概念非常重要，因为这是智能体的目标就是最大化期望奖励。期望奖励是推动智能体做出决策的动力。 折扣期望奖励 在一个MDP过程中，智能体的目标是最大化它的期望折扣奖励Reward 折扣率 \\(\\gamma\\) ，取值0~1之间，是折扣将来奖励的比率。 \\begin{eqnarray*} G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma &#94;{2}R_{t+3}+\\cdots \\\\ &=&\\sum_{k=0}&#94;{\\infty }\\gamma &#94;{k}R_{t+k+1}\\text{.} \\end{eqnarray*} 折扣奖励的定义使智能体更关心即时奖励而不是未来奖励，因为未来奖励将得到更大的折扣。因此，尽管智能体确实考虑了预期在将来获得的奖励，但当智能体做出关于采取特定动作的决定时，越直接的奖励就具有更大的影响力。 \\begin{eqnarray*} G_{t} &=&R_{t+1}+\\gamma R_{t+2}+\\gamma &#94;{2}R_{t+3}+\\gamma &#94;{3}R_{t+4}+\\cdots \\\\ &=&R_{t+1}+\\gamma \\left( R_{t+2}+\\gamma R_{t+3}+\\gamma &#94;{2}R_{t+4}+\\cdots \\right) \\\\ &=&R_{t+1}+\\gamma G_{t+1} \\end{eqnarray*} 策略与值函数 策略 策略：给定一个状态，智能体选择任意一个动作的概率是多少。 策略用符号 \\(\\pi\\) 表示， \\(\\pi(a|s)\\) ：在 \\(t\\) 时刻， \\(\\pi\\) 策略下，给定状态 \\(s\\) 选择动作 \\(a\\) 的概率是 \\(\\pi(a|s)\\) 。 \\(\\pi\\) 是动作 \\(a\\) 的概率分布。 Q值函数 Value Functions, which generally give us an idea of how good some given state-action pair is for an agent in terms of expected reward. 值函数：智能体选择某一个状态有多好。从reward的角度看就是，在给定一个状态选择一个动作可能增加或者减少reward。 动作值函数： \\(q_\\pi\\) 表示基于策略 \\(\\pi\\) 的动作值函数。即 在给定一个状态，基于策略 \\(\\pi\\) 选择一个动作后，给定的值，这个值用来评价选择该动作有多好。值用期望奖励定义 \\(q_\\pi\\) 称Q值函数，Q: quality \\begin{eqnarray*} q_{\\pi }\\left( s,a\\right) &=&E_{\\pi }\\left[ G_{t}\\mid S_{t}=s,A_{t}=a \\rule[-0.05in]{0in}{0.2in}\\right] \\\\ &=&E_{\\pi }\\left[ \\sum_{k=0}&#94;{\\infty }\\gamma &#94;{k}R_{t+k+1}\\mid S_{t}=s,A_{t}=a\\right] \\text{.} \\end{eqnarray*} 最优策略 什么是最优策略？ \\begin{equation*}\\pi \\geq \\pi&#94;\\prime \\text{ if and only if } q_{π}(s, a) \\geq q_{π&#94;\\prime}(s, a) \\text{ for all } s\\in\\boldsymbol{S}\\text{.} \\end{equation*} 对于所有状态 \\(s\\) ，当且仅当基于策略 \\(\\pi\\) 的值函数均大于其他所有策略 \\(\\pi&#94;\\prime\\) 的值函数时，这个策略 \\(\\pi\\) 就是最优测策略。 最优Q值函数 相似的， 最优策略有一个最优动作值函数，即最优Q值函数， \\(q_*\\) 表示。 \\begin{equation*} q_{\\ast }\\left( s,a\\right) =\\max_{\\pi }q_{\\pi }\\left( s,a\\right) \\end{equation*} \\(q_*\\) 表示对于给定状态动作对，该策略比任何其他策略更能获得最大期望奖励。 贝尔曼最优方程 \\(q_*\\) 最优Q值函数的基本属性，满足贝尔曼最优方程： \\begin{eqnarray*} q_{\\ast }\\left( s,a\\right) &=&E\\left[ R_{t+1}+\\gamma \\max_{a&#94;{\\prime }}q_{\\ast }\\left( s&#94;\\prime,a&#94;{\\prime }\\right)\\right] \\end{eqnarray*}","tags":"NLP","url":"articles/reinforcement-learning.html","loc":"articles/reinforcement-learning.html"},{"title":"A* Heuristic Algorithm","text":"#include <algorithm> // for sort #include <fstream> #include <iostream> #include <sstream> #include <string> #include <vector> using std :: cout ; using std :: ifstream ; using std :: istringstream ; using std :: sort ; using std :: string ; using std :: vector ; using std :: abs ; // TODO: Add kStart and KFinish enumerators to the State enum. enum class State { kStart , kFinish , kEmpty , kObstacle , kClosed , kPath }; // directional deltas const int delta [ 4 ][ 2 ]{{ -1 , 0 }, { 0 , -1 }, { 1 , 0 }, { 0 , 1 }}; vector < State > ParseLine ( string line ) { istringstream sline ( line ); int n ; char c ; vector < State > row ; while ( sline >> n >> c && c == ',' ) { if ( n == 0 ) { row . push_back ( State :: kEmpty ); } else { row . push_back ( State :: kObstacle ); } } return row ; } vector < vector < State >> ReadBoardFile ( string path ) { ifstream myfile ( path ); vector < vector < State >> board {}; if ( myfile ) { string line ; while ( getline ( myfile , line )) { vector < State > row = ParseLine ( line ); board . push_back ( row ); } } return board ; } /** * Compare the F values of two cells. */ bool Compare ( const vector < int > a , const vector < int > b ) { int f1 = a [ 2 ] + a [ 3 ]; // f1 = g1 + h1 int f2 = b [ 2 ] + b [ 3 ]; // f2 = g2 + h2 return f1 > f2 ; } /** * Sort the two-dimensional vector of ints in descending order. */ void CellSort ( vector < vector < int >> * v ) { sort ( v -> begin (), v -> end (), Compare ); } // Calculate the manhattan distance int Heuristic ( int x1 , int y1 , int x2 , int y2 ) { return abs ( x2 - x1 ) + abs ( y2 - y1 ); } /** * Check that a cell is valid: on the grid, not an obstacle, and clear. */ bool CheckValidCell ( int x , int y , vector < vector < State >> & grid ) { bool on_grid_x = ( x >= 0 && x < grid . size ()); bool on_grid_y = ( y >= 0 && y < grid [ 0 ]. size ()); if ( on_grid_x && on_grid_y ) return grid [ x ][ y ] == State :: kEmpty ; return false ; } /** * Add a node to the open list and mark it as open. */ void AddToOpen ( int x , int y , int g , int h , vector < vector < int >> & openlist , vector < vector < State >> & grid ) { // Add node to open vector, and mark grid cell as closed. openlist . push_back ( vector < int > { x , y , g , h }); grid [ x ][ y ] = State :: kClosed ; } /** * Expand current nodes's neighbors and add them to the open list. */ void ExpandNeighbors ( const vector < int > & current , int goal [ 2 ], vector < vector < int >> & openlist , vector < vector < State >> & grid ) { // Get current node's data. int x = current [ 0 ]; int y = current [ 1 ]; int g = current [ 2 ]; // Loop through current node's potential neighbors. for ( int i = 0 ; i < 4 ; i ++ ) { int x2 = x + delta [ i ][ 0 ]; int y2 = y + delta [ i ][ 1 ]; // Check that the potential neighbor's x2 and y2 values are on the grid and not closed. if ( CheckValidCell ( x2 , y2 , grid )) { // Increment g value and add neighbor to open list. int g2 = g + 1 ; int h2 = Heuristic ( x2 , y2 , goal [ 0 ], goal [ 1 ]); AddToOpen ( x2 , y2 , g2 , h2 , openlist , grid ); } } } /** * Implementation of A* search algorithm */ vector < vector < State >> Search ( vector < vector < State >> grid , int init [ 2 ], int goal [ 2 ]) { // Create the vector of open nodes. vector < vector < int >> open {}; // Initialize the starting node. int x = init [ 0 ]; int y = init [ 1 ]; int g = 0 ; int h = Heuristic ( x , y , goal [ 0 ], goal [ 1 ]); AddToOpen ( x , y , g , h , open , grid ); while ( open . size () > 0 ) { // Get the next node CellSort ( & open ); auto current = open . back (); open . pop_back (); x = current [ 0 ]; y = current [ 1 ]; grid [ x ][ y ] = State :: kPath ; // Check if we're done. if ( x == goal [ 0 ] && y == goal [ 1 ]) { // TODO: Set the init grid cell to kStart, and // set the goal grid cell to kFinish before returning the grid. grid [ init [ 0 ]][ init [ 1 ]] = State :: kStart ; grid [ goal [ 0 ]][ goal [ 1 ]] = State :: kFinish ; return grid ; } // If we're not done, expand search to current node's neighbors. ExpandNeighbors ( current , goal , open , grid ); } // We've run out of new nodes to explore and haven't found a path. cout << \"No path found!\" << \" \\n \" ; return std :: vector < vector < State >> {}; } string CellString ( State cell ) { switch ( cell ) { case State :: kObstacle : return \"⛰️ \" ; case State :: kPath : return \"🚗 \" ; case State :: kEmpty : return \"E \" ; case State :: kClosed : return \"C \" ; case State :: kStart : return \"🚦 \" ; case State :: kFinish : return \"🏁 \" ; default : return \"? \" ; // TODO: Add cases to return \"🚦 \" for kStart // and \"🏁 \" for kFinish. } } void PrintBoard ( const vector < vector < State >> board ) { for ( int i = 0 ; i < board . size (); i ++ ) { for ( int j = 0 ; j < board [ i ]. size (); j ++ ) { cout << CellString ( board [ i ][ j ]); } cout << \" \\n \" ; } } #include \"test.cpp\" int main () { int init [ 2 ]{ 0 , 0 }; int goal [ 2 ]{ 4 , 5 }; auto board = ReadBoardFile ( \"1.board\" ); auto solution = Search ( board , init , goal ); PrintBoard ( solution ); // Tests TestHeuristic (); TestAddToOpen (); TestCompare (); TestSearch (); TestCheckValidCell (); TestExpandNeighbors (); }","tags":"Algorithms","url":"articles/A_star_algorithm.html","loc":"articles/A_star_algorithm.html"},{"title":"Sort","text":"Quick Sort Merge Sort Heap Sort Topological Sort Quick Sort 引例：荷兰国旗问题 Merge Sort class Solution { public : ListNode * sortList ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) { return head ; } ListNode * fast = head ; ListNode * slow = head ; while ( fast -> next && fast -> next -> next ) { slow = slow -> next ; fast = fast -> next -> next ; } fast = slow -> next ; slow -> next = nullptr ; return mergeTwoLists ( sortList ( head ), sortList ( fast )); } ListNode * mergeTwoLists ( ListNode * l1 , ListNode * l2 ) { if ( l1 == nullptr && l1 == nullptr ) { return nullptr ; } if ( l1 == nullptr || l2 == nullptr ) { return l1 == nullptr ? l2 : l1 ;} ListNode dummy ( -1 ), * current ; current = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { current -> next = l1 ; l1 = l1 -> next ; } else { current -> next = l2 ; l2 = l2 -> next ; } current = current -> next ; } current -> next = l1 == nullptr ? l2 : l1 ; return dummy . next ; } }; Heap Sort https://docs.python.org/3/library/heapq.html#heapq.heapify 时间复杂度O(N*logN)，额外空间复杂度O(1)。 堆结构非常重要 堆结构的 heapInsert 和 heapify 堆结构的增大和减少 如果只是建堆的过程，时间复杂度为O(N) 优先级队列结构，就是堆结构 堆的定义： 堆结构 就是一个 完全二叉树 ，数组的结构实现，通过约定下标规则。 对于任意下标为 i 的结点， 左孩子： 2*i + 1 右孩子： 2*i + 2 父节点： (i-1) // 2 大根堆 ：在一棵完全二叉树中，任何一棵子树的最大值都是这棵子树的根，所形成的结构叫大根堆。小根堆类似。 堆的特点与优势： 大/小根堆有一个很重要的属性：它的最大/小元素始终是根节点 heap[0] 。 堆的调整代价只和 层数 有关，所以 入堆 和 出堆 的代价只有 O(lgN) 。 大根堆的实现： This implementation uses arrays for which heap[i] > heap[2*i+1] and heap[i] > heap[2*i+2] for all i , counting elements from zero. 给定数组，都可根据约定 视其为堆 ，但其不是大根堆， 如何将数组调整为大根堆 ？ 建立大根堆： heapInsert : 经历一个新结点加入一个已经调整好的堆中，同时往上调整的过程。调整停止条件：当加入结点值不大于其父节点时， 调整停止。 堆在数组上可伸缩 def insertHeap ( arr , index ) : par_i = ( index - 1 ) // 2 while par_i >= 0 and arr [ index ] > arr [ par_i ] : # 插入结点值比父结点大时 ， 往上调整 arr [ index ] , arr [ par_i ] = arr [ par_i ] , arr [ index ] # 与父结点交换 index = par_i # 插入结点来到父节点的位置 par_i = ( index - 1 ) // 2 def createHeap ( arr ) : if arr == None or len ( arr ) < 2 : return arr for i in range ( len ( arr )) : # 依次将结点加入堆中 ， 最终将数组 （ 堆 ） 调整为大根堆 insertHeap ( arr , i ) return arr 时间复杂度分析： O(N) 当第 i 个结点加入堆中时， 0 ~ i-1 已经调整为大根堆，其高度为 O(log(i-1)) ，即调整代价为 O(log(i-1)) 。(沿其父结点依次向上> 比较调整) 所以, N 个结点的调整代价为： O(lg1) + O(lg2) + ... + O(lgN) 收敛于 O(N) 堆化： def heapify ( arr , index , heapSize ) : left = 2 * index + 1 # 左孩子未越界 ， 在堆上 ， 继续循环判断是否下沉 while left < heapSize : # 1. 求左右孩子最大的下标 largest = left + 1 if ( left + 1 ) < heapSize and arr [ left+1 ] > arr [ left ] else left # 2. 最大孩子和本结点最大的下标 largest = index if arr [ index ] >= arr [ largest ] else largest # 3. 如果最大的结点就是自身 ， heapify完成跳出 if largest == index : break # 4. 否则 ， 交换下沉 arr [ largest ] , arr [ index ] = arr [ index ] , arr [ largest ] index = largest left = 2 * index + 1 堆排序： 堆大小：heapSize = len(arr) 数组最后一个数下标：heapSize -= 1 把数组 arr 创建为大根堆 堆顶 arr[0] 与数组最后一个数 arr[heapSize] 交换 将堆的大小缩小 heapSize -= 1 0~heapSize 做 heapify 至 2 循环，直到堆大小减到0，数组有序 def heapSort ( arr ) : createHeap ( arr ) heapSize = len ( arr ) while heapSize > 1 : heapSize -= 1 arr [ 0 ] , arr [ heapSize ] = arr [ heapSize ] , arr [ 0 ] heapify ( arr , 0 , heapSize ) Topological Sort # Definition for a Directed graph node class DirectedGraphNode : def __init__ ( self , x ): self . label = x self . neighbors = [] class Solution : \"\"\" @param graph: A list of Directed graph node @return: A list of integer \"\"\" def topSort ( self , graph ): # 1. 统计结点入度 indegree = self . get_indegree ( graph ) # 2. BFS order = [] start_nodes = [ n for n in graph if indegree [ n ] == 0 ] # 入度为0的所有结点 queue = collections . deque ( start_nodes ) # 队列中存储的是入度为0的点 while queue : node = queue . popleft () order . append ( node ) for neighbor in node . neighbors : # 遍历该节点的所有邻居节点，第一层遍历。 indegree [ neighbor ] -= 1 # 将队列中输出的点的所以临界点入度减1 if indegree [ neighbor ] == 0 : # 将入度为0的结点放入队列 queue . append ( neighbor ) return order def get_indegree ( self , graph ): \"\"\" 计算每一个结点的入度数 \"\"\" indegree = { x : 0 for x in graph } # 初始化每一个结点的入度数为0 for node in graph : for neighbor in node . neighbors : indegree [ neighbor ] += 1 return indegree","tags":"Algorithms","url":"articles/Sort.html","loc":"articles/Sort.html"},{"title":"C++","text":"1. 基于对象的程序设计 1.1 不含指针成员的类 1.2 含有指针成员的类 1.3 new 1.4 delete 1.5 this 1.6 static 1.7 单例模式 1.8 class template 1.9 function template 2. 面向对象的程序设计 2.1 组合 定义 构造与析构 内存角度 适配器模式 2.2 委托 定义 Handle & Body 2.2 继承 定义 构造与析构 含有虚函数的继承 模板方法模式 1. 基于对象的程序设计 只包含单一类的程序设计，存在两种类型的类的设计思想：是否含有指针成员。 编写类 class 的良好习惯： 数据私有化 private 构造函数尽量使用初始化列表 initialization list ，效率高。 参数与返回值尽量引用传递 by reference ，返回的值只要不是local object，尽量使用引用返回。 在类体中的函数需要加 const 的尽量加，不会修改成员变量。 1.1 不含指针成员的类 Complex class #ifndef __COMPLEX__ #define __COMPLEX__ class Complex { public : Complex ( double r = 0 , double i = 0 ) : re ( r ), im ( i ) { } Complex & operator += ( const Complex & ); double real () const { return re ; } double imag () const { return im ; } private : double re , im ; friend Complex & __doapl ( Complex & , const Complex & ); }; inline Complex & __doapl ( Complex * ths , const Complex & r ){ ths -> re += r . re ; ths -> im += r . im ; return * ths ; } // class member function inline Complex & Complex :: operator += ( const Complex & r ){ return __doapl ( this , r ); } // class non-member function, global function inline Complex operator + ( const Complex & x , const Complex & y ){ // temporary object, return by value return Complex ( real ( x ) + real ( y ), imag ( x ) + imag ( y )); } inline Complex operator + ( double x , const Complex & y ){ return Complex ( x + real ( y ), imag ( y )); } inline Complex operator + ( const Complex & x , double y ){ return Complex ( real ( x ) + y , imag ( x )); } // class non-member function, global function inline ostream & operator << ( ostream & os , const Complex & r ){ // return by referance due to the example of `cout << c1 << c2 << endl` return std :: cout << '(' << real ( r ) << ' , ' << ' imag ( r ) ' << ')' ; } #endif 1.2 含有指针成员的类 String class #ifndef __MYSTRING__ #define __MYSTRING__ #include <cstring> class String { public : // Big Three String ( const char * cstr ); // constructor func String ( const String & str ); // copy constructor func String & operator = ( const String & str ); // copy asignment func ~ String (); // destructor func char * get_c_str () const { return m_data ; } private : char * m_data ; }; inline String :: String ( const char * cstr = 0 ){ if ( cstr ){ m_data = new char [ strlen ( cstr ) + 1 ]; strcpy ( m_data , cstr ); } else { m_data = new char [ 1 ]; * m_data = '\\0' ; } } inline String :: String ( const String & str ){ m_data = new char [ strlen ( str . m_data ) + 1 ]; strcpy ( m_data , str . m_data ); // deep copy } inline String & String :: operator = ( const String & str ){ // checking self-assignment if ( this == & str ){ return * this ; } delete [] m_data ; // preventing memory leak m_data = new char [ strlen ( str . m_data ) + 1 ]; strcpy ( m_data , str . m_data ); return * this ; } inline String ::~ String (){ delete [] m_data ; } #include <iostream> using namespace std ; ostream & operator << ( ostream & os , const String & str ){ os << str . get_c_str (); return os ; } #endif 1.3 new 先分配内存，再调用构造函数。 new是运算符，内部调用C语言的malloc函数。 Complex* pc = new Complex(1, 2); 编译器转化为： Complex * pc ; // operator new是函数名，内部调用malloc(n) void * mem = operator new ( sizeof ( Complex ) ); // 1. 分配内存 pc = static_cast < Complex *> ( mem ); // 2. 类型转换 pc -> Complex :: Complex ( 1 , 2 ) // 3. 构造函数 // 3等价于Complex::Complex(pc, 1, 2); pc即this 1.4 delete 先调用析构函数，再释放内存。 delete是运算符，内部调用C语言的free函数。 注意： delete存在两个删除动作。1.先调用析构函数，把类中动态分配的内存空间 \"Hello\" 删除；2.再将指向字符串的指针 ps 删除。 String * ps = new String ( \"Hello\" ); ... delete ps ; 编译器转化为： String ::~ String ( ps ); // 析构函数 // operator delete是函数名，内部调用free(ps) operator delete ( ps ); // 释放内存 array new 与 array delete 搭配使用，否则存在内存泄漏。 1.5 this 类的成员函数在内存中只有一份，默认存在this指针，指向实例化的对象，类的成员函数是通过this指针来处理每个对象对应的数据。隐式将this指针即&c1对象地址作为参数传递传递给类成员函数。 Complex c1 , c2 ; c1 . real (); --- > Complex :: real (& c1 ); c2 . real (); --- > Complex :: real (& c2 ); 1.6 static static data members // 静态数据，与类的对象脱离 static member func // 静态函数，与类的成员函数相同，内存中只存在一份 静态函数与类成员函数最大区别在于：没有this指针 class Account { public : static double m_rate ; static void set_rate ( const double & x ) { m_rate = x ; } }; double Account :: m_rate = 8.0 ; // 静态数据成员一定要在类体外定义 int main () { // 静态函数的调用两种方法 // 类名调用 Account :: set_rate ( 5.0 ); // 对象调用 Account a ; a . set_rate ( 5.0 ); } 1.7 单例模式 一个类只存在一个对象，实现方法是将类的构造器私有化private与static的综合应用。 class Singleton { public : static Singleton & getInstance () { return a ; } // 获取单例对象。 void setup () { ... } private : Singleton (); // 构造器私有化，外界禁止创建对象。 Signleton ( const Singleton & rhs ); static Singleton a ; // 定义唯一一个静态单列对象。 ... }; Singleton :: getInstance . setup (); // 获取单例对象并调用对象的类成员函数setup()。 上述单例模式并非最佳写法，待优化的问题是：对象a已经在内存中构建，即使不使用。最佳写法如下： class Singleton { public : static Singleton & getInstance (); void setup () { ... } private : Singleton (); Signleton ( const Singleton & rhs ); ... }; // 单例对象a在使用时内存构建，getInstance函数退出时，static变量依然存在。 Single :: getInstance () { static Singleton a ; return a ; } Singleton :: getInstance . setup (); 1.8 class template template template < typename T > class Complex { public : Complex ( T r = 0 , T i = 0 ) : re ( r ), im ( i ) { } Complex & operator += ( const Complex & ); T real () const { return re ; } T imag () const { return im ; } private : T re , im ; friend Complex & __doapl ( Complex & , const Complex & ); }; int main () { // 代码膨胀 Complex < double > c1 ( 1.2 , 2.3 ); Complex < int > c1 ( 2 , 3 ); return 0 ; } 1.9 function template template ：编译器会对模板函数做参数类型推导。C++标准库中算法均采用函数模板。 template < class T > inline const T & min ( const T & a , const T & b ) { return b < a ? b : a ; } class Stone { public : Stone ( int w , int h , int weight ) : _w ( w ), _h ( h ), _weight ( w ) { } bool operator < ( const Stone & rhs ) const { return _weight < rhs . _weight ; } private : int _w , _h , _weight ; }; int main () { Stone r1 ( 2 , 3 ), r2 ( 3 , 3 ), r3 ; r3 = min ( r1 , r2 ); return 0 ; } 使用时， min 函数不需要像类模板 min<double>(r1, r2) 指定参数，通过参数推导。通过推导 T 为 Stone 类型， b < a 调用对象 b 的类型 T 中的 Stone::operator< ，即在 Stone 类中。 2. 面向对象的程序设计 面向多个类的程序设计，类与类之间存在关系 2.1 组合 定义 一个类包含另一个类，has-a关系。实现方式：一个类的成员变量为另一个类的类对象。 构造与析构 组合的构造与析构：构造 由内而外 ，析构 由外而内 内存角度 组合的内存角度： 适配器模式 适配器模式(Adapter Pattern) 是一种结构型设计模式， 它能使接口不兼容的对象能够相互合作。适配器模式将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 queue -> deque ，设计模式：Adapter是适配器模式。 适配器模式：现有类deque的功能已经满足客户需求，但是deque类的名称和接口无法满足客户。需要一个新的queue类包装现有的deque类，适配满足客户的接口和名称。 2.2 委托 定义 委托又称通过引用的组合。一个类String通过引用包含另一个类StrigRef的指针，并非真正意义上的包含。 Handle & Body 思想：称为Handle and Body 或 pimpl指针实现。Handle类通过指针指向实现类Body。 优点：解耦。Body实现类任意变动不会影响Handle端，Handle端进而也不会影响客户端。 class String { public : String (); String ( const char * s ); String ( const String & s ); String & Operator = ( const String & s ); ~ String (); private : StringRef * rep ; // pimpl该方法称为指针实现 }; namespace { class StringRef { friend class String ; StringRep ( const char * s ); ~ StringRep (); int count ; char * ref ; }; } 2.2 继承 定义 子类对象包含父类成分，is-a关系 构造与析构 继承与组合一样：构造 由内而外 ，析构 由外而内 。子类构造函数先调用父类构造函数，再执行自己；子类的析构函数先执行自己，再调用父类的析构函数。 父类的析构函数为什么必须是虚函数virtual？ 原因：当基类指针指向派生类的时候，若基类析构函数不声明为虚函数，在析构时，只会调用基类而不会调用派生类的析构函数，从而导致内存泄露。 含有虚函数的继承 继承最重要的是需要与虚函数结合才能达到强有力的效果。 类的成员函数分为：非虚函数，虚函数，纯虚函数。 非虚函数：子类是不可覆盖override的。 虚函数：存在默认定义，期望子类覆盖override。 纯虚函数：不存在默认定义，子类一定需要覆盖override。 class Shape { public : virtual draw () const = 0 ; // pure virtual virtual error ( const std :: string & msg ); // impure virtual int objectID () const ; // non-virtual }; class Rectangle : public Shape { ... }; 模板方法模式 模板方法模式(Template Method Pattern) 是一种行为设计模式，它在父类中定义了一个算法的框架，允许子类在不修改结构的情况下重写算法的特定步骤。模板方法模式，定义一个操作中的算法骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重新定义该算法的某些特定步骤。 经典场景： 应用程序框架 中大量使用模板方法，不能确定的功能设置为虚函数推迟到子类实现。 模板方法就是图中的 OnFileOpen() 方法，方法体内的 Serialize 是虚函数，推迟到子类 CMyDoc 中实现。 子类的实现的虚函数是如何调用的？ 即子类对象调用父类方法的过程。 子类对象 myDoc 在调用父类方法 OnFileOpen 时，编译器会隐式的将子类对象 &myDoc 的地址传递给父类 OnFileOpen(&myDoc) 方法，即 this = &myDoc 指针。 父类 OnFileOpen 方法在执行时，遇到虚函数 Serialize 实际时执行 this->Serialize() 子类的 Serialize() 方法。 #include <iostream> using namespace std ; class CDocument { public : void OnFileOpen () { // 模板方法，存在尚未实现的虚函数 // 应用程序框架，每一个cout表示一个实际的操作 cout << \"dialogue ...\" << endl ; cout << \"check file status ...\" << endl ; cout << \"open file ...\" << endl ; Serialize (); cout << \"close file ...\" << endl ; } virtual void Serialize () { }; // 推迟到子类实现 }; class CMyDoc : public CDocument { public : virtual void Serialize () { cout << CMyDoc :: Serialize () << endl ; } }; int main () { CMyDoc myDoc ; myDoc . OnFileOpen (); }","tags":"Programming","url":"articles/C++.html","loc":"articles/C++.html"},{"title":"LinkedList","text":"链表结点定义 链表基本操作 Remove Duplicates from Sorted List Reverse Linked List Merge Two Sorted Lists Merge k Sorted Lists Sort List 擅用Dummy节点 Remove Duplicates from Sorted List II Partition List 快慢指针 综合设计 LRU Cache GitHub LinkedList 链表结点定义 class ListNode { public : int val ; ListNode * next ; ListNode () : val ( 0 ), next ( nullptr ) {} ListNode ( int x ) : val ( x ), next ( nullptr ) {} ListNode ( int x , ListNode * next ) : val ( x ), next ( next ) {} }; 链表基本操作 链表的基本操作与技巧，需要熟记于心，信手拈来 插入、删除、翻转（基础） 合并与中分（进阶） Remove Duplicates from Sorted List 由于重复节点要保留一个，则不需要考虑删除头节点的特殊情况。 class Solution : def deleteDuplicates ( self , head : ListNode ) -> ListNode : if not head or not head . next : return head cur = head while cur . next : if cur . val == cur . next . val : cur . next = cur . next . next else : cur = cur . next return head Reverse Linked List Python class Solution : if not head or not head . next : return head pre , cur = None , head while cur : pos = cur . next cur . next = pre pre = cur cur = pos return pre C ++ class Solution { public : ListNode * reverseList ( ListNode * head ){ if ( ! head ) return nullptr ; ListNode * pre , * cur , * post ; cur = head ; pre = nullptr ; while ( cur ){ post = cur -> next ; cur -> next = pre ; pre = cur ; cur = post ; } return pre ; } }; Merge Two Sorted Lists Python class Solution : def mergeTwoLists ( self , l1 , l2 ): dummy = cur = ListNode ( - 1 ) while l1 and l2 : if l1 . val < l2 . val : cur . next = l1 l1 = l1 . next else : cur . next = l2 l2 = l2 . next cur = cur . next cur . next = l1 or l2 return dummy . next C ++ class Solution { public : ListNode * mergeTwoLists ( ListNode * l1 , ListNode * l2 ){ if ( ! l1 && ! l2 ) return nullptr ; if ( ! l1 || ! l2 ) return ! l1 ? l2 : l1 ; ListNode dummy ( -1 ), * current ; current = & dummy ; while ( l1 && l2 ){ if ( l1 -> val < l2 -> val ){ current -> next = l1 ; l1 = l1 -> next ; } else { current -> next = l2 ; l2 = l2 -> next ; } current = current -> next ; } current -> next = l1 == nullptr ? l2 : l1 ; return dummy . next ; } }; Merge k Sorted Lists #include <iostream> #include <vector> using namespace std ; class ListNode { int val ; ListNode * next ; ListNode () : val ( 0 ), next ( nullptr ) { } ListNode ( int val ) : val ( val ), next ( nullptr ) { } ListNode ( int val , ListNode * next ) : val ( val ), next ( next ) { } }; class Solution { public : ListNode * mergeKLists ( vector < ListNode *>& lists ) { /*O(K*N)*/ if ( lists . size () == 0 ) { return nullptr ; } if ( lists . size () == 1 ) { return lists [ 0 ]; } ListNode * res = lists [ 0 ]; int n = lists . size (); for ( int i = 1 ; i < n ; ++ i ) { res = mergeLists ( res , lists [ i ]); } return res ; } ListNode * mergeKLists ( vector < ListNode *>& lists ) { /* 时间复杂度分析：K 条链表的总结点数是 N，平均每条链表有 N/K 个 * 节点，因此合并两条链表的时间复杂度是 O(N/K)。从 K 条链表开始 * 两两合并成 1 条链表，因此每条链表都会被合并 logK 次，因此 K * 条链表会被合并 K * logK 次，因此总共的时间复杂度是 K*logK*N/K * 即 O（NlogK）。*/ return partition ( lists , 0 , lists . size () - 1 ); } ListNode * partition ( vector < ListNode *>& lists , int start , int end ) { if ( start > end ) {{ return nullptr ; } if ( start == end ) { return lists [ start ]; } if ( start < end ) { int mid = start + (( end - start ) >> 1 ); ListNode * l = partition ( lists , start , mid ); ListNode * r = partition ( lists , mid + 1 , end ); return mergeTwoLists ( l , r ); } return nullptr ; } ListNode * mergeTwoLists ( ListNode * l1 , ListNode * l2 ){ if ( l1 == nullptr && l2 == nullptr ) { return nullptr ; } if ( l1 == nullptr ) { return l2 ; } if ( l2 == nullptr ) { return l1 ; } ListNode dummy ( -1 ), * cur ; cur = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { cur -> next = l1 ; l1 = l1 -> next ; } else { cur -> next = l2 ; l2 = l2 -> next ; } cur = cur -> next ; } cur -> next = l1 == nullptr ? l2 : l1 ; return dummy . next ; } }; Sort List class Solution { public : ListNode * sortList ( ListNode * head ) { if ( head == nullptr || head -> next == nullptr ) { return head ; } ListNode * fast = head ; ListNode * slow = head ; while ( fast -> next && fast -> next -> next ) { slow = slow -> next ; fast = fast -> next -> next ; } fast = slow -> next ; slow -> next = nullptr ; return mergeTwoLists ( sortList ( head ), sortList ( fast )); } ListNode * mergeTwoLists ( ListNode * l1 , ListNode * l2 ) { if ( l1 == nullptr && l1 == nullptr ) { return nullptr ; } if ( l1 == nullptr || l2 == nullptr ) { return l1 == nullptr ? l2 : l1 ;} ListNode dummy ( -1 ), * current ; current = & dummy ; while ( l1 && l2 ) { if ( l1 -> val < l2 -> val ) { current -> next = l1 ; l1 = l1 -> next ; } else { current -> next = l2 ; l2 = l2 -> next ; } current = current -> next ; } current -> next = l1 == nullptr ? l2 : l1 ; return dummy . next ; } }; 擅用Dummy节点 以下两种情况下需要使用 Dummy Node ： 当链表的结构发生变化时 当需要返回的链表头不确定时 Remove Duplicates from Sorted List II 重要!!! 由于要删掉所有的重复项，头节点也可能被删除，而最终却还需要返回链表的头节点，所以定义一个新的节点dummy链上原链表来简化。 dummy = p = ListNode(-1) p = p.next dummy.next class Solution : def deleteDuplicates ( self , head : ListNode ) -> ListNode : if not head or not head . next : return head dummy = p = ListNode ( - 1 ) # dummy和p是这个节点的引用 dummy . next = head # dummy与p引用的节点next指针均指向了head节点 while p . next and p . next . next : if p . next . val == p . next . next . val : sameVal = p . next . val while p . next and p . next . val == sameVal : p . next = p . next . next # 在dummy与p依旧引用同一个节点的情况下，dummy与p同时修改next指针 else : p = p . next # p引用重新指向下一个节点，dummy不变 return dummy . next # 所以dummy.next能够保持总是指向第一个节点 Partition List more.next = None ：后半段的链表，尾节点的 next 指针要设置为 None ！ class Solution : def partition ( self , head : ListNode , x : int ) -> ListNode : if not head and not head . next : return head dummy1 = less = ListNode ( - 1 ) dummy2 = more = ListNode ( - 1 ) while head : if head . val < x : less . next = head less = head else : more . next = head more = head head = head . next more . next = None # Note!!! less . next = dummy2 . next return dummy1 . next Error : dummy1 = dummy2 = ListNode ( - 1 ) # 引用同一个对象！ less = dummy1 more = dummy2 Right : dummy1 = less = ListNode ( - 1 ) dummy2 = more = ListNode ( - 1 ) 快慢指针 综合设计 LRU Cache 分析：为了保持cache的性能，使查找，插入，删除都有较高的性能，我们使用 双向链表 和 哈希表 作为cache的数据结构，因为： 双向链表 插入 和 删除 效率高 哈希表保存每个节点的地址，可以基本保证在O(1)时间内 查找 节点 具体实现细节： 越靠近链表头部，表示节点上次访问距离现在时间最短，尾部的节点表示最近访问最少 查询或者访问节点时，如果节点存在，把该节点交换到链表头部，同时更新hash表中该节点的地址 插入节点时，如果cache的size达到了上限，则删除尾部节点，同时要在hash表中删除对应的项。新节点都插入链表头部。 注意： 这题更多的是考察用数据结构进行设计的能力，在写代码时尽量将子函数拆分出来，先写个整体的框架。 移出链表最后一个节点时，要记得在链表和哈希表中都移出该元素，所以节点中也要记录Key的信息，方便在哈希表中移除 头尾节点采用dummy的技巧，极大简化程序。 class ListNode (): def __init__ ( self , key , val ): self . key = key # 记录Key的信息，方便在哈希表中移除 self . val = val self . pre = None self . next = None class DoubleLinkedList (): def __init__ ( self ): self . head = ListNode ( - 1 , - 1 ) # dummy head node 极大简化了程序头尾的处理 self . tail = ListNode ( - 1 , - 1 ) # dummy tail node self . head . next = self . tail self . tail . pre = self . head def insertHead ( self , node ): '''头插法 ''' node . next = self . head . next node . pre = self . head self . head . next . pre = node self . head . next = node def removeNode ( self , node ): '''删除任意一个指定节点 ''' node . pre . next = node . next node . next . pre = node . pre def removeTailNode ( self ): '''删除尾节点，尾节点是最久未使用的 ''' removeNode = self . tail . pre self . removeNode ( removeNode ) class LRUCache : def __init__ ( self , capacity : int ): self . cache = DoubleLinkedList () self . map = {} # 加快搜索速度：{key：对应节点的地址} self . cap = capacity # LRU Cache的容量大小 def get ( self , key : int ) -> int : '''查询操作 ''' if key not in self . map : return - 1 node = self . map [ key ] # key在字典中 self . cache . removeNode ( node ) # 将key对应的节点删除 self . cache . insertHead ( node ) # 然后将这个节点添加到双向链表头部 return node . val # 并返回节点的value def put ( self , key : int , value : int ) -> None : ''' 1. 设置value。 2. 如果key不存在则插入value，需注意cache容量。 ''' if key in self . map : # 如果key在字典中 node = self . map [ key ] self . cache . removeNode ( node ) #先在链表cache中删掉key对应的节点 self . cache . insertHead ( node ) # 然后将这个节点插入到链表的头部 node . val = value # 将这个节点的值val改写为value else : node = ListNode ( key , value ) # 新建一个Node节点，val值为value self . map [ key ] = node # 将key和node的对应关系添加到字典中 self . cache . insertHead ( node ) # 将这个节点添加到链表表头 if len ( self . map ) > self . cap : del self . map [ self . cache . tail . pre . key ] self . cache . removeTailNode ()","tags":"Algorithms","url":"articles/LinkedList.html","loc":"articles/LinkedList.html"},{"title":"【 RL 】Q Learning","text":"Training Inference import numpy as np import gym import random import time from IPython.display import clear_output \"\"\"Creating the Environment\"\"\" env = gym . make ( \"FrozenLake-v0\" ) \"\"\"Creating the Q-Table and initializing all the Q-Values to zero for each state-action pair.\"\"\" action_space_size = env . action_space . n state_space_size = env . observation_space . n q_table = np . zeros (( state_space_size , action_space_size )) q_table array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) \"\"\"Initializing the parametres for Q-Learning algorithm\"\"\" num_episodes = 10000 max_steps_per_episode = 100 learning_rate = 0.1 discount_rate = 0.99 # for exploration-exploitation trade-off: epsilon-greedy policy exploration_rate = 1 max_exploration_rate = 1 min_exploration_rate = 0.01 exploration_decay_rate = 0.001 Training rewards_all_episodes = [] # Q-Learning algorithm for episode in range ( num_episodes ): # initialize new episode parameter state = env . reset () done = False rewards_current_episode = 0 for step in range ( max_steps_per_episode ): # exploration-exploitation trade-off: agent explores or # exploits the environment in this time-step. exploration_rate_threshold = random . uniform ( 0 , 1 ) if exploration_rate_threshold > exploration_rate : action = np . argmax ( q_table [ state , :]) # print(\"policy action: {}\".format(action)) else : action = env . action_space . sample () # print(\"random action: {}\".format(action)) # taking action new_state , reward , done , info = env . step ( action ) # Update Q-Table for Q(s, a) q_table [ state , action ] = ( 1 - learning_rate ) * q_table [ state , action ] \\ + learning_rate * ( reward + discount_rate * np . max ( q_table [ new_state , :])) # transition to the next state state = new_state rewards_current_episode += reward if done is True : break # exploration rate decay exploration_rate = min_exploration_rate + ( max_exploration_rate - min_exploration_rate ) \\ * np . exp ( - exploration_decay_rate * episode ) rewards_all_episodes . append ( rewards_current_episode ) # Calculate and print the average reward per thousand episodes rewards_per_thosand_episodes = np . split ( np . array ( rewards_all_episodes ), num_episodes / 1000 ) count = 1000 for r in rewards_per_thosand_episodes : print ( count , \": \" , str ( sum ( r / 1000 ))) count += 1000 ********Average reward per thousand episodes******** 1000 : 0.057000000000000044 2000 : 0.21100000000000016 3000 : 0.3760000000000003 4000 : 0.5990000000000004 5000 : 0.6180000000000004 6000 : 0.6640000000000005 7000 : 0.6830000000000005 8000 : 0.6610000000000005 9000 : 0.6620000000000005 10000 : 0.6840000000000005 q_table array([[0.57489904, 0.52079771, 0.51748556, 0.48793235], [0.42212101, 0.26917985, 0.34271382, 0.5203443 ], [0.41164658, 0.4280145 , 0.40879341, 0.48422459], [0.32569816, 0.37293149, 0.36049325, 0.46078178], [0.59779375, 0.39344761, 0.32263887, 0.28641992], [0. , 0. , 0. , 0. ], [0.29567781, 0.13502175, 0.22982768, 0.21277447], [0. , 0. , 0. , 0. ], [0.35598767, 0.48995879, 0.42166608, 0.62406041], [0.45348018, 0.70820025, 0.49994856, 0.49117507], [0.58119802, 0.44345026, 0.26892586, 0.38122787], [0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. ], [0.33119261, 0.66936184, 0.77995132, 0.48286706], [0.72566318, 0.8968122 , 0.70399676, 0.7334053 ], [0. , 0. , 0. , 0. ]]) Inference # The Q-Table is the konwledge we gained by training. # agent choose the best action from each state according to the Q-Table. for episode in range ( 3 ): state = env . reset () done = False print ( \"EPISODE {} \" . format ( episode + 1 )) time . sleep ( 1 ) for step in range ( max_steps_per_episode ): # show the current state of environment on screen. env . render () time . sleep ( 0.3 ) clear_output ( wait = True ) # choose action with highest Q-Value for current state. action = np . argmax ( q_table [ state , :]) # update the state, reward, done for the action. new_state , reward , done , info = env . step ( action ) if done : clear_output ( wait = True ) env . render () if reward == 1 : print ( \"Reached the goal!\" ) time . sleep ( 3 ) clear_output ( wait = True ) else : print ( \"Fell the hole!\" ) time . sleep ( 3 ) clear_output ( wait = True ) break state = new_state env . close () (Down) SFFF FHFH FFFH HFF\u001b[41mG\u001b[0m Reached the goal!","tags":"NLP","url":"articles/【RL】Q-Learning.html","loc":"articles/【RL】Q-Learning.html"},{"title":"Algorithms","text":"Linked List 206. Reverse Linked List 21. Merge Two Sorted Lists 234. Palindrome Linked List 19. Remove Nth Node From End of List Search DFS 695. Max Area of Island 547. Number of Provinces 417. Pacific Atlantic Water Flow Backtracking 46. Permutations 77. Combinations 79. Word Search Linked List 206. Reverse Linked List class ListNode : def __init__ ( self , val ): self . val = val self . next = None class Solution : def reverseLinkedList ( self , head : ListNode ) -> ListNode : if not head or not head . next : return head pre , cur , post = None , head , head . next while cur : post = cur . next cur . next = pre pre = cur cur = post return pre 21. Merge Two Sorted Lists class ListNode : def __init__ ( self , val : int ): self . val = val self . next = None class Solution : def mergeTwoLists ( self , l1 : ListNode , l2 : ListNode ) -> ListNode : if not l1 and not l2 : return None if not l1 : return l2 if not l2 : return l1 dummpy = cur = ListNode ( - 1 ) while l1 and l2 : if l1 . val < l2 . val : cur . next = l1 l1 = l1 . next else : cur . next = l2 l2 = l2 . next cur = cur . next cur . next = l1 or l2 return dummpy . next 234. Palindrome Linked List class Solution : def isPalindrome ( self , head : ListNode ) -> bool : slow = fast = head while fast and fast . next : slow = slow . next fast = fast . next . next fast = head slow = self . reverseLinkedList ( slow ) while slow : if fast . val != slow . val : return False fast = fast . next slow = slow . next return True def reverseLinkedList ( self , head : ListNode ) -> ListNode : pre , cur = None , head while cur : post = cur . next cur . next = pre pre = cur cur = post return pre 19. Remove Nth Node From End of List class Solution : def removeNthFromEnd ( self , head : ListNode , n : int ) -> ListNode : dummpy = slow = fast = ListNode ( - 1 ) fast . next = head for _ in range ( n + 1 ): fast = fast . next while fast : fast = fast . next slow = slow . next slow . next = slow . next . next return dummpy . next Search DFS 695. Max Area of Island class Solution : \"\"\"O(M*N)：所有节点只遍历一次。 M*N个节点；每个节点有4条边。 \"\"\" def maxAreaOfIsland ( self , grid : List [ List [ int ]]) -> int : if not grid : return 0 row , col = len ( grid ), len ( grid [ 0 ]) max_area = 0 for i in range ( row ): for j in range ( col ): max_area = max ( max_area , self . dfs ( grid , i , j )) return max_area def dfs ( self , grid , i , j ): if i < 0 or i >= len ( grid ) or j < 0 or j >= len ( grid [ 0 ]) or grid [ i ][ j ] == 0 : return 0 grid [ i ][ j ] = 0 # do choice, mark visited return 1 + self . dfs ( grid , i - 1 , j ) + self . dfs ( grid , i + 1 , j ) + self . dfs ( grid , i , j - 1 ) + self . dfs ( grid , i , j + 1 ) 547. Number of Provinces class Solution : \"\"\"O(N*N) N个节点；每个节点至少1条边（只与自己相连），最多N条边（与所有节点相连）。 \"\"\" def findCircleNum ( self , isConnected : List [ List [ int ]]) -> int : node = len ( isConnected ) visited = [ False ] * node count = 0 for i in range ( node ): if visited [ i ] == False : self . dfs ( isConnected , i , visited ) count += 1 return count def dfs ( self , isConnected , i , visited ): visited [ i ] = True for j in range ( len ( isConnected )): if isConnected [ i ][ j ] == 1 and visited [ j ] == False : self . dfs ( isConnected , j , visited ) 417. Pacific Atlantic Water Flow class Solution : directions = [[ - 1 , 0 ], [ 1 , 0 ], [ 0 , - 1 ], [ 0 , 1 ]] def pacificAtlantic ( self , heights : List [ List [ int ]]) -> List [ List [ int ]]: row , col = len ( heights ), len ( heights [ 0 ]) pa_status = [[ False for _ in range ( col )] for _ in range ( row )] al_status = [[ False for _ in range ( col )] for _ in range ( row )] for r in range ( row ): self . dfs ( heights , r , 0 , pa_status ) self . dfs ( heights , r , col - 1 , al_status ) for c in range ( col ): self . dfs ( heights , 0 , c , pa_status ) self . dfs ( heights , row - 1 , c , al_status ) results = [] for i in range ( row ): for j in range ( col ): if pa_status [ i ][ j ] == True and al_status [ i ][ j ] == True : results . append ([ i , j ]) return results def dfs ( self , heights , r , c , status ): if status [ r ][ c ] == True : return status [ r ][ c ] = True for direction in self . directions : r_new , c_new = r + direction [ 0 ], c + direction [ 1 ] if r_new >= 0 and r_new < len ( heights ) and c_new >= 0 and c_new < len ( heights [ 0 ]) and heights [ r_new ][ c_new ] >= heights [ r ][ c ]: self . dfs ( heights , r_new , c_new , status ) Backtracking 46. Permutations class Solution : def permute ( self , nums : List [ int ]) -> List [ List [ int ]]: results , track = [], [] self . backtrack ( nums , track , results ) return results def backtrack ( self , nums , track , results ): if len ( track ) == len ( nums ): results . append ( track . copy ()) return for num in nums : if num in track : continue track . append ( num ) self . backtrack ( nums , track , results ) track . remove ( num ) 77. Combinations 79. Word Search class Solution : \"\"\" Time: O(M*N) Space: O(M*N) \"\"\" def exist ( self , board : List [ List [ int ]], word : str ) -> bool : row , col = len ( board ), len ( board [ 0 ]) visited = [[ False for _ in range ( col )] for _ in range ( row )] for i in range ( row ): for j in range ( col ): if self . backtrack ( board , i , j , word , 0 , visited ): return True return False def backtrack ( self , board , i , j , word , index , visited ): if i < 0 or i >= len ( board ) or j < 0 or j >= len ( board [ 0 ]) or visited [ i ][ j ] == True or board [ i ][ j ] != word [ index ]: return False if index == len ( word ) - 1 and board [ i ][ j ] == word [ index ]: return True visited [ i ][ j ] = True # avoid visit agian res = self . backtrack ( board , i - 1 , j , word , index + 1 , visited ) \\ or self . backtrack ( board , i + 1 , j , word , index + 1 , visited ) \\ or self . backtrack ( board , i , j - 1 , word , index + 1 , visited ) \\ or self . backtrack ( board , i , j + 1 , word , index + 1 , visited ) visited [ i ][ j ] = False return res class Solution : \"\"\" Time: O(M*N) Space: O(1) \"\"\" def exist ( self , board : List [ List [ int ]], word : str ) -> bool : row , col = len ( board ), len ( board [ 0 ]) for i in range ( row ): for j in range ( col ): if self . backtrack ( board , i , j , word , 0 ): return True return False def backtrack ( self , board , i , j , word , index ): if i < 0 or i >= len ( board ) or j < 0 or j >= len ( board [ 0 ]): return False if board [ i ][ j ] == \"#\" or board [ i ][ j ] != word [ index ]: return False if index == len ( word ) - 1 and board [ i ][ j ] == word [ index ]: return True temp = board [ i ][ j ] board [ i ][ j ] = \"#\" # avoid visit agian res = self . backtrack ( board , i - 1 , j , word , index + 1 ) \\ or self . backtrack ( board , i + 1 , j , word , index + 1 ) \\ or self . backtrack ( board , i , j - 1 , word , index + 1 ) \\ or self . backtrack ( board , i , j + 1 , word , index + 1 ) board [ i ][ j ] = temp return res","tags":"Algorithms","url":"articles/Algorithms.html","loc":"articles/Algorithms.html"},{"title":"【 RL 】User Simulator","text":"User Simulator BackGround Rule-Based Simulator Design User Simulator BackGround 为什么需要用户模拟器？ 监督学习方法缺陷： 需要收集大量实际的人机与人人的训练标注数据，昂贵且耗时。 此外，即使有大量的训练数据，也有可能在训练数据中未充分探究某些对话状态空间，从而阻止了受监督的学习者找到好的策略。 强化学习优势： 不需要大量训练数据，仅给出奖励信号，智能体即可通过与用户交互来优化对话策略。 强化学习在对话领域存在的问题： 强化学习模型需要从环境中获取很多样本，但是从零开始与真实用户交互是不切实际的。所以需要用户模拟器模拟真实世界的环境。 用户模拟器： 用户模拟的目标是产生自然而合理的对话，从而使强化学习智能体能够探索策略空间。 基于模拟的方法允许代理探索先前观察到的数据中可能不存在的trajectory，克服了基于模拟的方法的主要局限性。 在用户模拟器上训练的智能体可以作为有效的起点，之后部署在真实世界与人互动，从而通过强化学习来进一步改进。 NLU ：自然语言 —-> 字典语义标签 DST ：对话历史追踪 DP ：选择对话动作 NLG ：对话动作 —-> 自然语言 对话系统回复（action）—-> 用户模拟器 用户模拟器与对话系统结构相似。 User goal：用户模拟器第一步首先生成对话目标。对话系统Agent对此目标不可知，但是需要通过多轮对话完成用户模拟器的目标。 e.g. user goal = inform_slots + request_slots： { \" request_slots \" : { \" ticket \" : \" UNK \" }, \" inform_slots \" : { \" city \" : \" seattle \" , \" numberofpeople \" : \" 2 \" , \" theater \" : \" regal meridian 16 \" , \" starttime \" : \" 9 : 25 pm \" , \" date \" : \" tomorrow \" , \" moviename \" : \" zoolander 2 \" } } 信息槽：slot=value。若干槽值对，是用户用于查询的约束。 请求槽：slot。若干槽，没有信息值，用户期望通过对话从对话系统方获取的信息值。 User model：用户模型对应对话系统的对话管理模块。它的任务是根据对话历史生成当前的用户动作，用户动作是预先定义好的语义标签，例如\"inform, request, greet, bye\"。 Paper: End-to-End Task-Completion Neural Dialogue Systems Rule-Based Simulator Agenda-Based Simulator: Plato Paper: Statistical User Simulation with a Hidden Agenda 在对话过程中，用户模拟器维护着一个紧凑的，类似堆栈的表示形式，称为用户议程Agenda，其中用户状态被分解为议程A和目标G。该目标G由约束C和请求R组成。在每个时间步t处，用户模拟器都会基于当前状态和上一个代理动作生成下一个用户动作，然后更新当前状态。 当用户模拟器收到输入时，它会参考Policy/ Rule以查看将哪些内容推送到议程Agenda中，作为对输入的响应。 经过一些整理后（例如，删除重复的内容或不再有效的内容），用户模拟器会将一个或多个项目从议程Agenda中弹出作为回复。 Design 用户模拟器维护每一轮的状态self.state，self.state具有字段request_slots, inform_slots, rest_slots, history_slots, turn, diaact Response for Request (System Action：request_slots) case1: 在目标信息槽中 system_action in goal.inform_slots 系统agent的问题槽在用户模拟器目标的信息槽中，1. 用目标信息槽值直接填self.state.inform_slots作为回复，2. 同时从状态self.state的剩余槽栈self.state.rest_slots中删除，3. 并清空self.state.request_slots状态的请求槽，因为已经确定为用户模拟器回答的陈述句。 case2: 在目标请求槽中，且在状态历史槽中，不在剩余槽中。 问题已经回答system_action in goal.request_slots and not in self.state.rest_slots and in self.history_slots 系统agent的问题槽在用户模拟器目标的请求槽中，并且在用户对话状态的历史槽中，不在剩余栈槽中。表示该问题已经回答，1. 从历史槽中取值构造回复即可。2. 清空self.state.request_slots状态的请求槽 case3: 在目标请求槽中，且在状态剩余槽中。 问题未曾回答system_action in goal.request_slots and not in self.state.rest_slots 在目标的请求槽中，并且在状态的剩余槽中， case4: 不在目标的请求槽和信息槽中 ，即不在用户模拟器goal中。 将当前对话状态的信息槽填值为：self.state.inform_slots.slots=dialog_config.I_DO_NOT_CARE并回复。并检查self.state的请求槽和剩余槽栈是否为空，设置对话状态。","tags":"NLP","url":"articles/【RL】User-Simulator.html","loc":"articles/【RL】User-Simulator.html"},{"title":"BackTracking","text":"BackTracking Permutation I/ II Subset Combination N Queens Word Search Ref. Backtracking BackTracking 回溯算法：本质是N叉树的遍历问题 ，关键就是在前序遍历和后序遍历的位置做一些操作。回溯算法框架： 路径：也就是已经做出的选择。 选择列表：也就是你当前可以做的选择。（一般会定义一个visited布尔数组，用于剪枝） 结束条件：也就是到达决策树底层，无法再做选择的条件。 result = [] def dfs ( 路径, 选择列表 ) : if 满足结束条件: result . append ( 路径 ) return for 选择 in 选择列表: 做选择 dfs ( 路径, 选择列表 ) 撤销选择 写dfs函数时，需要维护走过的 路径 和当前可以做的 选择列表 ，当触发 结束条件 时，将 路径 记入结果集。 Permutation I/ II #include <string> #include <vector> #include <algorithm> #include <iostream> class Solution { private : std :: vector < std :: string > res ; std :: string track ; public : std :: vector < std :: string > permutation ( std :: string s ) { if ( s . empty ()){ return res ; } std :: vector < bool > visited ( s . size (), false ); std :: sort ( s . begin (), s . end ()); dfs ( res , track , s , visited ); return res ; } void dfs ( std :: vector < std :: string >& res , \\ std :: string & track , \\ std :: string & s , \\ std :: vector < bool >& visited ){ if ( track . size () == s . size ()){ res . push_back ( track ); } for ( int i = 0 ; i < s . size (); i ++ ){ if ( visited [ i ]){ continue ; } if ( i > 0 && visited [ i -1 ] && s [ i -1 ] == s [ i ]){ continue ; } visited [ i ] = true ; track . push_back ( s [ i ]); dfs ( res , track , s , visited ); track . pop_back (); visited [ i ] = false ; } } }; Subset Combination N Queens class Solution { /* In this problem, we can go row by row, and in each position, we need to check * if the column, the 45° diagonal and the 135° diagonal had a queen before. * Solution: Directly check the validity of each position using nQueens. */ public : vector < vector < string > > solveNQueens ( int n ) { vector < vector < string > > res ; vector < string > nQueens ( n , string ( n , '.' )); solveNQueens ( res , nQueens , 0 , n ); return res ; } private : void solveNQueens ( vector < vector < string > >& res , \\ vector < string >& nQueens , \\ int row , int & n ) { if ( row == n ) { res . push_back ( nQueens ); return ; } for ( int col = 0 ; col != n ; ++ col ) if ( isValid ( nQueens , row , col , n )) { nQueens [ row ][ col ] = 'Q' ; solveNQueens ( res , nQueens , row + 1 , n ); nQueens [ row ][ col ] = '.' ; } } bool isValid ( vector < string >& nQueens , int row , int col , int & n ) { //check if the column had a queen before. for ( int i = 0 ; i != row ; ++ i ) if ( nQueens [ i ][ col ] == 'Q' ) return false ; //check if the 45° diagonal had a queen before. for ( int i = row - 1 , j = col - 1 ; i >= 0 && j >= 0 ; -- i , -- j ) if ( nQueens [ i ][ j ] == 'Q' ) return false ; //check if the 135° diagonal had a queen before. for ( int i = row - 1 , j = col + 1 ; i >= 0 && j < n ; -- i , ++ j ) if ( nQueens [ i ][ j ] == 'Q' ) return false ; return true ; } }; Word Search #include <vector> #include <iostream> using std :: vector ; using std :: string ; class Solution { private : int row , col ; const int delta [ 4 ][ 2 ]{{ -1 , 0 }, { 1 , 0 }, { 0 , -1 }, { 0 , 1 }}; public : bool exist ( vector < vector < char >>& board , string word ) { if ( board . empty () || word . empty ()){ return false ; } row = board . size (); col = board [ 0 ]. size (); vector < vector < bool >> visted ( row , vector < bool > ( col , false )); for ( int i = 0 ; i < row ; i ++ ){ for ( int j = 0 ; j < col ; j ++ ){ // 如果搜到直接返回，否则继续搜索 if ( dfs ( i , j , board , word , visted , 0 )){ return true ; } } } return false ; } bool dfs ( int x , \\ int y , \\ vector < vector < char >>& board , \\ string word , \\ vector < vector < bool >>& visted , \\ int index ){ if ( index == word . size () -1 ){ return board [ x ][ y ] == word . at ( index ); } if ( board [ x ][ y ] == word . at ( index )){ visted [ x ][ y ] = true ; // 分别从四个方向进行搜索 for ( int i = 0 ; i < 4 ; i ++ ){ int newRow = x + delta [ i ][ 0 ]; int newCol = y + delta [ i ][ 1 ]; if ( checkValid ( newRow , newCol , visted ) && \\ dfs ( newRow , newCol , board , word , visted , index + 1 )){ return true ; } } // 当前点(x, y)的四个方向都没搜到， // 回溯需要重置visted[x][y]为false，用于其他位置开始查询。 visted [ x ][ y ] = false ; } return false ; } bool checkValid ( int x , int y , vector < vector < bool >>& visted ){ if ( x >= 0 && x < row && y >= 0 && y < col ){ return visted [ x ][ y ] == false ; } return false ; } };","tags":"Algorithms","url":"articles/BackTracking.html","loc":"articles/BackTracking.html"},{"title":"NLP Attention","text":"Context attention - LSTM Self attention Masked self attention Self Attention Cross Attention - Transformer http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention Context attention - LSTM Encoder and Decoder class Attention ( nn . Module ): def __init__ ( self , method , hidden_size ): super ( Attention , self ) . __init__ () self . method = method self . hidden_size = hidden_size if self . method == 'general' : self . attention = nn . Linear ( self . hidden_size , self . hidden_size ) elif self . method == 'concat' : self . attention = nn . Linear ( self . hidden_size * 3 , self . hidden_size ) self . v = nn . Parameter ( nn . init . normal_ ( torch . empty ( self . hidden_size ))) def forward ( self , hidden , encoder_outputs ): attention_energies = self . score ( hidden , encoder_outputs ) attention_energies = attention_energies . t () return F . softmax ( attention_energies , dim = 1 ) . unsqueeze ( 1 ) def score ( self , hidden , encoder_output ): if self . method == 'dot' : energy = hidden . dot ( encoder_output ) return energy elif self . method == 'general' : energy = self . attention ( encoder_output ) energy = hidden . dot ( energy ) return energy elif self . method == 'concat' : encoder_output = encoder_output . transpose ( 0 , 1 ) energy = self . attention ( torch . cat (( hidden . expand ( encoder_output . size ( 0 ), - 1 , - 1 ), encoder_output ), 2 )) . tanh () return torch . sum ( self . v * energy , dim = 2 ) # attention by jerry self . attention = Attention ( attn_model , hidden_size ) # Calculate attention weights from the current RNN last hidden output attn_weights = self . attention ( last_hidden . unsqueeze ( 0 ), encoder_outputs ) # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector context = attn_weights . bmm ( encoder_outputs ) # Concatenate weighted context vector and GRU output using Luong eq. 5 last_hidden = last_hidden . squeeze ( 0 ) context = context . squeeze ( 1 ) concat_input = torch . cat (( last_hidden , context ), 1 ) concat_output = torch . tanh ( self . concat ( concat_input )) Self attention Encoder or Decoder Masked self attention Decoder padding masked & sequence masked Self Attention Cross Attention - Transformer from torch import nn , Tensor from typing import Dict , List , Optional , Tuple class Attention ( nn . Module ): \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\" def __init__ ( self , embed_dim , num_heads , dropout = 0.0 , bias = True , encoder_decoder_attention = False , # otherwise self_attention ): super () . __init__ () self . embed_dim = embed_dim self . num_heads = num_heads self . dropout = dropout self . head_dim = embed_dim // num_heads assert self . head_dim * num_heads == self . embed_dim , \"embed_dim must be divisible by num_heads\" self . scaling = self . head_dim ** - 0.5 self . encoder_decoder_attention = encoder_decoder_attention self . k_proj = nn . Linear ( embed_dim , embed_dim , bias = bias ) self . v_proj = nn . Linear ( embed_dim , embed_dim , bias = bias ) self . q_proj = nn . Linear ( embed_dim , embed_dim , bias = bias ) self . out_proj = nn . Linear ( embed_dim , embed_dim , bias = bias ) self . cache_key = \"encoder_decoder\" if self . encoder_decoder_attention else \"self\" def _shape ( self , tensor , seq_len , bsz ): \"\"\"多头注意力，隐层维度reshap \"\"\" return tensor . contiguous () . view ( seq_len , bsz * self . num_heads , self . head_dim ) . transpose ( 0 , 1 ) def forward ( self , query , key : Tensor , key_padding_mask : Optional [ Tensor ] = None , layer_state : Optional [ Dict [ str , Tensor ]] = None , attn_mask : Optional [ Tensor ] = None , output_attentions = False , ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Input shape: Time(SeqLen) x Batch x Channel\"\"\" static_kv : bool = self . encoder_decoder_attention tgt_len , bsz , embed_dim = query . size () # get here for encoder decoder cause of static_kv if layer_state is not None : # reuse k,v and encoder_padding_mask saved_state = layer_state . get ( self . cache_key , {}) if \"prev_key\" in saved_state and static_kv : # previous time steps are cached - no need to recompute key and value if they are static key = None else : # this branch is hit by encoder saved_state = None # Scale q to prevent the dot product between q and k from growing too large. q = self . q_proj ( query ) * self . scaling if static_kv and key is None : # cross-attention with cache k = v = None elif static_kv and key is not None : # cross-attention no prev_key found in cache k = self . k_proj ( key ) v = self . v_proj ( key ) else : # self-attention k = self . k_proj ( query ) v = self . v_proj ( query ) q = self . _shape ( q , tgt_len , bsz ) if k is not None : k = self . _shape ( k , - 1 , bsz ) if v is not None : v = self . _shape ( v , - 1 , bsz ) if saved_state : k , v = self . _concat_saved_state ( k , v , saved_state , static_kv , bsz ) # Update cache if isinstance ( layer_state , dict ): cached_shape = ( bsz , self . num_heads , - 1 , self . head_dim ) # bsz must be first for reorder_cache layer_state [ self . cache_key ] = dict ( prev_key = k . view ( * cached_shape ), prev_value = v . view ( * cached_shape )) src_len = k . size ( 1 ) assert key_padding_mask is None or key_padding_mask . shape == ( bsz , src_len ) # 注意力矩阵，即query与key的求出的矩阵系数 attn_weights = torch . bmm ( q , k . transpose ( 1 , 2 )) assert attn_weights . size () == ( bsz * self . num_heads , tgt_len , src_len ) if attn_mask is not None : attn_weights = attn_weights . view ( bsz , self . num_heads , tgt_len , src_len ) + attn_mask attn_weights = attn_weights . view ( bsz * self . num_heads , tgt_len , src_len ) # Note: deleted workaround to get around fork/join parallelism not supporting Optional types. on 2020/10/15 if key_padding_mask is not None : # don't attend to padding symbols attn_weights = attn_weights . view ( bsz , self . num_heads , tgt_len , src_len ) reshaped = key_padding_mask . unsqueeze ( 1 ) . unsqueeze ( 2 ) attn_weights = attn_weights . masked_fill ( reshaped , float ( \"-inf\" )) attn_weights = attn_weights . view ( bsz * self . num_heads , tgt_len , src_len ) attn_weights = F . softmax ( attn_weights , dim =- 1 ) attn_probs = F . dropout ( attn_weights , p = self . dropout , training = self . training ) assert v is not None attn_output = torch . bmm ( attn_probs , v ) assert attn_output . size () == ( bsz * self . num_heads , tgt_len , self . head_dim ) attn_output = attn_output . transpose ( 0 , 1 ) . contiguous () . view ( tgt_len , bsz , embed_dim ) attn_output = self . out_proj ( attn_output ) if output_attentions : attn_weights = attn_weights . view ( bsz , self . num_heads , tgt_len , src_len ) else : attn_weights = None return attn_output , attn_weights","tags":"NLP","url":"articles/NLP-Attention.html","loc":"articles/NLP-Attention.html"},{"title":"【 NLP 】 BERT","text":"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time","tags":"NLP","url":"articles/【NLP】BERT.html","loc":"articles/【NLP】BERT.html"},{"title":"【 NLP 】ELMo","text":"ELMo: Embeddings from Language models, BiLSTM vector concat，weighed hidden layers stacked 与GloVe embedding最大区别，引入了上下文，contextualized word-embeddings ( BERT , ELMo) ELMo不会为每个单词使用固定的embedding向量，而是会在为每个单词分配embedding之前先查看整个句子(上下文)。 http://jalammar.github.io/illustrated-bert","tags":"NLP","url":"articles/【NLP】ELMo.html","loc":"articles/【NLP】ELMo.html"},{"title":"【 RL 】Policy Gradient","text":"1. Reinforcement Learning 2. Deep Learning 1. Reinforcement Learning Actor(Policy) Neural Network as Actor (Deep). vs lookup Table(Q Learning). 使用神经网络作为Actor比查表的优势？ 查表无法穷举输入，e.g.图像画面或者语言输入。NN泛化性比较强，对于未看过的Observation，举一反三，合理的输出。 Environment Reward 2. Deep Learning 如何选取Actor？ Neural Network as Actor (Deep) 如何衡量Actor的好坏？ Maxmize Reward 的期望。Reward是一个回合 episode ，每轮Reward的总和。由于Actor是stochastic随机的，每个回合的Reward不同。所以maxmize sampling N回合Reward的期望。 期望就衡量了Actor","tags":"NLP","url":"articles/【RL】Policy-Gradient.html","loc":"articles/【RL】Policy-Gradient.html"},{"title":"【 NLP 】 RASA Issues","text":"1. Conversation Design 2. Stories 3. Data Augmentation 4. Policy 1. Conversation Design Dialogue Elements intent and goal are easily confused. 2. Stories rasa-stories rasa-core-understanding-stories 3. Data Augmentation rasa-data-augmentation data-augmentation 4. Policy Transformer Embedding Dialogue ( TED ) Policy Paper: Dialogue Transformers","tags":"NLP","url":"articles/【NLP】RASA-Issues.html","loc":"articles/【NLP】RASA-Issues.html"},{"title":"CNN","text":"(batch_size, height, width, channel) = (200, 32, 32, 256) 卷积核，用于提取特定特征，由卷积核决定。网络学习的参数也就是卷积核的参数，所以相比于全连接网络大大减少了参数。滑动卷积核在图片上提取相应特征。 通道数等于卷积核数，每个通道参数共享，负责特定特征提取。与RGB颜色三通道区别开来。 台大李宏毅 Why convolution ? Andrew Ng CNN &TextCNN","tags":"ML","url":"articles/CNN.html","loc":"articles/CNN.html"},{"title":"Deploy my blog quickly","text":"1. Clone Blog 2. Create virtual environment 3. Install themes & plugins download and install theme elegant download pelican plugins 4. Deploy Blog 1. Clone Blog $ git clone git@github.com:jerrylsu/blog.git 2. Create virtual environment $ conda create -n blog python = 3 .6.8 $ conda activate blog $ pip install pelican -i https://pypi.doubanio.com/simple $ pip install bs4 markdown webassets cssmin -i https://pypi.doubanio.com/simple 3. Install themes & plugins The directory of themes and plugins can be pulled by pelican site. download and install theme elegant github.com/getpelican/pelican-themes $ cd /blog/themes $ git clone git@github.com:Pelican-Elegant/elegant.git $ pelican-themes --install themes/elegant --verbose download pelican plugins github.com/getpelican/pelican-plugins $ cd blog $ git clone git@github.com:getpelican/pelican-plugins.git $ mv peliacn-plugins plugins 4. Deploy Blog $ mkdir output # Binding domain name. $ touch output/CNAME $ echo 'www.jerrulsu.com' >> output/CNAME # Generate automatically deployment files into output directory # and deploy blog to github.com:jerrylsu/jerrylsu.github.io.git. # if use windows conda environment on cygwin, you must use command # python -i ./cmder p $ ./cmder p","tags":"Tools","url":"articles/Deploy-my-blog-quickly.html","loc":"articles/Deploy-my-blog-quickly.html"},{"title":"【 NLP 】 RASA Dialogue Transformers","text":"Dialogue Transformers 基于transformer架构的dialogue level的对话策略，其中自注意力机制关注多轮对话序列。RNN网络结构是在对整个对话序列进行编码，故而认为历史序列的每一轮对话turn均是相关的。尽管复杂LSTM结构对RNN进行了改进，可以对历史信息选择性遗忘，然后这需要有足够的训练数据。基于这些，在训练数据有限和多主题对话的场景（一组对话中会交叠多种对话片段）下，网络无法解决这些问题。而transformer结构会选取哪些轮参与当前轮对话状态的编码，自然而然的忽略无关和选取相关历史对话轮，有效克服了RNN结构的限制以LSTM对大训练数据的依赖。 基于自注意力机制的transformer结构，对开放域多主题的对话有明显的优势。在对编码的level上，NLP通常是基于单轮的token序列进行编码，而rasa的自注意力机制是对整个对话序列编码。 e.g. ## Generated Story case-0-568983 * inform_face_name{\"e_face_name_2\": \"Jerry\"} - action_set_face_name - slot{\"s_face_name_1\": \"Jerry\"} 对于Transformer encode网络的bert模型来说，上例作为一条训练样本的story，共有三轮对话且具有时序关系，即3 time steps。每一个time step的vector有用户输入（意图和实体） + 槽 + 上一轮的Action，每一个time step同时对应一个action的预测vector。区别与原始的bert双向自注意力机制，该策略中使用单向注意力，即关注当前轮之前的对话历史信息。 Transformer embedding策略体系结构，包括以下步骤： 将用户输入（用户意图和实体），每个时间步骤的先前系统动作，槽concatenate为输入向量输入embedding层； 送入transformer； 对transformer的输出应用一个全连接层，获得这个时间步对话embedding向量； 在每个时间步上对全部的系统actions创建embedding向量； 计算对话embedding向量和系统actions embedding向量之间的相似度，即loss。参见StarSpace的论文。 Dialogue_features作为输入特征，总共0，1，2三轮对话。FullDialoguePolicy默认参照最长story进行-1 padding，即输入4的作用。 Bot_features作为每一轮预测的输出特征，同样-1 padding并最终默认转换为action_listen向量。 对于多主题自注意力机制矩阵的测试结果： 如上图自注意力矩阵所示，纵轴为被预测的对话轮，横轴为policy所关注的对话历史。由于使用单向transformer结构，矩阵的上三角0掩码，以确保不会去关注未来的对话轮。图示可以证明学习的注意力权重很容易解释并反映对话逻辑，在每一轮的对话中，与当前预测相关的对话历史有较高的注意力权重。 References: https://arxiv.org/abs/1910.00486 https://github.com/jerrylsu/learning/tree/master/nlp/rasa/featurizers","tags":"NLP","url":"articles/【NLP】RASA-Dialogue-Transformers.html","loc":"articles/【NLP】RASA-Dialogue-Transformers.html"},{"title":"【 NLP 】 RASA Featurizer","text":"训练集stories如何构建状态state作为训练输入数据？ 构建的状态state作为输入X如何编码？ 输出y是什么？如何编码？ https://rasa.com/docs/rasa/api/core-featurization/ Dialogue Transformers Transformer policy是对dialogue turn level编码，而不是token level编码。 1.1 输入特征X编码 1.1.1 用户输入和系统动作词典构建 1.1.2 encode——词袋模型 1.2 输出Y编码 FullDialogueTrackerFeaturizer 训练集stories如何构建状态state作为训练输入数据？ 构建的状态state作为输入X如何编码？ 输出y是什么？如何编码？ https://rasa.com/docs/rasa/api/core-featurization/ Dialogue Transformers LabelTokenizerSingleStateFeaturizer creates a vector based on the feature label: All active feature labels (e.g. prev_action_listen) are split into tokens and represented as a bag-of-words. For example, actions utter_explain_details_hotel and utter_explain_details_restaurant will have 3 features in common, and differ by a single feature indicating a domain. Labels for user inputs (intents, entities) and bot actions are featurized separately. Each label in the two categories is tokenized on a special character split_symbol (e.g. action_search_restaurant = {action, search, restaurant}), creating two vocabularies. A bag-of-words representation is then created for each label using the appropriate vocabulary. The slots are featurized as binary vectors, indicating their presence or absence at each step of the dialogue. Transformer policy是对dialogue turn level编码，而不是token level编码。 we apply self-attention at the discourse level, attending over the sequence of dialogue turns rather than the sequence of tokens in a single turn. 一条训练数据是，X个dialogue turns和Y个action_names。 1.1 输入特征X编码 1.1.1 用户输入和系统动作词典构建 prepare_from_domain Creates internal vocabularies for user intents and bot actions to use for featurization user_labels = [] slot_labels = [] bot_labels = [] bot_vocab = None user_vocab = None 只有slot_labels没有生成字典，使用列表[‘slot_s_answer_error_0'…]，也没有split(‘_') bot_labels = [ 'action_listen' , 'action_restart' , 'utter_ask_is_staff' , 'utter_ask_visitor_reserve' ] distinct_tokens = set ([ token for label in bot_labels for token in label . split ( '_' )]) bot_vocab = { token : idx for idx , token in enumerate ( sorted ( distinct_tokens ))} bot_vocab {'action': 0, 'ask': 1, 'is': 2, 'listen': 3, 'reserve': 4, 'restart': 5, 'staff': 6, 'utter': 7, 'visitor': 8} user_labels = [ 'intent_ask_human_service' , 'intent_bye' , 'intent_chitchat' , 'intent_confirm' ] distinct_tokens = set ([ token for label in user_labels for token in label . split ( '_' )]) user_vocab = { token : idx for idx , token in enumerate ( sorted ( distinct_tokens ))} user_vocab {'ask': 0, 'bye': 1, 'chitchat': 2, 'confirm': 3, 'human': 4, 'intent': 5, 'service': 6} slot_labels = [ 'slot_s_answer_error_0' , 'slot_s_chitchat_turn_0' , 'slot_s_digits_key_0' , 'slot_s_host_name_0' , 'slot_s_is_call_human_0' , 'slot_s_is_call_human_1' , 'slot_s_is_chitchat_0' , 'slot_s_is_chitchat_1' , 'slot_s_is_reserve_visitor_0' , 'slot_s_is_reserve_visitor_1' , 'slot_s_is_same_name_0' , 'slot_s_is_same_name_1' , 'slot_s_is_staff_0' , 'slot_s_is_staff_1' , 'slot_s_is_valid_info_0' , 'slot_s_is_valid_info_1' , 'slot_s_is_valid_reserve_0' , 'slot_s_is_valid_reserve_1' , 'slot_s_is_valid_staff_0' , 'slot_s_is_valid_staff_1' , 'slot_s_is_visitor_0' , 'slot_s_is_visitor_1' , 'slot_s_phone_number_0' , 'slot_s_self_name_0' , 'slot_s_staff_homophonic_name_0' , 'slot_s_title_name_0' , 'slot_s_visitor_homophonic_name_0' ] 1.1.2 encode——词袋模型 总特征向量维度：num_features = 用户输入（意图和实体）字典维度 + 槽维度 + 系统action字典维度 user_feature_len = len ( user_vocab ) print ( \"user feature len: {} \" . format ( user_feature_len )) slot_feature_len = len ( slot_labels ) print ( \"slot feature len: {} \" . format ( slot_feature_len )) bot_feature_len = len ( bot_vocab ) print ( \"bot feature len: {} \" . format ( bot_feature_len )) num_features = len ( user_vocab ) + len ( slot_labels ) + len ( bot_vocab ) print ( \"num_feature = user vocab + slot labels + bot vocab: {} \" . format ( num_features )) user feature len: 7 slot feature len: 27 bot feature len: 9 num_feature = user vocab + slot labels + bot vocab: 43 # 特征化向量，固定长度。 import numpy as np used_features = np . zeros ( num_features , dtype = float ) used_features array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) # 通过if判断状态属于用户、槽、还是系统动作，以确定在向量num_features中的偏移量offset。 # 用户的意图实体 + 槽 + 系统动作 # 输入的状态state（包括用户输入的意图实体、槽、系统动作） state = { 'entity_e_phone_number' : 1.0 , 'intent_deny+inform_name_self+inform_phone_number_self' : 1.0 , 'prev_action_listen' : 1.0 , 'entity_e_name' : 1.0 , 'entity_e_four_digits' : 1.0 } def split_state_name ( state_name : str ): \"\"\"Split multiple intents with '+' and '_'. Add the string of 'intent'. e.g. \"intent_deny+inform_name_self+inform_phone_number_self\" -> ['intent', 'deny', 'inform', 'name', 'self', 'inform', 'phone', 'number', 'self', 'intent', 'intent'] \"\"\" intents = state_name . split ( '+' ) intents_num , res = len ( intents ), [] for words in intents : res . extend ( words . split ( '_' )) for _ in range ( intents_num - 1 ): res . append ( 'intent' ) return res used_features = np . zeros ( num_features , dtype = float ) idx = 0 PREV_PREFIX = \"prev_\" for state_name , prob in state . items (): idx += 1 print () print ( 'state name {} : {} ' . format ( idx , state_name )) if state_name in user_labels : # 用户输入编码入向量used_features #for t in [word for words in state_name.split('+') for word in words.split('_')]: # Bingo!!! for t in split_state_name ( state_name ): #for t in state_name.split('_'): idx = user_vocab [ t ] used_features [ idx ] += prob print ( t ) print ( used_features ) elif state_name in slot_labels : # 槽编码入向量used_features offset = len ( user_vocab ) idx = slot_labels . index ( state_name ) used_features [ offset + idx ] += prob elif state_name [ len ( PREV_PREFIX ) : ] in bot_labels : # 系统动作编码入向量used_features action_name = state_name [ len ( PREV_PREFIX ) :] for t in action_name . split ( '_' ): offset = len ( user_vocab ) + len ( slot_labels ) idx = bot_vocab [ t ] used_features [ offset + idx ] += prob print ( t ) print ( used_features ) state name 1: entity_e_phone_number entity [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] e [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] phone [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] number [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] state name 30: intent_deny+inform_name_self+inform_phone_number_self intent [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] deny [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] inform [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] name [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] self [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] inform [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] phone [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] number [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] self [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] intent [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 2. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] intent [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 3. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] state name 26: prev_action_listen action [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 3. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] listen [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 2. 3. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] state name 33: entity_e_name entity [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 2. 3. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] e [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 2. 3. 0. 0. 1. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] name [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 2. 3. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] state name 29: entity_e_four_digits entity [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 3. 0. 0. 0. 0. 0. 2. 3. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] e [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 3. 3. 0. 0. 0. 0. 0. 2. 3. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] four [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 3. 3. 1. 0. 0. 0. 0. 2. 3. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] digits [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 3. 3. 1. 0. 0. 0. 0. 2. 3. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] state_nam = 'intent_deny+inform_name_self' def split_state_name ( state_name : str ): intents = state_name . split ( '+' ) intents_num = len ( intents ) res = [] for words in intents : res . extend ( words . split ( '_' )) for _ in range ( intents_num - 1 ): res . append ( 'intent' ) return res split_state_name ( state_nam ) ['intent', 'deny', 'inform', 'name', 'self', 'intent'] [ word for words in state_name . split ( '+' ) for word in words . split ( '_' )] ['entity', 'e', 'four', 'digits'] if PREV_PREFIX + ACTION_LISTEN_NAME in state: 前一轮是action_listen,才对用户的输入（即，意图和实体）编码。而后，几轮虽然包含前几轮信息，但对信息中的用户输入不再编码 X 的shape=（1，4，153）1th维是story数，2维是对话轮数，3维是每轮的编码vector长度。固定长度，有padding 1.2 输出Y编码 对action进行one-hot编码。 ‘action_listen' —-> ‘1,0,0,…,0,0,0' (62,)维向量 构造bot_action字典，\" \"拆分。 ‘action_listen' —-> ‘1, 0, 0, .., 1, …, 0, 0' (62, 72)维矩阵。62个actions，每个action用72维表示，72是' ‘拆分所有action_names后构成的字典数。 one-hot向量转bot_action字典向量。np.argmax(onehot vector)获取索引, 查(62, 72)矩阵。 Y 最终的输出维度是(1dim, 2dim, 3dim) = (1, 4, 72): 1 story, 4 turns dialogue, 72 vector of action dict。对于FullDialogueTrackerFeaturizer,第2个维度是对话轮数大小，按照最长story长度padding -1, 用-1补齐的轮对应的标签是action_listen。 FullDialogueTrackerFeaturizer 和 MaxHistoryTrackerFeaturizer 的区别就是再对2nd维度上的控制， if y . ndim == 3 and isinstance ( self , MaxHistoryTrackerFeaturizer ) : # if it is MaxHistoryFeaturizer , remove time axis y = y [:, 0 , :] ## Generated Story case-0-56898 * inform_face_name{\"e_face_name_1\": \"张春梅\"} - action_set_face_name - slot{\"s_face_name_1\": \"张春梅\"} - utter_goodbye # domain action_names = [ 'action_listen' , 'action_restart' , 'action_session_start' , 'action_default_fallback' , 'action_deactivate_form' , 'action_revert_fallback_events' , 'action_default_ask_affirmation' , 'action_default_ask_rephrase' , 'action_back' , 'action_confirm_homophonic_name' , 'action_judge_same_name_order_or_department_validity' , 'action_match_answer_info' , 'action_match_reserve_visitor_name' , 'action_match_staff_name' , 'action_set_face_name' , 'action_set_full_name' , 'action_set_host_name' , 'action_set_reality_name_by_face' , 'action_set_reality_name_by_joint' , 'action_set_same_name_order_or_department' , 'action_set_self_name' , 'action_set_user_domain' , 'action_set_user_info' , 'utter_ask_is_staff' , 'utter_ask_staff_digits_key' , 'utter_ask_staff_digits_key_again' , 'utter_ask_staff_name' , 'utter_ask_staff_name_and_digits_key' , 'utter_ask_user_is_staff' , 'utter_ask_visitor_host' , 'utter_ask_visitor_host_full_name' , 'utter_ask_visitor_name' , 'utter_ask_visitor_name_and_phone' , 'utter_ask_visitor_phone' , 'utter_ask_visitor_phone_again' , 'utter_ask_visitor_phone_error' , 'utter_ask_visitor_phone_error_again' , 'utter_ask_visitor_phone_no_pass' , 'utter_ask_visitor_reserve' , 'utter_call_human_service' , 'utter_chitchat' , 'utter_chitchat_again' , 'utter_chitchat_once_again' , 'utter_confirm_host_name' , 'utter_confirm_staff_name' , 'utter_confirm_visitor_name' , 'utter_correct_answer' , 'utter_default' , 'utter_error_authentication' , 'utter_error_catch' , 'utter_goodbye' , 'utter_greet_short' , 'utter_help_find_people' , 'utter_staff_digits_key_error' , 'utter_staff_digits_key_error_again' , 'utter_staff_inform_no_collection' , 'utter_staff_invalid_name' , 'utter_staff_welcome' , 'utter_visitor_lack_host_information' , 'utter_visitor_lack_reserve_name' , 'utter_visitor_wait' , 'utter_wrong_answer' ] bot_vocab = { 'action' : 0 , 'affirmation' : 1 , 'again' : 2 , 'and' : 3 , 'answer' : 4 , 'ask' : 5 , 'authentication' : 6 , 'back' : 7 , 'by' : 8 , 'call' : 9 , 'catch' : 10 , 'chitchat' : 11 , 'collection' : 12 , 'confirm' : 13 , 'correct' : 14 , 'deactivate' : 15 , 'default' : 16 , 'department' : 17 , 'digits' : 18 , 'domain' : 19 , 'error' : 20 , 'events' : 21 , 'face' : 22 , 'fallback' : 23 , 'find' : 24 , 'form' : 25 , 'full' : 26 , 'goodbye' : 27 , 'greet' : 28 , 'help' : 29 , 'homophonic' : 30 , 'host' : 31 , 'human' : 32 , 'info' : 33 , 'inform' : 34 , 'information' : 35 , 'invalid' : 36 , 'is' : 37 , 'joint' : 38 , 'judge' : 39 , 'key' : 40 , 'lack' : 41 , 'listen' : 42 , 'match' : 43 , 'name' : 44 , 'no' : 45 , 'once' : 46 , 'or' : 47 , 'order' : 48 , 'pass' : 49 , 'people' : 50 , 'phone' : 51 , 'reality' : 52 , 'rephrase' : 53 , 'reserve' : 54 , 'restart' : 55 , 'revert' : 56 , 'same' : 57 , 'self' : 58 , 'service' : 59 , 'session' : 60 , 'set' : 61 , 'short' : 62 , 'staff' : 63 , 'start' : 64 , 'user' : 65 , 'utter' : 66 , 'validity' : 67 , 'visitor' : 68 , 'wait' : 69 , 'welcome' : 70 , 'wrong' : 71 } num_actions = len ( action_names ) num_actions 62 trackers_as_actions = [ 'action_listen' , 'action_set_face_name' , 'utter_goodbye' , 'action_listen' ] ['action_listen', 'action_set_face_name', 'utter_goodbye', 'action_listen'] # Encode system action as one-hot vector. labels = [] # Multi_story def action_as_one_hot ( action ): y = np . zeros ( num_actions , dtype = int ) index_for_action = action_names . index ( action ) #print(f'Index for action: {index_for_action}') y [ index_for_action ] = 1 return y #print(f'One-hot of y: {y}\\n y shape: {y.shape}') story_labels = [ action_as_one_hot ( action ) for action in trackers_as_actions ] # one story and multi_turns labels . append ( story_labels ) y = np . array ( labels ) y . shape # (1dim, 2dim, 3dim) = (1, 4, 62) 1 story, 4 turns dialogue, 62 one-hot vector of action (1, 4, 62) # Create matrix with all actions from domain encoded in rows as bag of words def create_encoded_all_actions () -> np . ndarray : encoded_all_actions = np . zeros (( num_actions , len ( bot_vocab )), dtype = np . int32 ) for idx , name in enumerate ( action_names ): for t in name . split ( '_' ): encoded_all_actions [ idx , bot_vocab [ t ]] = 1 return encoded_all_actions encoded_all_label_ids = create_encoded_all_actions () encoded_all_label_ids . shape # 62 num of action, 72 bot vocab size (62, 72) # extract actual training data to feed to tf session print ( y . shape ) y # One-hot encode. (1, 4, 62): 1 story, 4 turns dialogue(one turn is one action), 62 vector of action (1, 4, 62) array([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]) label_ids = y . argmax ( - 1 ) label_ids array([[ 0, 14, 50, 0]]) d = [ - 1 , - 1 , - 1 ] d = np . array ( d ) d . argmax ( - 1 ) 0 label_ids . shape (1, 4) # full dialogue res = [] for seq_label_ids in label_ids : for label_idx in seq_label_ids : res . append ( encoded_all_label_ids [ label_idx ]) res [array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int32), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)] res = np . stack ( res ) res array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32) label_ids . shape # explicitly add last dimension to label_ids to track correctly dynamic sequences (1, 4) label_ids = np . expand_dims ( label_ids , - 1 ) label_ids array([[[ 0], [14], [50], [ 0]]]) FullDialogueTrackerFeaturizer # Training data is padded up to the length of the longest dialogue with -1. ! jupyter nbconvert -- to markdown featurizer . ipynb [ NbConvertApp ] Converting notebook featurizer . ipynb to markdown [ NbConvertApp ] Writing 27001 bytes to featurizer . md","tags":"NLP","url":"articles/RASA-Featurizer.html","loc":"articles/RASA-Featurizer.html"},{"title":"【 NLP 】Embedding","text":"embedding_dim=16 model = keras.Sequential([ layers.Embedding(vocab_size, embedding_dim, input_length=maxlen), layers.GlobalAveragePooling1D(), layers.Dense(16, activation='relu'), layers.Dense(1, activation='sigmoid') ]) Embedding: This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. Embedding层的任务： 定义矩阵，大小为[vocab_size, embedding_dim] 对于idx后的文本向量[102, 33, 41, …]，查矩阵，将idx -> embedding向量。最后将句子表示为[max_len, embedding_size]的矩阵。其中max_len是做了padding补全或者截断操作，变长文本->定长。 模型训练时是以batch_size为单位的，所以embedding层最终的输出是：batch_size * max_len * embedding_dim的3维矩阵。 embedding层输出的数据可以做 GlobalAveragePooling1D 合并。","tags":"NLP","url":"articles/【NLP】Embedding.html","loc":"articles/【NLP】Embedding.html"},{"title":"【 NLP 】GlobalAveragePooling1D","text":"embedding_dim=16 model = keras.Sequential([ layers.Embedding(vocab_size, embedding_dim, input_length=maxlen), layers.GlobalAveragePooling1D(), layers.Dense(16, activation='relu'), layers.Dense(1, activation='sigmoid') ]) GlobalAveragePooling1D: return a fixed-length output vector for each example by averaging over the steps dimension. This allows the model to handle input of variable length, in the simplest way possible. Embedding 输出的文本数据batch_size * max_len * embedding_size的 合并 -> 全局平均池化操作GlobalAveragePooling1D ： 输入数据：(batch-size, steps, features)。是经过embedding层的稠密矩阵，steps是文本中tokens的个数（变长），features是embedding-dim的维度。 输出数据：(batch-size, features)。对于每一个feature map即一条文本语句的embedding矩阵，按照steps方向求平均，embedding矩阵被池化为embedding-dim维度的向量，来表示本条文本语句。 优点：解决本文语句的变长问题 缺点： 信息损失，由于是均值降维且padding噪音稀释数据。 无效计算过多，由于padding。 解决方法：循环神经网络RNN","tags":"NLP","url":"articles/【NLP】GlobalAveragePooling1D.html","loc":"articles/【NLP】GlobalAveragePooling1D.html"},{"title":"Cross Validation- CV","text":"CV要解决的问题是什么？ K折交叉验证 Cross Validation CV要解决的问题是什么？ 当评价模型的不同设置（\"hyperparameters(超参数)\"）时， 由于在训练集上，通过调整参数设置使模型的性能达到了最佳状态；但在测试集上 可能会出现过拟合的情况。 此时，测试集上的信息反馈足以颠覆训练好的模型，评估的指标不再有效反映出模型的泛化性能。 为了解决此类问题，还应该准备另一部分被称为 \"validation set(验证集)\" 的数据集，模型训练完成以后在验证集上对模型进行评估。 当验证集上的评估实验比较成功时，在测试集上进行最后的评估。 然而，通过将原始数据分为3个数据集合，我们就大大减少了可用于模型学习的样本数量， 并且得到的结果依赖于集合对（训练，验证）的随机选择。这个问题可以通过 交叉验证（ CV ） j来解决。 交叉验证仍需要测试集做最后的模型评估，但不再需要验证集。 K折交叉验证","tags":"ML","url":"articles/Cross-Validation-CV.html","loc":"articles/Cross-Validation-CV.html"},{"title":"Docker","text":"nvidia-docker多用户共享GPU服务器环境搭建 创建容器 进入容器 添加已运行容器端口 修改容器容量 docker授权非root用户 容器内中文乱码 服务器docker中启可远程notebook PyCharm + Docker炼丹炉 制作镜像并上传仓库 docker-compose && dockerfile nvidia-docker多用户共享GPU服务器环境搭建 https://blog.csdn.net/hangvane123/article/details/88639279 nvidia驱动 docker-ce nvidia-docker pull带有cuda和cudnn的ubuntu镜像 https://hub.docker.com/r/nvidia/cuda docker pull nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 勿下载runtime版，而是devel，否则很多配置文件找不到。 启动容器，进入容器 apt-get update apt-get install python3-pip 安装与cuda, cudnn版本匹配的tensorflow-gpu 创建容器 # - it交互式终端运行 , 参数 / bin / bash启动ubuntu , -- name命名容器 # docker run - it - p [ host_port ] : [ container_port ] ( do not use 8888 ) -- name =[ container_name ] [ image_name ] - v [ host_path ] : [ container_path ] / bin / bash $ nvidia - docker run - itd -- name = dev -- shm - size 10 g - v / data / mnt : / home - p 8180 : 8180 - p 8280 : 8280 - p 8380 : 8380 - p 8480 : 8480 - p 9380 : 22 nvidia / cuda : 10.1 - cudnn7 - devel - ubuntu18 .04 / bin / bash # 查看所有容器 $ docker ps - a 进入容器 $: docker exec - it cuda10_0 env LANG = C . UTF - 8 / bin / bash 添加已运行容器端口 查看容器号 docker ps -a 停止容器 docker stop 查找容器目录 docker inspect [容器号ID] | grep Id 停止docker服务(systemctl stop docker) 查找容器路径： find / -name containers 修改这个容器的 /usr/local/docker/containers/... hostconfig.json文件中的端口: \"PortBindings\":{\"3306/tcp\":[{\"HostIp\":\"\",\"HostPort\":\"3307\"}]} 前者是容器端口, 后者是宿主机端口。 修改该容器的config.v2.json文件中的ExposedPorts。 启动docker服务(systemctl start docker) 启动容器 https://blog.csdn.net/lypeng_/article/details/98176138 https://blog.csdn.net/wesleyflagon/article/details/78961990 修改容器容量 # CID ---> /dev/mapper/docker # 100 ---> 100G . / script . sh 100 ` CID = \"62f54c85d02ec67b64c1ea15b0c3820edeea6744f7a052f0e795ea127d3fb28e\" SIZE =$ 1 if [ \"$CID\" != \"\" ] && [ \"$SIZE\" != \"\" ]; then DEV =$ ( basename $ ( echo / dev / mapper / docker -*-$ CID )); dmsetup table $ DEV | sed \"s/0 [0-9]* thin/0 $(($SIZE*1024*1024*1024/512)) thin/\" | dmsetup load $ DEV ; dmsetup resume $ DEV ; xfs_growfs / dev / mapper /$ DEV ; docker start container_name docker exec - it container_name env LANG = C . UTF - 8 / bin / bash echo \"Resize $CID completed.\" else echo \"Usage: sh resize_container 459fd505311ad364309940ac24dcdb2bdfc68e3c3b0f291c9153fb54fbd46771 100\" ; fi docker授权非root用户 sudo usermod -a -G docker AD\\\\yckj2939 sudo groupadd docker # 添加docker用户组 sudo gpasswd -a $USER docker # 将登陆用户加入到docker用户组中 newgrp docker # 更新用户组 docker ps # 测试docker命令是否可以使用sudo正常使用 systemctl restart docker # ！！！一定重启 容器内中文乱码 $ vim /etc/profile $ export LANG = C.UTF-8 服务器docker中启可远程notebook 启ubuntu容器，开出端口8888 nvidia-docker run -itd --name=cuda10_0 -v /mnt/docker_share:/home/centos -p 8888:8888 nvidia/cuda 进入容器安装anaconda $: docker exec - it cuda10_0 / bin / bash $: cd ~ | . / Anaconda . sh 配置notebook在.jupyter文件夹下 $: jupyter notebook --generate-config # 会自动生成.jupyter文件夹 $: jupyter notebook password 生成密钥： jupyter_notebook_config.json 修改文件： jupyter_notebook_config.py c.NotebookApp.ip='*' c.NotebookApp.password = u'生成的密钥' # jupyter_notebook_config.json文件中的password字段 c.NotebookApp.open_browser = False c.NotebookApp.port = 8888 #可指定一个端口, 访问时使用该端口（虽然运行jupyter时可以直接指定端口） c.NotebookApp.notebook_dir = '/home' # 自定义启动目录 https://blog.csdn.net/qq_42001765/article/details/96144442 后台运行 nohup jupyter notebook --ip=0.0.0.0 --no-browser --allow-root --port 8888 > jupyter.log 2>&1 & 远程访问 # server_ip:port 10.235.3.43:8888 容器内自动化脚本： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/sh # enter docker cd ~ jupyter notebook --generate-config echo 'Please input jupyter notebook password' jupyter notebook password echo 'Password input success!' chmod 777 ~/.jupyter/jupyter_notebook_config.json PASSWORD = $( cat ~/.jupyter/jupyter_notebook_config.json | grep password | awk -F '\"' '{print $4}' ) echo \"c.NotebookApp.ip='*'\" >> ~/.jupyter/jupyter_notebook_config.py echo \"c.NotebookApp.password = ' $PASSWORD '\" >> ~/.jupyter/jupyter_notebook_config.py echo \"c.NotebookApp.open_browser = False\" >> ~/.jupyter/jupyter_notebook_config.py echo \"c.NotebookApp.port = 8480\" >> ~/.jupyter/jupyter_notebook_config.py echo \"c.NotebookApp.notebook_dir = '/home'\" >> ~/.jupyter/jupyter_notebook_config.py nohup jupyter notebook --ip = 0 .0.0.0 --no-browser --allow-root --port 8480 > jupyter.log 2 > & 1 & PyCharm + Docker炼丹炉 PyCharm Pro + Nvidia Docker 参照上文增加容器22端口，SFTP默认使用22端口。 远程服务器参数查看 $ docker port <your container name> 22 # 此操作将查看docker container中端口22，在远程服务器上端口的映射 # 输出结果如下所示 0 .0.0.0:9380 # 表明只要ssh链接远程服务器的9380端口，实际是链接docker container中的22端口 ssh root@<服务器的ip地址> -p 9380 # 可以进入容器，passwd命令可以修改容器root密码 进入容器配置ssh服务 # 修改root密码 $ passwd # 安装open-ssh $ apt-get update $ apt-get install openssh-server $ vim /etc/ssh/sshd_config $ server ssh restart /etc/ssh/sshd_config修改以下位置： # Subsystem sftp /usr/libexec/openssh/sftp-server Subsystem sftp internal-sftp PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys #公钥文件路径（和上面生成的文件同） PermitRootLogin yes 最常见的问题就是docker容器停了以后里面的SSH服务也会相应停止，记得去docker里重启一下ssh服务： $ service ssh restart https://www.cnblogs.com/ruiyang-/p/10158658.html 制作镜像并上传仓库 dockerhub上创建仓库 jerrysu666/cuda10.0 终端登录： docker login 制作镜像： docker commit containerId dockerUserName/repoName 镜像打标签： docker tag imageName dockerUserName/repoName[:tag] 推送镜像： docker push dockerUserName/repoNme[:tag] docker tag local-image:tagname new-repo:tagname docker push new-repo:tagname docker push jerrysu666/cuda10.0:tagname docker-compose && dockerfile # docker - compose version : \"2.3\" services : detectron2 : build : context : . dockerfile : Dockerfile args : USER_ID : $ { USER_ID :- 1000 } runtime : nvidia shm_size : \"8gb\" ulimits : memlock : - 1 stack : 67108864 ports : - \"8170:8170\" - \"8270:8270\" - \"8370:8370\" - \"8470:8470\" - \"8570:22\" volumes : - / data : / home environment : - DISPLAY = $ DISPLAY - NVIDIA_VISIBLE_DEVICES = all # dockerfile FROM nvidia / cuda : 10.1 - cudnn7 - devel - ubuntu18 . 04 ENV DEBIAN_FRONTEND noninteractive RUN apt - get update && apt - get install - y \\ python3 - opencv ca - certificates python3 - dev git wget sudo && \\ rm - rf / var / lib / apt / lists /* # create a non-root user ARG USER_ID = 1000 RUN useradd - m -- no - log - init -- system -- uid $ { USER_ID } appuser - g sudo RUN echo ' %s udo ALL=(ALL) NOPASSWD:ALL' >> / etc / sudoers USER appuser WORKDIR / home / appuser ENV PATH = \"/home/appuser/.local/bin:${PATH}\" RUN wget https : // bootstrap . pypa . io / get - pip . py && \\ python3 get - pip . py -- user && \\ rm get - pip . py # install dependencies # See https://pytorch.org/ for other options if you use a different version of CUDA RUN pip install -- user torch torchvision tensorboard cython - i https : // pypi . doubanio . com / simple RUN pip install -- user 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI' RUN pip install -- user 'git+https://github.com/facebookresearch/fvcore' # install detectron2 RUN git clone https : // github . com / facebookresearch / detectron2 detectron2_repo ENV FORCE_CUDA = \"1\" # This will build detectron2 for all common cuda architectures and take a lot more time, # because inside `docker build`, there is no way to tell which architecture will be used. ENV TORCH_CUDA_ARCH_LIST = \"Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing\" RUN pip install -- user - e detectron2_repo # Set a fixed model cache directory. ENV FVCORE_CACHE = \"/tmp\" WORKDIR / home / appuser / detectron2_repo","tags":"Tools","url":"articles/Docker.html","loc":"articles/Docker.html"},{"title":"Conda","text":"anaconda3多用户共享安装 创建conda虚拟环境 在Notebook中切换conda虚拟环境 anaconda3多用户共享安装 > sudo bash Anaconda - latest - Linux - x86_64 . sh > 在 linux下安装第三方多用户使用的共享软件一般都按在 / usr / local 目录 > 配置 / etc / profile 文件，在该文件最后加入 export PATH =/ usr / local / anaconda3 / bin : $ PATH > source / etc / profile 创建conda虚拟环境 > conda info --envs > conda create -n rasa_101 python=3.6.8 > conda activate rasa_101 > pip install rasa==1.0.1 -i https://pypi.tuna.tsinghua.edu.cn/simple > pip install tensorflow==1.14.0 -i https://pypi.tuna.tsinghua.edu.cn/simple > conda info --envs -i 豆瓣源https://pypi.doubanio.com/simple pychram->file-> Settings -> Project :-> Project Interpreter C : \\ Users \\ YCKJ2939 \\ AppData \\ Local \\ Continuum \\anaconda3\\envs\\rasa_101 在Notebook中切换conda虚拟环境 进入容器安装nb_conda插件 conda install nb_conda 进入虚拟环境安装jupyter $: source activate env_name $: conda install -y jupyter 退出虚拟环境重启jupyter $: ps -aux | grep jupyter $: kill pid $: nohup jupyter notebook --ip=0.0.0.0 --no-browser --allow-root --port 8888 > jupyter.log 2>&1 & Error # An error occurred while retrieving installed packages . # EnvironmentLocationNotFound : Not a conda environment : / root / anaconda3 / envs / anaconda3 # 解决方法 ： # 找到Anaconda安装路径下nb_conda库的envmanager . py文件 # win系统在目录 ： Anaconda3 \\ Lib \\ site - packages \\ nb_conda \\ envmanager . py # linux系统在目录 ： / root / anaconda3 / pkgs / nb_conda - 2.2.1 - py37_0 / lib / python3 .7 / site - packages / nb_conda / envmanager . py # 找到该文件后在83 ~ 86 行代码改成如下代码 ： return { \"environments\" : [ root_env ] + [ get_info(env) for env in info['envs' ] if env != root_env [ 'dir' ] ] } # 重启jupyter ， 参见上述 https://blog.csdn.net/IT_xiao_bai/article/details/102765922","tags":"Tools","url":"articles/Conda.html","loc":"articles/Conda.html"},{"title":"Logistic Regression","text":"1. Linear Regression 1.1 线性模型 1.2 拟合线性模型的损失函数 1.3 线性回归的正则化 2. Logistic Regression 2.1 逻辑回归模型 2.2 拟合逻辑回归模型的损失函数 2.3 LR模型的损失函数可以使用线性模型的平方损失函数吗？ 2.3 LR模型的损失函数如何推导? 2.4 损失函数的紧凑形式是什么？为什么是这种形式？ 2.5 如何拟合参数？ 2.6 如何最小化损失函数？ 2.7 极大似然估计 2.8 逻辑回归的正则化 3. 回归和分类的本质区别 1. Linear Regression 1.1 线性模型 $$f(x) = \\Theta&#94;Tx$$ 1.2 拟合线性模型的损失函数 平方损失： $$\\frac{1}{m}\\sum_{n=1}&#94;{m} \\frac{1}{2} \\left ( f(x&#94;{(n)}) - y&#94;{(n)} \\right )&#94;2$$ 什么是最小二乘法？ 基于平方损失误差最小化进行模型求解的方法称为 最小二乘法 。在线性模型中，最小二乘法就是试图找到一条直线，使得所有样本到直线上的欧式距离之和最小。 平方损失函数是连续可微的凸函数，存在全局最小值，可以通过梯度下降法求解最优值。 1.3 线性回归的正则化 2. Logistic Regression 2.1 逻辑回归模型 Sigmoid函数: $$ f(x) = \\frac{1}{1+e&#94;{-z}} $$ 2.2 拟合逻辑回归模型的损失函数 $$-\\frac{1}{m}\\left [ \\sum_{i=1}&#94;{m} y&#94;{(i)}logf(x&#94;{(i)}) + (1-y&#94;{(i)})log(1-f(x&#94;{(i)})) \\right ], \\ \\ f(x)为逻辑模型$$ 逻辑回归解决的是分类问题，是 广义线性模型 ，在线性模型 \\(z=\\Theta&#94;Tx\\) 上套一层sigmoid函数。 2.3 LR模型的损失函数可以使用线性模型的平方损失函数吗？ 不可以，将LR模型非线性的sigmoid函数带入平方损失函数f(x)得到的是一个非凸函数，存在若干个局部最小值，无法利用梯度下降法求解最优值问题。 2.3 LR模型的损失函数如何推导? 图像性质： 如果标签y=1，预测值h(x)也为1，此时的损失值最小为0；当h(x)趋向0时，损失值趋近于无穷大。所以，预测值h(x)与y越接近，损失值越趋向于0。 case y=0: 反之，预测值h(x)接近标签y值0，则损失值收敛与0。 2.4 损失函数的紧凑形式是什么？为什么是这种形式？ $$-\\frac{1}{m}\\left [ \\sum_{i=1}&#94;{m} y&#94;{(i)}logf(x&#94;{(i)}) + (1-y&#94;{(i)})log(1-f(x&#94;{(i)})) \\right ], \\ \\ f(x)为逻辑模型$$ 损失函数是统计学中的极大似然估计推导而来，是统计学中为不同模型快速寻找参数的方法。同时拥有一个比较好的性质，是凸函数。 2.5 如何拟合参数？ 通过最小化损失函数，来拟合训练数据集，从而找到模型参数 \\(\\Theta\\) ，最终确定模型。 2.6 如何最小化损失函数？ 对损失函数： 梯度下降法 2.7 极大似然估计 2.8 逻辑回归的正则化 3. 回归和分类的本质区别 目标和方法： 对于回归问题：目标和方法是一致的 目标： pred = y 方法： 最小化预测值pred和真实值y的距离，即 minimize dist(pred, y) 对于分类问题： 目标： maxmize baseline. e.g. accuracy 方法： $$minimize dst(p_θ(y|x), p_r(y|x))$$ entropy","tags":"ML","url":"articles/Logistic-Regression.html","loc":"articles/Logistic-Regression.html"},{"title":"Spark","text":"3.spark简介 3.1.spark定义 3.2.spark和hadoop关系 3.3.spark优点 4. spark部署模式 4.1.local本地模式 4.2.standalone集群模式 4.3.yarn集群模式 5.spark集群搭建 5.1.机器准备 5.2.机器的环境配置 5.2.1.免密登陆 5.2.2.关闭防火墙 5.2.3.ip与host的映射关系 3.spark简介 3.1.spark定义 spark是基于 内存 的， 分布式 的， 大数据 并行计算框架 （处理引擎） 迭代式计算，优先使用内存，内存不足，再使用磁盘。 分布式：数据存储分布式；运算分布式。 spark + 数据源（hdfs） 3.2.spark和hadoop关系 hadoop：hdfs mapreduce yarn spark + hadfs spark + yarn spark 和 mapreduce比较 mapreduce：第一代分布式运行框架。分治编程思想。 mapreduce： 两步计算，磁盘存储; spark：多步计算，内存存储。类似于scala的函数式编程，链式编程。 spark基于内存，迭代效率更高（由于DAG有向无环图） spark容错性更好（由于RDD） spark算子更多 spark支持的语言更多（scala、java、python、R） 结论：spark是mapreduce的替代方案，兼容hdfs、hive，可融入hadoop生态圈，弥补mapreduce的不足。 3.3.spark优点 速度快 易用性 支持scala、java、python、r 通用性、 a. 一站式解决方案：离线分析 实时处理 机器学习 图计算 sql b. 减少公司开发的人力物力成本 兼容性 a. spark + mysql redis kafka hdfs yarn zookeeper 4. spark部署模式 4.1.local本地模式 开箱即用，一台机器即可。多线程模拟。 4.2.standalone集群模式 是spark安装包自带的集群模式 4.3.yarn集群模式 yarn：资源调度平台（公司常用模式） 把spark任务提交到yarn集群运行 5.spark集群搭建 集群搭建特指：standslone集群。角色：master 和 worker ssh远程服务器：ip port user password 跳板机器：如果服务器rno是内网的，外网无法直接访问。外网需要一个跳板机（bastion）。 5.1.机器准备 a1 192.168.23.1 master a2 192.168.23.2 worker a3 192.168.23.3 worker 至少两台 5.2.机器的环境配置 5.2.1.免密登陆 配置主节点到从节点的免密登陆即可。 # ssh-keygen # ssh-copy-id ip 5.2.2.关闭防火墙 大数据集群，一般都是内网集群，不需要开启防火墙。 有单独的机器，具备内网和外网环境，可以通过该机器进行对外通信。 # service iptables status // 查看防火墙 # service iptables stop // 关闭防火墙 # ckconfig iptables off // 永久关闭 5.2.3.ip与host的映射关系 使用hostname # cat /etc/hosts 127.0.0.1 localhost 255.255.255.255 broadcasthost ::1 localhost 202.76.247.23 ion-ljz.corp.ebay.com 192.168.23.1 a1 192.168.23.2 a2 192.168.23.3 a3","tags":"Programming","url":"articles/Spark.html","loc":"articles/Spark.html"},{"title":"Spark Partitions with Coalesce and Repartition","text":"Spark splits data into partitions and executes computations on the partitions in parallel. You should understand how data is partitioned and when you need to manually adjust the partitioning to keep your Spark computations running efficiently. https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4","tags":"Programming","url":"articles/Spark-Partitions-with-Coalesce-and-Repartition.html","loc":"articles/Spark-Partitions-with-Coalesce-and-Repartition.html"},{"title":"Scala Tips","text":"foreach val xs = List ( \"date\" , \"since\" , \"other1\" , \"other2\" ) xs . foreach { str => str match { case \"date\" => println ( \"Match Date\" ) case \"since\" => println ( \"Match Since\" ) case unknow => println ( \"Others\" ) } println ( \"Put your post step here\" ) } 注意: 如果要使用一段代码作为foreach（）的参数，则应使用{}而不是（）。","tags":"Programming","url":"articles/Scala-Tips.html","loc":"articles/Scala-Tips.html"},{"title":"Spark Tips","text":"Apache Spark - Best Practices and Tuning https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/ Big Data Analysis with Scala and Spark https://www.coursera.org/learn/scala-spark-big-data/home/welcome","tags":"Programming","url":"articles/Spark-Tips.html","loc":"articles/Spark-Tips.html"},{"title":"Scala Collection","text":"collection.mutable collection.immutable collection.mutable collection.immutable","tags":"Programming","url":"articles/Scala-Collection.html","loc":"articles/Scala-Collection.html"},{"title":"Automated Feature Engineering: Featuretools","text":"思考： 当特征深度过深大于2时的特征可解释性？ 生成的特征过多带来新的问题：维度诅咒，那么如何选取特征？ feature reduction and selection feature selection http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf http://featuretools.com https://github.com/Featuretools https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219 https://github.com/WillKoehrsen/automated-feature-engineering/blob/master/walk_through/Automated_Feature_Engineering.ipynb https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset","tags":"ML","url":"articles/Automated-Feature-Engineering:-Featuretools.html","loc":"articles/Automated-Feature-Engineering:-Featuretools.html"},{"title":"PCA","text":"协方差与相关系数 协方差矩阵 协方差矩阵的特征值和特征向量 协方差与相关系数 相关系数的直觉 协方差矩阵 如何求得协方差矩阵？ 协方差矩阵的特征值和特征向量 PCA Tutorial Intuition","tags":"ML","url":"articles/PCA.html","loc":"articles/PCA.html"},{"title":"SQLite Full-text Search","text":"Sqlite Full-text Search Sqlite Full-text Search 理解虚表 理解全文本查找 https://www.sqlite.org/fts5.html http://www.sqlitetutorial.net/sqlite-full-text-search def _is_to_exec ( self , sha1 ): \"\"\"Check whether the query has been run in the past. \"\"\" sql = f ''' SELECT DISTINCT sha1 FROM queries WHERE queries MATCH ' { sha1 } ' ''' if self . _conn . execute ( sql ) . fetchall (): answer = input ( 'The sql has been run. \\n Are you sure to run the sql again? (y/[n]): ' ) if answer . strip () . lower () != 'y' : return False return True Don't use match! match is for full-text search. Use ordinary SQL queries please. Just compare the column sha1 with the sha1sum of the query Pease refer to the search function to put the matched queries into the srps table so that you can print them to the user. Print out details of the matched queries here. You can simply use the function show_srps if you have put the matched queries into the srps table. As a related task, please make the command ./spark_sql.py run support option -i 2, —all. You can then add a -f/—force option to force rerun a query.","tags":"Programming","url":"articles/SQLite-Full-text-Search.html","loc":"articles/SQLite-Full-text-Search.html"},{"title":"L1 and L2 Regularization","text":"L范数 L1与L2正则化 引用 L范数 L0范数：向量中非零元素的个数 在机器学习中，如果使用L0范数即希望大部分权重w为0，即w向量时稀疏的，可以用于特征选择，通过最小化L0，来寻找最优的稀疏特征。然而L0范数的优化问题时一个NP Hard问题，故而通常L1的最优化问题通常会放宽到L1，L2下的最优化。 L1范数：向量中每个元素的绝对值之和 曼哈顿距离 Lasso回归 L2范数：向量元素绝对值的平方和再开方 欧几里得距离 Ridge回归 LP范数 LP -Norm推导： LP -Norm最终是： \\(max(x_1, x_2,..,x_n)\\) 中的绝对值最大的元素，即二维是一个正方形。 L1与L2正则化 为什么需要正则化？ 抑制模型复杂度，防止过拟合。 几何解释：解空间 解空间：损失函数的等高线与圆形正方形相交的区域。 L1：函数连续，但存在不可导点。在特征为二维时，约束线是一个菱形，等值线 大概率 最先与顶点相交，在这种情况下有一个维度的特征就会为0，这就带来了稀疏。当特征的维度变高，坐标轴上角与边都会变多，这更会加大等值线与他们先相交的概率，从而导致了稀疏性。 L2：函数连续且处处可导。它的约束线是一个圆形，等值线可能与它任意一个位置的点首先相切，这个切点在坐标轴上的概率大大减小，从而不太容易导致稀疏。L2正则化通过 权重衰减 ，在权重较大时衰减地快，权重较小时衰减得慢，保证了模型的简单，提高了泛化能力。 下降速度 当w较大时，L2的斜率大于L1，L2正则化权重衰减地比L1正则化快。 当w较小时，L2的斜率小于L1，L1正则化权重衰减地比L2正则化快。 因此L1正则化最终会导致模型保留了重要的大权重，不重要的小权重都被衰减为0，产生了稀疏。而L2正则化可以通过限制权重大小让模型变得简单，但却不会导致稀疏。 引用 L1 and L2 regularization L0、L1、L2范数在机器学习中的应用","tags":"ML","url":"articles/L1-and-L2-Regularization.html","loc":"articles/L1-and-L2-Regularization.html"},{"title":"Bias vs Variance","text":"方差与偏差 为什么RF的树深一般大于GBDT树的深度？ 方差与偏差 泛化误差分为：偏差和方差 偏差：指算法的期望预测值与真实值之间的偏差程度，反应的是模型本身拟合能力。(单模型) 方差：度量了同等大小数据集的变动导致学习性能的变化，刻画数据扰动所导致的影响。(多模型) 当模型越复杂时，训练数据的拟合程度就越高，模型的训练偏差就越小。但如果还一组数据可能模型的变化就很大，即模型的方差很大。所以复杂度高的模型容易产生过拟合。 当模型简单时，即使还一组训练数据，得出的学习器之间差别不是很大，即模型的方差较小。但由于模型简单，所以存在比较大的偏差。 所以，在训练一个模型时，需要平衡好方差和偏差。 为什么RF的树深一般大于GBDT树的深度？ 对于Bagging算法，由于时并行的训练若干个弱学习器，他们之间相互独立，主要目的降低方差。所以为了平衡好方差与偏差，每一个弱学习器目标便是如何降低偏差，因而会采用复杂度高的模型作为弱学习器，例如深度较深甚至不剪枝的树，神经网络等。 对于Boosting算法，训练的弱学习器都是在上一轮基础上更加的拟合数据，保证的是模型的偏差。所以为了平衡好方差与偏差，每一个弱学习器目标便是如何降低弱学习器之间的方差，因而会采用复杂度低的模型作为弱学习器，例如深度很浅的树。","tags":"ML","url":"articles/Bias-vs-Variance.html","loc":"articles/Bias-vs-Variance.html"},{"title":"Topological Sorting","text":"Topological Sorting Topological Sorting # Definition for a Directed graph node class DirectedGraphNode : def __init__ ( self , x ): self . label = x self . neighbors = [] class Solution : \"\"\" @param graph: A list of Directed graph node @return: A list of integer \"\"\" def topSort ( self , graph ): # 1. 统计结点入度 indegree = self . get_indegree ( graph ) # 2. BFS order = [] start_nodes = [ n for n in graph if indegree [ n ] == 0 ] # 入度为0的所有结点 queue = collections . deque ( start_nodes ) # 队列中存储的是入度为0的点 while queue : node = queue . popleft () order . append ( node ) for neighbor in node . neighbors : # 遍历该节点的所有邻居节点，第一层遍历。 indegree [ neighbor ] -= 1 # 将队列中输出的点的所以临界点入度减1 if indegree [ neighbor ] == 0 : # 将入度为0的结点放入队列 queue . append ( neighbor ) return order def get_indegree ( self , graph ): \"\"\" 计算每一个结点的入度数 \"\"\" indegree = { x : 0 for x in graph } # 初始化每一个结点的入度数为0 for node in graph : for neighbor in node . neighbors : indegree [ neighbor ] += 1 return indegree","tags":"Algorithms","url":"articles/Topological-Sorting.html","loc":"articles/Topological-Sorting.html"},{"title":"Breadth First Search","text":"概述 宽搜要点 二叉树上的宽搜 Binary Tree Level Order Traversal Binary Tree Zigzag Level Order Traversal Serialize and Deserialize Binary Tree 图上的宽搜 图的遍历 层级遍历 由点及面 拓扑排序 矩阵上的宽搜 概述 典型BFS三层循环: 先明确队列中放的是什么，即满足怎样的条件才能入队。例如，拓扑排序入度为0的节点入队。 队列不空, while queue 分层遍历，for当前层的size。 循环何时可省？ 不需要分层 从当前点出发，向周围点去循环 例如:拓扑排序去for循环遍历当前点的邻居节点。 循环何时可省？ 二叉树层序遍历，周围点只可能有左右孩子，无需for循环。node.left, node.right分别看一眼即可。 宽搜要点 使用队列为主要数据结构Queue 是否需要分层实现：分层打印多一层for循环，len(queue)缓存队列每一层大小。 for _ range(len(queue)): 错，需缓存 二叉树上的宽搜 Binary Tree Level Order Traversal Binary Tree Zigzag Level Order Traversal Serialize and Deserialize Binary Tree 序列化是指将 内存 中的数据结构转为 字符串 的过程。系列化：object -> string；反序列化: string -> object。 图上的宽搜 图上的宽搜与树的区别： 图有环，同一节点可能重复进入队列。需用集合存储访问过的结点。queue.append与visited.add成对存在。 数据结构： 队列 + 集合 图如何存储？ 临接点 class GraphNode : def __init__ ( self , x ): self . label = x self . neighbors = [] 图的遍历 层级遍历 由点及面 拓扑排序 Topological Sorting 矩阵上的宽搜","tags":"Algorithms","url":"articles/Breadth-First-Search.html","loc":"articles/Breadth-First-Search.html"},{"title":"GBDT","text":"问题引入 残差与梯度下降的关系 问题引入 给定数据集 \\((x_1, y_1), (x_2, y_2),...,(x_n, y_n)\\) , 拟合一个模型 \\(F(x)\\) 。 \\(F(x_1) = 1.4\\) 而 \\(y_1=1.3\\) \\(F(x_2) = 0.9\\) 而 \\(y_2=0.8\\) … 在不改变 \\(F(x)\\) 模型前提下，如何提升模型 \\(F(x)\\) ？ 在原有模型 \\(F(x)\\) 基础上增加模型 \\(h(x)\\) ,即 \\(F(x) + h(x)\\) 。 $$F(x_n) + h(x_n) = y_n$$ $$h(x_n) = y_n - F(x_n)$$ 所以对数据 \\((x_1, y_1-F(x_1)), (x_2, y_2-F(x_2)),...,(x_n, y_n-F(x_n))\\) 拟合一个回归树 \\(h\\) 。 \\(y_i - F(x_i)\\) 是 残差 ，就是模型 \\(F(x)\\) 未很好拟合的那一部分误差。 \\(h(x)\\) 作用就是弥补现有模型的残差。如果 \\(F+h\\) 依然没有很好的拟合原数据，则继续添加另一个回归树模型去拟合新的残差。 残差与梯度下降的关系 梯度下降是指在函数当前点对应梯度的反方向迭代移动以达到函数的局部最小。 损失函数： \\(L(y, F(x)) = \\frac{1}{2} (y - F(x))&#94;2\\) 我们通过迭代调整 \\(F(x_1), F(x_2),..,F(x_n)\\) 以到达损失函数 \\(J = \\sum_i L(y_i, F(x_i))\\) 的最小化。 注： \\(F(x_1), F(x_2),...,F(x_n)\\) 仅仅是一些数，所以可以将 \\(F(x_i)\\) 当作是变量。那么梯度推导： \\begin{align} \\frac{\\partial J}{\\partial F(x_i)} & = \\frac{\\partial \\sum_i L(y_i, F(x_i))}{\\partial F(x_i)} \\\\ & = \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\\\ & = \\frac{\\partial \\frac{1}{2}(y_i-F(x_i))&#94;2}{\\partial x} \\\\ & = F(x_i)-y_i \\end{align} 所以对于具有平方损失的回归： \\begin{align} 残差y_i-F(x_i) &<===>负梯度-\\frac{\\partial J}{\\partial F(x_i)} \\\\ 对残差拟合h &<===> 对负梯度拟合h \\\\ 基于残差更新模型F &<===> 基于负梯度更新F \\\\ \\end{align} 所以使用梯度下降来更新模型F，梯度下降的概念比残差更通用且有用。 残差与负梯度的关系：是由平方损失函数建立的，平方损失函数的一阶导数即梯度正是负残差，而平方损失函数又常用于回归问题","tags":"ML","url":"articles/GBDT.html","loc":"articles/GBDT.html"},{"title":"Dynamic Programming","text":"什么是动态规划 与递归三要素对比 Triangle Minimum Path Sum Maximal Square Longest Increasing Subsequence ( LIS ) Longest Common Subsequence ( LCS ) Longest Palindromic Substring Maximum Subarray Maximum Product Subarray Coin Change 什么是动态规划 动态规划是一种最优化方法，一般的表现形式是求最值。因而求解动态规划的核心问题便是穷举，穷举所有结果获取最优值。类似于回溯问题，暴力穷举所有可能的结果，但穷举的效率极其低下。针对于动态规划这类问题，均有一个共同的特点就是存在 重叠子问题 ，利用这个特性采用DP Table备忘表的技巧优化穷举，避免重复计算。（引出自顶向下法递归法） 然而，对于动态规划的问题千变万化，虽然核心思想是穷举求最值，但是在实际实践中穷举所有可能的结果并未易事。所以引出动态规划问题的另外两个特性： 最优子结构 和 状态转移方程 来帮助正确穷举。最优子结构即通过子问题的最优解推出原问题的最优解（重复子问题无后效性），状态的定义和状态的转移方程就显得尤为重要。（引出自底向上递推法） 思维框架：明确状态 -> 定义dp数组的含义 -> 明确选择 -> 明确base case 总结： 动态规划问题具备的三要素： 重叠子问题，最优子结构，状态转移方程 动态规划问题的两种解法： 带备忘的自顶向下的递归法：主要根据重叠子问题特性，通过备忘录避免重复计算的方法，本质是递归穷举搜索。 自底向上的迭代递推法：这才是真正意义上的动态规划，通过子问题状态迭代递推出原问题的状态。 状态的定义 状态的定义是动态规划最难的地方，根据定义状态可以将其分类。 状态的转移方程 状态之间的联系，如何通过小状态推出大状态。 状态的初始化 最小状态是什么，即递归中的base case起点。 以及转移方程推算不出的需要手工计算的状态。 返回结果 最终要求解的问题，即最大的状态。终点 与递归三要素对比 所有的动态规划都是由暴力递归优化而来 动态规划是一种分阶段求解决策问题的数学思想。 三个重要的概念： 状态 （最优子结构）、 递推方程 、 边界 。 动态规划利用 自底向上的递推 方式，实现时间和空间上的最优化。 递归展开过程中存在重复状态，即重叠子问题 重复状态无后效性：与到达这个状态的路径无关，即只要这个状态参数确定，则返回值确定。 暴力递归到动态规划的解法： 找出需要求解的 状态位置 回到Base case中设置不被依赖的 边界状态 分析 普遍状态 如何依赖 Triangle def minimumTotal ( self , triangle ): if not triangle : return None row = len ( triangle ) dp = [[ 0 for _ in range ( len ( row ))] for row in triangle ] dp [ 0 ][ 0 ] = triangle [ 0 ][ 0 ] for i in range ( 1 , row ): for j in range ( len ( triangle [ i ])): if j == 0 : dp [ i ][ j ] = triangle [ i ][ j ] + dp [ i - 1 ][ j ] elif j == len ( triangle [ i ]) - 1 : dp [ i ][ j ] = triangle [ i ][ j ] + dp [ i - 1 ][ j - 1 ] else : dp [ i ][ j ] = triangle [ i ][ j ] + min ( dp [ i - 1 ][ j ], dp [ i - 1 ][ j - 1 ]) return min ( dp [ - 1 ]) Minimum Path Sum Python def minPathSum ( self , grid ): if not grid : return row = len ( grid ) col = len ( grid [ 0 ]) #dp = [[0 for i in range(col)] for j in range(row)] dp = [[ 0 for _ in range ( len ( row ))] for row in grid ] dp [ 0 ][ 0 ] = grid [ 0 ][ 0 ] # the first row for i in range ( 1 , col ): dp [ 0 ][ i ] = grid [ 0 ][ i ] + dp [ 0 ][ i - 1 ] # the first column for i in range ( 1 , row ): dp [ i ][ 0 ] = grid [ i ][ 0 ] + dp [ i - 1 ][ 0 ] # other location for i in range ( 1 , row ): for j in range ( 1 , col ): dp [ i ][ j ] = grid [ i ][ j ] + min ( dp [ i ][ j - 1 ], dp [ i - 1 ][ j ]) print ( dp ) return dp [ - 1 ][ - 1 ] C ++ class Solution { public : int minPathSum ( vector < vector < int >>& grid ){ int row = grid . size (); int col = grid [ 0 ]. size (); vector < vector < int >> dp ( row , vector < int > ( col , 0 )); dp [ 0 ][ 0 ] = grid [ 0 ][ 0 ]; // first row for ( int i = 1 ; i < col ; ++ i ) dp [ 0 ][ i ] = grid [ 0 ][ i ] + dp [ 0 ][ i -1 ]; // first col for ( int i = 1 ; i < row ; ++ i ) dp [ i ][ 0 ] = grid [ i ][ 0 ] + dp [ i -1 ][ 0 ]; for ( int i = 1 ; i < row ; ++ i ){ for ( int j = 1 ; j < col ; ++ j ){ dp [ i ][ j ] = grid [ i ][ j ] + min ( dp [ i -1 ][ j ], dp [ i ][ j -1 ]); } } return dp [ row -1 ][ col -1 ]; } }; Maximal Square 状态定义：dp[i][j]表示以坐标为 i, j 的这个点，作为正方形的右下角，可以扩展的最大边长 转移方程：dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1 取最小的是因为加上(i, j)这一点一定时正方形。画图理解 class Solution : def maximalSquare ( self , matrix : List [ List [ str ]]) -> int : if not matrix : return 0 row , col = len ( matrix ), len ( matrix [ 0 ]) dp = [[ int ( matrix [ i ][ j ]) for j in range ( col )] for i in range ( row )] res = max ( max ( row ) for row in dp ) # 初值，特列[['1']] for i in range ( 1 , row ): for j in range ( 1 , col ): if matrix [ i ][ j ] == '1' : # 注意勿丢 dp [ i ][ j ] = min ( dp [ i - 1 ][ j ], dp [ i ][ j - 1 ], dp [ i - 1 ][ j - 1 ]) + 1 res = max ( res , dp [ i ][ j ]) else : dp [ i ][ j ] = 0 return res ** 2 Longest Increasing Subsequence ( LIS ) 状态的定义：dp[i]是以第i个元素结尾的最大上升序列的长度 本题特殊在于：当前状态dp[i]并非由前一个状态dp[i-1]直接推来，而是由前dp[0] ~ dp[i-1]状态中中最大的推来。如下： dp[j]是前一个状态， j属于0 ~ i-1中最大的状态 Python class Solution : def lengthOfLIS ( self , nums : List [ int ]) -> int : if not nums : return 0 dp = [ 1 ] * len ( nums ) for i in range ( len ( nums )): # 分别计算每一个状态 for j in range ( i ): # 遍历寻上前一个满足条件的状态 if nums [ j ] < nums [ i ]: dp [ i ] = max ( dp [ i ], dp [ j ] + 1 ) # dp[i]迭代放置最大状态 return max ( dp ) C ++ class Solution { public : int lengthOfLIS ( vector < int >& nums ){ int n = nums . size (); if ( n < 1 ) return n ; vector < int > dp ( n , 1 ); for ( int i = 1 ; i < n ; ++ i ){ for ( int j = 0 ; j < i ; ++ j ){ if ( nums [ j ] < nums [ i ]){ dp [ i ] = max ( dp [ i ], dp [ j ] + 1 ); } } } int max_length = 1 ; for ( auto & ele : dp ) max_length = max ( max_length , ele ); return max_length ; } }; Longest Common Subsequence ( LCS ) 状态的定义： dp[i][j]是子串string1[0 ~ i]与字串string[0 ~ j]的最长公共字串的长度。 递推方程： 若求原问题dp[i][j]，从子问题dp[i-1][j-1]或者max(dp[i-1][j], dp[i][j-1])递推而来。 当string1[i] == string2[j]时，原问题dp[i][j] = dp[i-1][j-1] + 1，即string1[0 ~ i-1]与字串string[0 ~ j-1]的最长公共字串的长度加1 当string1[i] != string2[j]时，则必须求解dp[i-1][j]和dp[i][j-1]两个子问题并取最大的结果，原问题dp[i][j] = max(dp[i-1][j], dp[i][j-1])，即子串string1[0 ~ i-1]与string[0 ~ j]的LCS与子串string1[0 ~ i]与string[0 ~ j-1]的LCS中最大的LCS。 边界初始化技巧：字符串前增加一个空字符#。 C ++ class Solution { public : int longestCommonSubsequence ( string text1 , string text2 ){ int len1 = text1 . size (); int len2 = text2 . size (); if ( len1 == 0 || len2 == 0 ) return 0 ; vector < vector < int >> dp ( len1 + 1 , vector < int > ( len2 + 1 , 0 )); /* 增加一个特殊字符#，利于初始化base case # a b a c k # 0 0 0 0 0 0 a 0 b 0 s 0 */ for ( int i = 1 ; i <= len1 ; ++ i ){ for ( int j = 1 ; j <= len2 ; ++ j ){ if ( text1 [ i -1 ] == text2 [ j -1 ]) dp [ i ][ j ] = dp [ i -1 ][ j -1 ] + 1 ; else dp [ i ][ j ] = max ( dp [ i -1 ][ j ], dp [ i ][ j -1 ]); } } return dp [ len1 ][ len2 ]; } }; Longest Palindromic Substring 状态定义：dp[i][j]是字符串s[i:j]是否为回文子串。 base case：1. 单个字母dp[i][i] 2. 相邻字母dp[i][i+1] dp数组遍历方式：遍历所有可能字串长度len （子串长度len + 开始位置start）方式 class Solution { public : string longestPalindrome ( string s ){ int n = s . size (); if ( n == 0 ) return s ; int start_substr = 0 ; int max_len = 1 ; vector < vector < bool >> dp ( n , vector < bool > ( n , false )); // base case one letter for ( int i = 0 ; i < n ; ++ i ) dp [ i ][ i ] = true ; // nase case two letter for ( int i = 0 ; i < n - 1 ; ++ i ){ if ( s [ i ] == s [ i + 1 ]){ dp [ i ][ i + 1 ] = true ; start_substr = i ; max_len = 2 ; } } // length of substring for ( int len = 3 ; len <= n ; ++ len ){ // start position of substring for ( int start = 0 ; start < n - len + 1 ; ++ start ){ // end position of substring int end = start + len - 1 ; if ( dp [ start + 1 ][ end -1 ] && s [ start ] == s [ end ]){ start_substr = start ; max_len = len ; dp [ start ][ end ] = true ; } } } return s . substr ( start_substr , max_len ); } }; Maximum Subarray 状态定义： dp[i]表示以nums[i]元素结尾（包含nums[i]）最大子数组的和 转移方程： $$dp[i] = \\begin{cases} nums[i], &\\ dp[i-1] \\leq 0 \\\\\\ dp[i-1] + nums[i], &\\ dp[i-1] > 0 \\end{cases}$$ 注：因为是连续子数组，所以dp[i]当前状态，只能由dp[i-1]状态推来。 转移方程代码实现： dp[i] = max(dp[i-1] + nums[i], nums[i]) 分析： 当以第i-1元素结尾的子数组中所有数字的和小于0时，如果把这个负数与第i个元素累加，得到的结果比nums[i]本身还小，所以dp[i]就是nums[i]本身。 若dp[i-1] < 0：dp[i] = nums[i] nums[i]加上负dp[i-1]一定比自身小，取nums[i] 若dp[i-1] > 0： dp[i] = dp[i-1] + nums[i] nums[i]加上正dp[i-1]一定比自身大，取 \\(dp[i-1] + nums[i]\\) 由于dp[i]仅与dp的前一个状态有关，即在计算dp[i]时， \\(dp[i-2],dp[i-3]...,dp[0]\\) 对于dp[i]没有影响，因此可以空间优化省去dp数组。 def maxSubArray ( self , nums ): if not nums : return dp = [ 0 ] * len ( nums ) dp [ 0 ] = nums [ 0 ] for i in range ( 1 , len ( nums )): dp [ i ] = max ( dp [ i - 1 ] + nums [ i ], nums [ i ]) return max ( dp ) def maxSubArray ( self , nums ): # 空间优化 if not nums : return dp , dp [ 0 ], res = [ 0 , 0 ], nums [ 0 ], nums [ 0 ] for i in range ( 1 , len ( nums )): x , y = i % 2 , ( i - 1 ) % 2 # 滚动数组，空间优化 dp [ x ] = max ( dp [ y ] + nums [ i ], nums [ i ]) res = max ( dp [ x ], res ) return res C ++ class Solution { public : int maxSubArray ( vector < int >& nums ) { int n = nums . size (); if ( n == 0 ) return 0 ; vector < int > dp ( n , 0 ); dp [ 0 ] = nums [ 0 ]; int s_max = nums [ 0 ]; for ( int i = 1 ; i < n ; ++ i ){ dp [ i ] = max ( nums [ i ], dp [ i -1 ] + nums [ i ]); s_max = max ( s_max , dp [ i ]); } return s_max ; } }; vector < int > nums { 2 , 4 , -1 , 7 , 3 , -3 , 1 }; // dp = [2, 6, 5, 12 ,15, 1,2 ,13] Maximum Product Subarray dp状态： dp [ i ][ 2 ] dp [ i ][ 0 ] => max dp [ i ][ 1 ] => min dp初始状态： dp[0][0], dp[0][1] = nums[0], nums[0] dp状态转移方程： 取决于当前值 nums[i] 的正负情况。 若为正 nums[i] >= 0 ： 则当前状态的最大值为上个状态的最大值乘以当前值 dp[i][0] = max(dp[i - 1][0] * nums[i], nums[i]) 则当前状态的最小值为上个状态的最小值乘以当前值 dp[i][1] = min(dp[i - 1][1] * nums[i], nums[i]) 若为负 nums[i] < 0 ： 则当前状态的最大值为上个状态的最小值乘以当前值 dp[i][0] = max(dp[i - 1][1] * nums[i], nums[i]) 则当前状态的最小值为上个状态的最大值乘以当前值 dp[i][1] = min(dp[i - 1][0] * nums[i], nums[i]) dp [ i ][ 0 ] = max ( dp [ i - 1 ][ 0 ] * nums [ i ] , dp [ i - 1 ][ 1 ] * nums [ i ] , nums [ i ] ) dp [ i ][ 1 ] = min ( dp [ i - 1 ][ 0 ] * nums [ i ] , dp [ i - 1 ][ 1 ] * nums [ i ] , nums [ i ] ) def maxProduct ( self , nums : List [ int ]) -> int : if not nums : return dp = [[ 0 for _ in range ( 2 )] for _ in range ( 2 )] dp [ 0 ][ 0 ], dp [ 0 ][ 1 ], res = nums [ 0 ], nums [ 0 ], nums [ 0 ] for i in range ( 1 , len ( nums )): x , y = i % 2 , ( i - 1 ) % 2 # 滚动数组 dp [ x ][ 0 ] = max ( dp [ y ][ 0 ] * nums [ i ], dp [ y ][ 1 ] * nums [ i ], nums [ i ]) dp [ x ][ 1 ] = min ( dp [ y ][ 0 ] * nums [ i ], dp [ y ][ 1 ] * nums [ i ], nums [ i ]) res = max ( res , dp [ x ][ 0 ]) return res Coin Change 分析: 有多少种面值的硬币，前一个状态dp[i - coin]就有多少种。 i表示当前的要计算的总额，i - coin表示去掉添加上来的硬币面值剩下总额。 所以需要遍历减去各种面值而得到的前一个状态，找到最小值，即最小硬币数。 dp[i - coin]是前一个状态,有多少面值的硬币前一个状态就有多少种， 遍历所有的前一个状态取值最小的那个状态 + 1个硬币数（加上当前面值）。 状态的定义： dp[i]到金额i（类似于台阶）所需要的最少硬币数。 转移方程： \\(dp[i] = min(dp[i-coin_1],\\ dp[i-coin_2],...,\\ dp[i-coin_n])\\) 初始状态：dp[0] = 0：面值为0需要0个硬币数; 若硬币面值有2， 5， 7三种，则dp[1]是转移方程无法计算出的，所以也需要手工初始化，因为这里求最小所以将其初始化为无穷大float(‘inf')。 时间复杂度 \\(O(M*N)\\) ： M是待兑换的总金额，从1递推到M；N是可以兑换的硬币种类数，也是loop所有币种。 def coinChange ( self , coins : List [ int ], amount : int ) -> int : dp = [ 0 ] + [ float ( 'inf' )] * amount for i in range ( 1 , amount + 1 ): for coin in coins : # 遍历前一个状态 if coin <= i : # 硬币的面值需要小于i，即当前总amount dp [ i ] = min ( dp [ i ], dp [ i - coin ] + 1 ) # dp[i]迭代放置最小状态， 1表示加上一个面值硬币后的最少硬币数 return dp [ - 1 ] if dp [ - 1 ] != float ( 'inf' ) else - 1","tags":"Algorithms","url":"articles/Dynamic-Programming.html","loc":"articles/Dynamic-Programming.html"},{"title":"Closure","text":"什么是闭包？ 如果自由变量值发生变化会怎样？ 如果闭包修改了自由变量的值会怎样？ 为什么需要闭包，有什么优势？ 什么是闭包？ 引用至少一个自由变量的函数称为闭包。 闭包是一个函数，可纯函数或非纯函数，可有名字或匿名，但重要的是它是一个函数。 为何称其为闭包，它与函数最重要的区别是： 引用自由变量 。 // p相对于getHike，是其自由变量。getHike函数没有局部变量和列表参数p。 var p = 10 def getHike ( salary : Double ) = salary * p / 100 getHike ( 5000 ) 如果自由变量值发生变化会怎样？ 执行闭包时，它采用最新的自由变量的值。 var p = 10 def getHike ( salary : Double ) = salary * p / 100 getHike ( 5000 ) //res1: Double = 500.0 p = 20 getHike ( 5000 ) //res2: Double = 1000.0 闭包是否为纯函数：取决于自由变量的类型var和val 如果闭包修改了自由变量的值会怎样？ 如果闭包修改了自由变量，则更改在闭包外部可见。 var p = 10 def getHike ( salary : Double ) = { p = p * 2 salary * p / 100 } println ( p ) //10 getHike ( 5000 ) //res8: Double = 1000.0 println ( p ) //20 为什么需要闭包，有什么优势？ 函数式编程，函数可以最为参数传递和返回，与面向对象类似。 对于某些例子，对象更灵活，因为对象携带方法和数据元素（状态）。然而，函数是唯一的，因为它没有任何数据元素（状态）。 所以，如果我们需要传递一堆状态和一个函数，那么使用： 闭包 和 自由变量 。 val l = ( 1001 to 1005 ). toList l . map ( getHike ) def getHike = { //Load employee and their current salary val e : Map [ Int , Double ] = Map ( 1001 -> 35000.00 , 1002 -> 43000.00 , 1003 -> 28000.00 , 1004 -> 54000.00 , 1005 -> 17000.00 ) // Some logic to derive percentage for each employee val p : Map [ Int , Double ] = Map ( 1001 -> 10.00 , 1002 -> 12.00 , 1003 -> 7.50 , 1004 -> 6.80 , 1005 -> 20.00 ) ( empID : Int ) => ( empID , e ( empID ) * p ( empID ) / 100.00 ) // 返回一个匿名函数，即闭包 } val f = getHike f : Int => ( Int , Double ) = < function1 > //Get Hike for an employee f ( 1001 ) //res10: (Int, Double) = (1001,3500.0) //Get Hike for a non existant employee f ( 1006 ) //java.util.NoSuchElementException: key not found: 1006 从最后一行返回的匿名函数是一个闭包。它使用两个自由变量e和p。 当我们从getHike返回它时，它带有e和p的状态。 所以，f包含它的数据。 闭包就像面向对象世界里传递的一个对象！ 它节省了大量复杂且不必要的代码，并简化了解决方案。","tags":"Programming","url":"articles/Closure.html","loc":"articles/Closure.html"},{"title":"Passing a function as an argument: Lambda Function","text":"f = lambda x: x * 2 is exactly the same thing as def f ( x ) : return x * 2 Lambdas are usually used to create small, anonymous functions. Actually, they are just a syntatic sugar to define functions. The lambda expression above is exactly the same as your function, only without a name. The main difference between lambda expressions and regular functions: Lambda表达式只能包含表达式，不能包含语句。表达式是任何可以放在=赋值右侧的。 # lambda function pass logger.add(tmpfile.name, compression=lambda path: email(path, app_name=conf['app_name'], model_owner=conf['model_owner'])) # function call !!! logger.add(tmpfile.name, compression=email(path, app_name=conf['app_name'], model_owner=conf['model_owner']))","tags":"Programming","url":"articles/Passing-a-function-as-an-argument:-Lambda-Function.html","loc":"articles/Passing-a-function-as-an-argument:-Lambda-Function.html"},{"title":"Using groupBy on multiple columns","text":"Group By X means put all those with the same value for X in the one group. Group By X, Y means put all those with the same values for both X and Y in the one group. To illustrate using an example, let's say we have the following table, to do with who is attending what subject at a university: Table: Subject_Selection Subject Semester Attendee --------------------------------- ITB001 1 John ITB001 1 Bob ITB001 1 Mickey ITB001 2 Jenny ITB001 2 James MKB114 1 John MKB114 1 Erica When you use a group by on the subject column only; say: select Subject, Count(*) from Subject_Selection group by Subject You will get something like: Subject Count ------------------------------ ITB001 5 MKB114 2 …because there are 5 entries for ITB001 , and 2 for MKB114 If we were to group by two columns: select Subject, Semester, Count(*) from Subject_Selection group by Subject, Semester we would get this: Subject Semester Count ------------------------------ ITB001 1 3 ITB001 2 2 MKB114 1 2 This is because, when we group by two columns, it is saying \"Group them so that all of those with the same Subject and Semester are in the same group, and then calculate all the aggregate functions (Count, Sum, Average, etc.) for each of those groups\".","tags":"Programming","url":"articles/Using-groupBy-on-multiple-columns.html","loc":"articles/Using-groupBy-on-multiple-columns.html"},{"title":"Python Object and Reference","text":"变量与对象 Python一切皆对象 可变对象和不可变对象 深拷贝 浅拷贝 C/C++中函数传递参数方式有：按值传递 和 按址传递。而在一切皆对象的Python中则完全不可延续C/C++的思想，而是要有 可变对象 和 不可变对象 的概念才能理解。 变量与对象 Python中的变量与C/C++中变量的概念是不同的。 C/C++： int a = 1 # 在栈上开辟地址为 a 的内存空间，存入值 1 。 a = 2 # 修改变量 a 的值。 int b = a # 将变量 a 赋值给另一个变量变量 b ,本质是：拷贝 a 的值给 b 。 Python Python的变量本质是： 内存对象的引用 。 a = 1 # 变量 a 指向了内存中的一个int型的对象。 a = 2 # 并非像C/C++中给变量 a 重新赋值，而是 a 将会移动并指向另一个对象 2 。当一个对象没有任何标签或引用指向它时，它就会被自动释放。 b = a # 把变量 a 赋给另一个变量 b ，只是给当前内存中对象增加一个引用而已。 Python一切皆对象 Python使用对象模型来储存数据，任何类型的值都是一个对象。所有的python对象都有3个特征： 身份、类型、值。 身份：每一个对象都有自己的唯一的标识，可以使用内建函数 id() 来得到它。这个值可以被认为是该对象的内存地址。 类型：对象的类型决定了该对象可以保存的什么类型的值，可以进行什么操作，以及遵循什么样的规则。 type() 函数来查看对象的类型。 值：对象表示的数据项。 a is b ：通过 id() 判断是否为同一个对象的引用。 a == b ：判断 a 和 b 引用的对象的值是否相同。 可变对象和不可变对象 Python的基本数据类型中。 可变对象： 列表， 字典 不可变对象： 数字，字符串，元组 a = 1 # a 指向内存中一个int型对象 a = 2 # 当将 a 重新赋值时，因为 1 是不可变对象，所以 a 会指向一个新的int型对象，其值为 2 。 lst = [1, 2] # lst 指向内存中一个list类型的对象 lst[0] = 2 # 重新赋值 lst 中第一个元素 因为list类型是可以改变的，所以第一个元素变更为2。更确切的说，lst的第一个元素是int型，重新赋值时一个新的int对象被指定给第一个元素，但是对于lst来说，它所指的列表型对象没有变，只是列表的内容（其中一个元素）改变了。 def foo ( arg ): arg = 5 print ( arg ) x = 1 # 不可变对象 foo ( x ) # 5 print ( x ) # 1 def foo ( arg ): arg . append ( 3 ) x = [ 1 , 2 ] # 可变对象 print ( x ) # [1, 2] foo ( x ) print ( x ) # [1, 2, 3] 对于不可变的对象，类似传值方式；对于可变对象，类似按址传递。 深拷贝 浅拷贝 赋值：简单地拷贝对象的引用，两个对象的id相同。 浅拷贝：创建一个新的组合对象，这个新对象与原对象共享内存中的子对象。 深拷贝：创建一个新的组合对象，同时递归地拷贝所有子对象，新的组合对象与原对象没有任何关联。虽然实际上会共享不可变的子对象，但不影响它们的相互独立性。 浅拷贝和深拷贝的不同仅仅是对组合对象来说，所谓的组合对象就是包含了其它对象的对象，如列表，类实例。而对于数字、字符串以及其它\"原子\"类型，没有拷贝一说，产生的都是原对象的引用。 深拷贝 浅拷贝","tags":"Programming","url":"articles/Python Object and Reference.html","loc":"articles/Python Object and Reference.html"},{"title":"Cygwin","text":"Cygwin中安装软件 Pycahrm中启Cygwin终端 Cygwin中使用windows anaconda Cygwin中安装软件 #download setup-x86_64.exe https://cygwin.com/setup-x86_64.exe $ cd C : cygwin64 $ ./ setup - x86_64 . exe - q - P wget , tar , qawk , bzip2 , subversion , vim , git # git clone https://github.com/transcode-open/apt-cyg $ cd apt-cyg $ mv apt-cyg /usr/local/bin/ $ apt-cyg --help $ cygcheck --help # modify mirror $ apt-cyg --mirror http://mirrors.163.com/cygwin # mirrors backup # ftp://mirror.mcs.anl.gov/pub/cygwin # http://mirrors.163.com/cygwin # ftp://ftp.ges.redhat.com/private/releng/cygwin-1.8 # apt-cyg install man cygwin-doc apt-cyg install vim screenwget subversion openssh Pycahrm中启Cygwin终端 Files|Settings|Tools|Terminal|Shellpath path: C:\\cygwin64\\bin\\bash.exe This is a non-interactive shell, and does not source your profile. The next try is: C:\\cygwin64\\bin\\bash.exe --login -i This produces an error from Pycharm that it cannot start the program correctly. A little checking says the leading command needs to be quoted, else Pycahrm treats the entire line as the name of the command, not as a command followed by flags. OK : \"C:\\cygwin64\\bin\\bash.exe\" --login -i It starts in my home directory, not in my project root. Starting in the project root is one of the nice features of the terminal in IntelliJ. Finally, two changes. First the IntelliJ setting: `\"C:\\cygwin64\\bin\\bash\" -c \"exec /usr/bin/env INTELLIJ=true $SHELL --login -i\" And an addition to my ~/.bashrc : ${INTELLIJ-false} && cd ${OLDPWD-.} Cygwin中使用windows anaconda vim ~/.bashrc 加入以下脚本： #####################By Jerry for using Anaconda on Cygwin################# # Anaconda Environment Selection - Plese set CONDA_BASE_DIR to the directory # containing the base installation of anaconda/miniconda. export CONDA_BASE_DIR =/ cygdrive / c / Users / YCKJ2939 / AppData / Local / Continuum / anaconda3 # Proxy Servers & Network Setup (if needed) export HTTP_PROXY = export HTTPS_PROXY = # IMPORTANT - Ignore carriage returns when using a Cygwin environment. export SHELLOPTS set - o igncr ############################################################################### # Manage conda environments for Python. We check the environment variable # $CONDA_DEFAULT_ENV to see which environment is desired. The default (base) # environment will be chosen if nothing is specified. Note that this variable # will be explicitly managed by the cactivate ( ) function we have defined # below, specifically for the purpose of changing environments. The root # environment is also handled slightly different from the others when it comes # to setting the CONDA_DEFAULT_ENV variable. if [ $ { CONDA_DEFAULT_ENV } ] && [ $ { CONDA_DEFAULT_ENV } != 'base' ] then # SELECT ONE OF THE NON-DEFAULT ENVIRONMENTS export CONDA_PREFIX =$ { CONDA_BASE_DIR } / envs /$ { CONDA_DEFAULT_ENV } else # SELECT THE DEFAULT ENVIRONMENT (and set CONDA_DEFAULT_ENV full path) export CONDA_DEFAULT_ENV = root export CONDA_PREFIX =$ { CONDA_BASE_DIR } fi ############################################################################### # Define cconda and cactivate to facilitate management of conda. alias cconda =$ { CONDA_BASE_DIR } / Scripts / conda . exe cactivate () { export CONDA_DEFAULT_ENV =$ 1 source ~/. bashrc cconda info -- envs } ############################################################################### # PATH - ALl of the anaconda/miniconda path entries appear first. PATH = PATH =$ PATH : $ CONDA_PREFIX PATH =$ PATH : $ CONDA_PREFIX / Library / mingw - w64 / bin PATH =$ PATH : $ CONDA_PREFIX / Library / usr / bin PATH =$ PATH : $ CONDA_PREFIX / Library / bin PATH =$ PATH : $ CONDA_PREFIX / Scripts PATH =$ PATH : $ HOME / scripts PATH =$ PATH : $ HOME / local / bin PATH =$ PATH : / usr / local / bin PATH =$ PATH : / usr / bin export PATH ############################################################################### Somethings to remember for this integration to work. Please install Anaconda directly from installer and not from package manager like Chocolatey, since for this approach to work the envs must be within the anaconda root directory. In order for anaconda python to work in cygwin commandline, you must use python -i . Using just python freezes the screen. Ref: using-anaconda-environments-with-cygwin-on-windows","tags":"Tools","url":"articles/Cygwin.html","loc":"articles/Cygwin.html"},{"title":"Case Class vs Class","text":"定义 比较 pattern matching 继承 case class的代码实现 定义 class的定义: class BankAccount { def deposit ( amount : Int ): Unit = { if ( amount > 0 ) balance = balance + amount } case class的定义: case class Note ( name : String , duration : String , octave : Int ) 创建 BankAccount 和 Note 的实例： val aliceAccount = new BankAccount () val c3 = Note ( \"C\" , \"Quarter\" , 3 ) case class 类实例化不需要 new , case class 有一个默认的 apply 方法来负责对象的创建。 创建带参 case class 时，参数时 val 类型的。 e.g： c3.name = ‘Jerry' //does not compile 比较 val aliceAccount = new BankAccount val bobAccount = new BankAccount // aliceAccount == bobAccount shouldBe False val c3 = Note ( \"C\" , \"Quarter\" , 3 ) val cThree = Note ( \"C\" , \"Quarter\" , 3 ) // c3 == cThree shouldBe True 在Scala中，默认情况下，比较对象将比较它们的引用，即 **按引用比较** 。 但在 case class 实例的情况下，重新定义相等性以比较聚合信息的值，即 **按值比较** 。 pattern matching pattern matching 不适用于 class 用 pattern matching 从 case class 实例中抽取信息 c3 match { case Note ( name , duration , octave ) => \"The duration of c3 is duration\" } 继承 class 可继承， case class 不可继承（因为不可能正确地实现它们的相等） case class的代码实现 case class 只是 class 的一个特例，目的是将多个值聚合为一个单值。Scala显示的支持 case class 是因为在实践中常用。 当我们在定义一个 case class 时，编译器实际定义了一个 使用更多方法 和 伴随对象 的增强 class 。 e.g: case class Note ( name : String , duration : String , octave : Int ) 编译器实际定义： class Note ( _name : String , _duration : String , _octave : Int ) extends Serializable { // Note class // Constructor parameters are promoted to members val name = _name val duration = _duration val octave = _octave // Equality redefinition override def equals ( other : Any ): Boolean = other match { case that : Note => ( that canEqual this ) && name == that . name && duration == that . duration && octave == that . octave case _ => false } def canEqual ( other : Any ): Boolean = other . isInstanceOf [ Note ] // Java hashCode redefinition according to equality override def hashCode (): Int = { val state = Seq ( name , duration , octave ) state . map ( _ . hashCode ()). foldLeft ( 0 )(( a , b ) => 31 * a + b ) } // toString redefinition to return the value of an instance instead of its memory addres override def toString = \"Note(name,duration,octave)\" // Create a copy of a case class, with potentially modified field values def copy ( name : String = name , duration : String = duration , octave : Int = octave ): Note = new Note ( name , duration , octave ) } object Note { // 伴随对象 // Constructor that allows the omission of the `new` keyword def apply ( name : String , duration : String , octave : Int ): Note = new Note ( name , duration , octave ) // Extractor for pattern matching def unapply ( note : Note ): Option [( String , String , Int )] = if ( note eq null ) None else Some (( note . name , note . duration , note . octave )) }","tags":"Programming","url":"articles/Case-Class-vs-Class.html","loc":"articles/Case-Class-vs-Class.html"},{"title":"Singleton & Companion Object","text":"Static in Java Singleton object Companion object Notes for Java programmers Static in Java Static in Java Singleton object 单例对象是一种特殊的类，有且只有一个实例。和惰性变量一样，单例对象是延迟创建的，当它第一次被使用时创建。 当对象定义于顶层时(即没有包含在其他类中)，单例对象只有一个实例。 当对象定义在一个类或方法中时，单例对象表现得和惰性变量一样。 package logging object Logger { def info ( message : String ): Unit = println ( message ) } 方法 info 可以在程序中的任何地方被引用。像这样创建功能性方法是单例对象的一种常见用法。 如何在另外一个包中使用 info 方法： import logging . Logger . info class Project ( name : String , daysToComplete : Int ) class Test { val project1 = new Project ( \"TPS Reports\" , 1 ) val project2 = new Project ( \"Website redesign\" , 5 ) info ( \"Created projects\" ) // Prints \"INFO: Created projects\" } 因为 import 语句 import logging.Logger.info ，方法 info 在此处是可见的。 import 语句要求被导入的标识具有一个\"稳定路径\"，一个单例对象由于全局唯一，所以具有稳定路径。 注意： 如果一个 object 没定义在顶层而是定义在另一个类或者单例对象中，那么这个单例对象和其他类普通成员一样是\"路径相关的\"。这意味着有两种行为， class Milk 和 class OrangeJuice ，一个类成员 object NutritionInfo \"依赖\"于包装它的实例，要么是牛奶要么是橙汁。 milk.NutritionInfo 则完全不同于 oj.NutritionInfo 。 Companion object 定义：当一个单例对象和某个类共享一个名称时，这个单例对象称为 伴生对象。 同理，这个类被称为是这个单例对象的伴生类。 类和它的伴生对象可以互相访问其私有成员。 作用：使用伴生对象来定义那些在伴生类中不依赖于实例化对象而存在的成员变量或者方法。 import scala . math . _ case class Circle ( radius : Double ) { import Circle . _ def area : Double = calculateArea ( radius ) } object Circle { private def calculateArea ( radius : Double ): Double = Pi * pow ( radius , 2.0 ) } val circle1 = new Circle ( 5.0 ) circle1 . area 这里的 class Circle 有一个成员 area 是和具体的实例化对象相关的. 单例对象 object Circle 包含一个方法 calculateArea ，它在每一个实例化对象中都是可见的。 Notes for Java programmers Java中 static成员 <===> Scala中 伴生对象的普通成员 大多数情况下，需要一个对象来保存可用的方法和值/变量，而无需首先实例化某个类的实例。这与Java中的静态成员密切相关。 object A { def twice ( i : Int ): Int = 2 * i } A . twice ( 2 ) // 直接调用 class A () { def twice ( i : Int ): Int = 2 * i } val a = new A () // 需先实例化，再调用 a . twice ( 2 ) 在 Java 代码中调用伴生对象时，伴生对象的成员会被定义成伴生类中的 static 成员。这称为 静态转发。这种行为发生在当你自己没有定义一个伴生类时。 Singleton objects Difference between object and class in scala","tags":"Programming","url":"articles/Singleton-&-Companion-Object.html","loc":"articles/Singleton-&-Companion-Object.html"},{"title":"Binary Tree","text":"二叉树结构定义 分治算法 遍历法 VS 分治法 递归 VS 非递归 前序遍历 中序遍历 后序遍历 层序遍历 之字层序遍历 序列化和反序列化 前中序重构二叉树 二叉树最大深度 二叉树最小深度 平衡二叉树 二叉搜索树 BST 二叉搜索树最近公共祖先 二叉树的最近公共祖先 Same Tree Subtree of Another Tree Binary Tree Maximum Path Sum 二叉树结构定义 struct TreeNode { int val ; TreeNode * left ; TreeNode * right ; TreeNode ( int val ) : val ( val ), left ( nullptr ), right ( nullptr ) {} }; 分治算法 分治法的重点在于 问题的划分 和 返回状态的定义 遍历法 VS 分治法 递归 是实现方式， 遍历法 和 分治法 是可以用 递归 实现的算法思想 Result in parameter vs Result in return value ，所以 分治法 一般不需要全局变量，可实现并行。 遍历法 的结果要改参数，返回参数； 分治法 的结果直接返回，是个更好的接口，因为传入的参数最好不要改。 递归是自顶向下 Top down VS 分治是自底向上 Bottom up 递归 VS 非递归 非递归其实是模拟递归用的Stack 为什么自己模拟的可以，调用计算机的就不行呢 ？ 因为 heap memory ≈ memory size ，new出的stack在里面，不用担心栈溢出。 而 stack memory ≈ process memory 是计算机分给每个程序的一个很小的独占的空间，所以递归的深度太深，容易栈溢出。 前序遍历 递归 def preorderTraversal ( root ): res = [] self . traversal ( root , res ) return res def traversal ( root , res ): if not root : return res . append ( root . val ) traversal ( root . left , res ) traversal ( root . right , res ) 分治 分治法的返回状态定义： 子树的先序遍历结果 List def preorderTraversal ( root ): # end condition if not root : return [] # divide & conquer left = preorderTraversal ( root . left ) right = preorderTraversal ( root . right ) # combine res = [] res . append ( root . val ) res . extend ( left ) res . extend ( right ) # return result return res 非递归 # 1. 首先把root入栈 # 2. 出栈的元素同时放进结果列表 # 3. 右左儿子依次入栈，这样出栈的顺序是先左后右（根节点已出） # 4. 按照次序继续，直到stack为空 def preorderTraversal ( root ): if not root : return [] stack , res = [ root ], [] while stack : node = stack . pop () res . append ( node . val ) if node . right : stack . append ( node . right ) if node . left : stack . append ( node . left ) return res 中序遍历 递归 def inorderTraversal ( root ): res = [] self . traversal ( root , res ) return res def traversal ( root , res ): if not root : return traversal ( root . left , res ) res . append ( root . val ) traversal ( root . right , res ) 分治 def inorderTraversal ( root ): # end condition if not root : return [] # divide & conquer left = preorderTraversal ( root . left ) right = preorderTraversal ( root . right ) # combine res = [] res . extend ( left ) res . append ( root . val ) res . extend ( right ) # return result return res 非递归 对于任一结点 cur ， 若其左孩子不为空，则将 cur 入栈并将 cur 的左孩子置为当前的 cur ，然后对当前结点 cur 再进行相同的处理； 若其左孩子为空，则取栈顶元素并进行出栈操作，访问该栈顶结点，然后将当前的 cur 置为栈顶结点的右孩子； 直到 cur 为 None 并且栈为空则遍历结束 class Solution : def inorderTraversal ( self , root : TreeNode ) -> List [ int ]: if not root : return [] stack = [] cur = root res = [] while stack or cur : if cur : # 左子树入栈到底，找到最左孩子。 stack . append ( cur ) cur = cur . left else : cur = stack . pop () res . append ( cur . val ) cur = cur . right return res 后序遍历 递归 def postorderTraversal ( root ): res = [] self . traversal ( root , res ) return res def traversal ( root , res ): if not root : return traversal ( root . left , res ) traversal ( root . right , res ) append ( root . val ) 分治 def postorderTraversal ( root ): # end condition if not root : return [] # divide & conquer left = preorderTraversal ( root . left ) right = preorderTraversal ( root . right ) # combine res = [] res . extend ( left ) res . extend ( right ) res . append ( root . val ) # return result return res 非递归 pass 层序遍历 使用队列数据结构：collections.deque 队列初始化，根节点先入队列：queue = [root] 查看队列是否为空：while queue 如果分层打印，则先立即缓存该层大小：n = len(queue), for _ range(n) 遍历该层节点依次出队，查看左右孩子，存在即入队尾。 Python from collections import deque class Solution : def levelOrder ( self , root : TreeNode ) -> List [ List [ int ]]: if not root : return [] res = [] queue = deque () queue . append ( root ) while queue : cur_level = [] n = len ( queue ) for _ in range ( n ): node = queue . popleft () cur_level . append ( node . val ) if node . left : queue . append ( node . left ) if node . right : queue . append ( node . right ) res . append ( cur_level ) return res CPP class Solution { public : vector < int > preorderTraversal ( TreeNode * root ) { vector < int > res ; if ( root == nullptr ) retur res ; stack < TreeNode *> stk ; stk . push ( root ); while ( ! stk . empty ()) { TreeNode * node = stk . top (); stk . pop (); res . push_back ( node -> val ); if ( node -> right ) stk . push ( node -> right ); if ( node -> left ) stk . push ( node -> left ); } return res ; } }; 之字层序遍历 class Solution : def zigzagLevelOrder ( self , root : TreeNode ) -> List [ List [ int ]]: if not root : return [] res = [] queue = collections . deque () queue . append ( root ) level = 0 while queue : level_cur = [] n = len ( queue ) for _ in range ( n ): node = queue . popleft () level_cur . append ( node . val ) if node . left : queue . append ( node . left ) if node . right : queue . append ( node . right ) res . append ( level_cur if level % 2 == 0 else level_cur [:: - 1 ]) level += 1 return res 序列化和反序列化 from collections import deque class Codec : def serialize ( self , root ): if not root : return '' res = [] queue = deque () queue . append ( root ) while queue : node = queue . popleft () if node : res . append ( str ( node . val )) queue . append ( node . left ) queue . append ( node . right ) else : res . append ( '#' ) return ',' . join ( res ) def deserialize ( self , data ): if not data : return None nodes = data . split ( ',' ) root = TreeNode ( int ( nodes [ 0 ])) queue = deque ([ root ]) index = 1 while queue : node = queue . popleft () if nodes [ index ] is not '#' : node . left = TreeNode ( int ( nodes [ index ])) queue . append ( node . left ) index += 1 if nodes [ index ] is not '#' : node . right = TreeNode ( int ( nodes [ index ])) queue . append ( node . right ) index += 1 return root 前中序重构二叉树 leetcode solution class Solution { public : TreeNode * buildTree ( vector < int >& preorder , vector < int >& inorder ) { unordered_map < int , int > map ; int n = preorder . size (); // 使用hash结构快速在中序遍历的结果中定位根结点的位置 for ( int i = 0 ; i < n ; ++ i ) { map [ inorder[i ] ] = i ; } return myBuildTree ( map , preorder , inorder , 0 , n - 1 , 0 , n - 1 ); } TreeNode * myBuildTree ( unordered_map < int , int >& map , vector < int >& preorder , vector < int >& inorder , int preorder_left , int preorder_right , int inorder_left , int inorder_right ) { if ( preorder_left > preorder_right || inorder_left > inorder_right ) { return nullptr ; } // 前序遍历第一个结点为根结点 int preorder_root = preorder_left ; // 建立根结点 TreeNode * root = new TreeNode ( preorder [ preorder_root ] ); // 根结点在中序中的位置 int inorder_root = map [ preorder[preorder_root ] ] ; // 中序的左子树左右区间 int inorder_sublefttree_left = inorder_left ; int inorder_sublefttree_right = inorder_root - 1 ; // 中序的右子树左右区间 int inorder_subrighttree_left = inorder_root + 1 ; int inorder_subrighttree_right = inorder_right ; // 前序的左子树左右区间 int preorder_sublefttree_left = preorder_left + 1 ; int preorder_sublefttree_right = inorder_sublefttree_right - inorder_sublefttree_left + preorder_sublefttree_left ; // 前序的右子树左右区间 int preorder_subrighttree_left = preorder_sublefttree_right + 1 ; int preorder_subrighttree_right = preorder_right ; root -> left = myBuildTree ( map , preorder , inorder , preorder_sublefttree_left , preorder_sublefttree_right , inorder_sublefttree_left , inorder_sublefttree_right ); root -> right = myBuildTree ( map , preorder , inorder , preorder_subrighttree_left , preorder_subrighttree_right , inorder_subrighttree_left , inorder_subrighttree_right ); return root ; } } ; 二叉树最大深度 分治法的返回状态定义： 子树的最大深度 class Solution : def maxDepth ( self , root ): # end condition if not root : return 0 # divide & conquer leftDepth = self . maxDepth ( root . left ) rightDepth = self . maxDepth ( root . right ) # combine：根节点的最大深度 = max(左子树最大深度，右子树最大深度) + 1 res = max ( leftDepth , rightDepth ) + 1 # return result return res 二叉树最小深度 分治法的返回状态定义： 子树的最小深度 class Solution : def minDepth ( self , root : TreeNode ) -> int : # end condition if not root : return 0 if None in [ root . left , root . right ]: # 需要讨论左右子树为空的情况 # divide conquer leftDepth = self . minDepth ( root . left ) rightDepth = self . minDepth ( root . right ) # combine res = max ( leftDepth , rightDepth ) + 1 # return result return res else : # divide conquer leftDepth = self . minDepth ( root . left ) rightDepth = self . minDepth ( root . right ) # combine res = min ( leftDepth , rightDepth ) + 1 # return result return res 平衡二叉树 分治法的返回状态定义： 子树的是否是平衡二叉树和子树的深度 (bool, int) class Solution : def isBalanced ( self , root : TreeNode ) -> bool : balance , _ = self . helper ( root ) return balance def helper ( self , root ): # end condition if not root : return True , 0 # divide conquer leftBalance , leftDepth = self . helper ( root . left ) rightBalance , rightDepth = self . helper ( root . right ) # combine & return result if not leftBalance : return False , 0 if not rightBalance : return False , 0 return abs ( leftDepth - rightDepth ) <= 1 , max ( leftDepth , rightDepth ) + 1 二叉搜索树 BST 二叉搜索树基本性质 定义：左子树都比根节点小，右子树都不小于根节点。左右子树也必须是BST。单节点树是BST。 BST的中序遍历是 不降序列 二叉搜索树最近公共祖先 class Solution : def lowestCommonAncestor ( self , root : 'TreeNode' , p : 'TreeNode' , q : 'TreeNode' ) -> 'TreeNode' : if not root : return None if root . val > p . val and root . val > q . val : return self . lowestCommonAncestor ( root . left , p , q ) if root . val < p . val and root . val < q . val : return self . lowestCommonAncestor ( root . right , p , q ) else : return root # 包含p或q就是root和p，q在root两边情况。 二叉树的最近公共祖先 我们自底部遍历，一旦我们到达一个与两个节点之一匹配的节点，我们就将它传递给它的父节点。 否则，在左右孩子中查找： - 如果左右孩子均返回一个节点，p和q存在，root就是LCA。 如果只有其中一个孩子返回一个节点，意味着在左或右分支上找到p或q。 如果左右孩子均没返回节点，则返回None。 只有一个返回节点情况理解：假设在左孩子返回p，右孩子返回None。这意味着q位于节点p下面的某处，其中p被发现我们不需要一直搜索，因为在这种情况下，找到p的节点是LCA。 分治法的返回状态定义： 找到的 p节点 或 q节点 class Solution : def lowestCommonAncestor ( self , root : 'TreeNode' , p : 'TreeNode' , q : 'TreeNode' ) -> 'TreeNode' : # end condition if root in [ None , p , q ]: return root # divide conquer left = self . lowestCommonAncestor ( root . left , p , q ) right = self . lowestCommonAncestor ( root . right , p , q ) # conbine & return result return root if left and right else left or right Same Tree class Solution : def isSameTree ( self , p , q ): if not p and not q : return True if not p or not q : return False if p . val != q . val : return False return self . isSameTree ( p . left , q . left ) and self . isSameTree ( p . right , q . right ) Subtree of Another Tree DST + DST class Solution : def isSubtree ( self , s : TreeNode , t : TreeNode ) -> bool : if not s and not t : return True if not s or not t : return False return self . isSameTree ( s , t ) or self . isSubtree ( s . left , t ) or self . isSubtree ( s . right , t ) Binary Tree Maximum Path Sum 返回状态的定义： maxPathSum , singlePathSum 首先，想一个简化版(single path)，找从root到任意点得最大值。类似于maxDepth，每次加root.val而不再是+1 求单路的时候，如果root加左儿子单路或者右儿子单路最后的值都小于0，则返回0，意味着不要root开始的这个单路了 本题思路 divide & conquer 求最大路径和就等于下面三个值的最大值： 左子树的最大路径和 右子树的最大路径和 左子树单路 + 右子树单路 + root.val class Solution ( object ): def maxPathSum ( self , root ): if not root : return 0 res , _ = self . helper ( root ) return res def helper ( self , root ): if not root : return - 0x7fffffff , 0 left = self . helper ( root . left ) right = self . helper ( root . right ) singlePathSum = max ( left [ 1 ] + root . val , right [ 1 ] + root . val , 0 ) maxPathSum = max ( left [ 0 ], right [ 0 ], left [ 1 ] + right [ 1 ] + root . val ) return maxPathSum , singlePathSum 思路二： DFS ，返回状态的定义： dfs(root)返回的是包括root这个结点的单一路径上的最大值。 则可能的结果有： left + right +root.val (左右子树和根构成路径为最大值） max(left, right) + root.val(左或者右子树和根构成最大值） root.val本身为最大值 和全局变量res比较更新即可。 需要注意的是dfs返回值，可能是 max(left, right) + root.val 某一条路径 root.val 只是该结点（下面都是负的了） class Solution : def maxPathSum ( self , root : TreeNode ) -> int : if not root : return 0 self . res = - 0x7fffffff self . helper ( root ) return self . res def helper ( self , root : TreeNode ): if not root : return - 0x7fffffff left = self . helper ( root . left ) right = self . helper ( root . right ) if left + right + root . val > self . res : self . res = left + right + root . val if max ( left , right ) + root . val > self . res : self . res = max ( left , right ) + root . val if root . val > self . res : self . res = root . val return max ( max ( left , right ) + root . val , root . val )","tags":"Algorithms","url":"articles/Binary-Tree.html","loc":"articles/Binary-Tree.html"},{"title":"Auto-generated subtitles for any video","text":"Autosub Install ffmpeg Install autosub Usage Autosub Autosub is a utility for automatic speech recognition and subtitle generation. It takes a video or an audio file as input, performs voice activity detection to find speech regions, makes parallel requests to Google Web Speech API to generate transcriptions for those regions, (optionally) translates them to a different language, and finally saves the resulting subtitles to disk. It supports a variety of input and output languages (to see which, run the utility with the argument —list-languages) and can currently produce subtitles in either the SRT format or simple JSON . Install ffmpeg sudo add-apt-repository ppa:djcj/hybrid sudo apt-get update sudo apt-get install ffmpeg Install autosub pip install autosub Usage autosub -S en -D en file_path","tags":"Programming","url":"articles/Auto-generated-subtitles-for-any-video.html","loc":"articles/Auto-generated-subtitles-for-any-video.html"},{"title":"Using loguru and notifiers instead of logging","text":"Configuring loguru Recording logging from loguru import logger import tempfile import notifiers Configuring loguru def email ( file_path : str , to : list ): with open ( file_path , 'r' ) as file : msg = file . read () if not msg : return params = { \"subject\" : 'Title' , \"from\" : \"receivers@163.com\" , \"to\" : to , \"host\" : \"smtp.163.com\" , } notifier = notifiers . get_notifier ( \"email\" ) notifier . notify ( message = msg , ** params ) logger . add ( sys . stdout , level = \"DEBUG\" , format = FORMAT ) # output stdout if send_email : tmpfile = tempfile . NamedTemporaryFile () logger . add ( tmpfile . name , # output email level = \"WARNING\" , format = FORMAT , compression = email ( file_path = tmpfile . name , to = 'name@gmail.com' )) Recording logging logger . info ( '...' ) logger . debug ( '...' ) logger . warning ( '...' ) logger . error ( '...' )","tags":"Programming","url":"articles/Using-loguru-and-notifiers-instead-of-logging.html","loc":"articles/Using-loguru-and-notifiers-instead-of-logging.html"},{"title":"Checking DataFrame/Series missing values","text":"Checking DataFrame missing values Checking Series missing values has_missing method Checking DataFrame missing values cols_missing = frame.columns[frame.isna().any()] Checking Series missing values series.isna().any() has_missing method from loguru import logger import pandas as pd from pandas.core.frame import DataFrame from pandas.core.series import Series import traceback def has_missing ( self , values ): \"\"\"Check whether values have missing value. \"\"\" if isinstance ( values , DataFrame ): cols_missing = values . columns [ values . isna () . any ()] if not cols_missing . empty : logger . warning ( f 'The columns { \", \" . join ( cols_missing ) } contain missing values!' + '' . join ( traceback . format_stack ())) return if isinstance ( values , Series ): if values . isna () . any (): logger . warning ( f 'The { Series . name } Series contain missing values! \\n ' + '' . join ( traceback . format_stack ())) return raise ValueError ( 'The argument values requires a DataFrame or a Series.' )","tags":"Programming","url":"articles/Checking-DataFrame/Series-missing-values.html","loc":"articles/Checking-DataFrame/Series-missing-values.html"},{"title":"Binary Search","text":"二分查找初级：二分模板 Classical Binary Search Find First and Last Position of Target ( lower & upper Bound ) Search a 2D Matrix 总结 二分查找进阶：转化为二分问题 Find Minimum in Rotated Sorted Array Find Minimum in Rotated Sorted Array II Search in Rotated Sorted Array II 二分查找高阶：Half Find Peak Element 二分查找初级：二分模板 Classical Binary Search 数组元素无重复 class Solution : def search ( self , nums : List [ int ], target : int ) -> int : if not nums or len ( nums ) == 0 : return - 1 left , right = 0 , len ( nums ) - 1 while left + 1 < right : # 相邻即退出，防止某些问题造成死循环。 mid = left + (( right - left ) >> 1 ) if nums [ mid ] == target : # find target and return immediately due to no duplicated elements return mid elif target < nums [ mid ]: right = mid else : left = mid #if the target is not the mid, check the right and left if target == nums [ left ]: return left if target == nums [ right ]: return right return - 1 模板四要素： 1. left + 1 < right 相邻时结束 2. left + ((right - left) >> 1) 3. nums[mid] == < > 三种情况讨论 4. nums[left] A[right] ? target 35. Search Insert Position 704. Binary Search Find First and Last Position of Target ( lower & upper Bound ) Target有重复，其他元素无重复。 class Solution : def searchRange ( self , nums : List [ int ], target : int ) -> List [ int ]: if not nums : return [ - 1 , - 1 ] lower , upper = - 1 , - 1 # lower left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if target == nums [ mid ]: # target有重复，找到target不立即退出。由于找下界，所以移动right指针。 right = mid elif target < nums [ mid ]: right = mid else : left = mid if target == nums [ right ]: lower = right if target == nums [ left ]: lower = left # upper left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if target == nums [ mid ]: # target有重复，找到target不立即退出。由于找上界，所以移动left指针。 left = mid elif target < nums [ mid ]: right = mid else : left = mid if target == nums [ left ]: upper = left if target == nums [ right ]: upper = right return [ lower , upper ] 34. Find First and Last Position of Element in Sorted Array Search a 2D Matrix class Solution : def searchMatrix ( self , matrix , target : int ) -> bool : if not matrix or not matrix [ 0 ]: return False row , col = len ( matrix ), len ( matrix [ 0 ]) left , right = 0 , row * col - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) i , j = mid // col , mid % col if matrix [ i ][ j ] < target : left = mid elif matrix [ i ][ j ] > target : right = mid else : return True i , j = left // col , left % col if matrix [ i ][ j ] == target : return True i , j = right // col , right % col if matrix [ i ][ j ] == target : return True return False 总结 二分查找的本质是缩小搜索范围： 区间缩小 ===> 剩下两个下标 ===> 判断两个下标 时间复杂度 T(n) = T(n/2) + O(1) = T(n/4) + O(1) + O(1) = T(n/8) + O(1) +O(1) +O(1) = T(1) + logn * O(1) = O(logn) O(1) 极少 O(logn) 几乎都是二分法 O(√n) 几乎是分解质因数 O(n) 高频 O(nlogn) 一般都可能要排序 O(n2) 数组，枚举，动态规划 O(n3) 数组，枚举，动态规划 O(2&#94;n) 与组合有关的搜索 combination O(n!) 与排列有关的搜索 permutation 比 O(n) 更优的时间复杂度，几乎只能是 O(logn) 的二分法。经验之谈： 根据时间复杂度倒推算法是面试中的常用策略 二分查找进阶：转化为二分问题 把具体的问题转变为： 找到数组中的 第一个/最后一个 满足某个条件的位 置/值 。 Find Minimum in Rotated Sorted Array 数组没有重复元素 class Solution : def findMin ( self , nums : List [ int ]) -> int : if not nums : return - 1 left , right = 0 , len ( nums ) - 1 if nums [ right ] > nums [ left ]: return nums [ left ] while left + 1 < right : mid = left + (( right - left ) >> 1 ) if nums [ mid ] < nums [ left ]: right = mid else : # nums[mid] > nums[left] ， 不存在nums[mid] == nums[left]情况。 left = mid return min ( nums [ left ], nums [ right ]) 153. Find Minimum in Rotated Sorted Array Find Minimum in Rotated Sorted Array II 数组包含 重复元素 class Solution : def findMin ( self , nums : List [ int ]) -> int : if not nums : return - 1 left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if nums [ mid ] < nums [ right ]: # 不可与nums[left]比较， [1, 2, 3, 4, 5] right = mid elif nums [ mid ] > nums [ right ]: left = mid else : right -= 1 # 重复元素，保守缩小right界, 防止越过[3, 4]。 [5, 5, 5, 5, 3, 4, 5, 5, 5] return min ( nums [ left ], nums [ right ]) ### Search in Rotated Sorted Array - 数组没有重复元素 class Solution : def search ( self , nums : List [ int ], target : int ) -> int : if not nums : return - 1 left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if nums [ mid ] == target : return mid if nums [ mid ] > nums [ left ]: # 确定有序的子数组, [left, mid]有序 if nums [ left ] <= target < nums [ mid ]: # 确定target是否在[left, mid]有序子数组 right = mid else : left = mid else : # 确定有序的子数组, [mid, right]有序 if nums [ mid ] < target <= nums [ right ]: # 确定target是否在[mid, right]有序子数组 left = mid else : # 否则，target在无序子数组 right = mid if target == nums [ left ]: return left if target == nums [ right ]: return right return - 1 33. Search in Rotated Sorted Array Search in Rotated Sorted Array II 数组包含 重复元素 class Solution : def search ( self , nums : List [ int ], target : int ) -> int : if not nums : return False left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if target == nums [ mid ]: return True if nums [ mid ] > nums [ left ]: # 确定有序的子数组, [left, mid]有序 if nums [ left ] <= target < nums [ mid ]: # 确定target是否在[left, mid]有序子数组 right = mid else : left = mid elif nums [ mid ] < nums [ left ]: # 确定有序的子数组, [mid, right]有序 if nums [ mid ] < target <= nums [ right ]: # 确定target是否在[mid, right]有序子数组 left = mid else : # 否则，target在无序子数组 right = mid else : # nums[mid] == nums[left] left += 1 if target == nums [ left ]: return True if target == nums [ right ]: return True return False 81. Search in Rotated Sorted Array II 二分查找高阶：Half Find Peak Element Conditions: array length is 1 -> return the only index array length is 2 -> return the bigger number's index array length is bigger than 2 -> (1) find mid, compare it with its left and right neighbors (2) return mid if nums[mid] greater than both neighbors (3) take the right half array if nums[mid] smaller than right neighbor (4) otherwise, take the left half class Solution : def findPeakElement ( self , nums : List [ int ]) -> int : if not nums : return - 1 left , right = 0 , len ( nums ) - 1 while left + 1 < right : mid = left + (( right - left ) >> 1 ) if nums [ mid ] > nums [ mid - 1 ] and nums [ mid ] > nums [ mid + 1 ]: return mid if nums [ mid ] < nums [ mid - 1 ]: right = mid elif nums [ mid ] < nums [ mid + 1 ]: left = mid return left if nums [ left ] > nums [ right ] else right","tags":"Algorithms","url":"articles/Binary-Search.html","loc":"articles/Binary-Search.html"},{"title":"Python email","text":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.image import MIMEImage #设置登录及服务器信息 mail_host = 'smtp.163.com' mail_user = 'jerrylsu' mail_pass = input ( '输入授权码102：' ) # 授权码， 非密码 sender = 'jerrylsu@163.com' receivers = [ 'sa517301@mail.ustc.edu.cn' ] #设置eamil信息 #添加一个MIMEmultipart类，处理正文及附件 message = MIMEMultipart () message [ 'From' ] = sender message [ 'To' ] = receivers [ 0 ] message [ 'Subject' ] = 'title' #推荐使用html格式的正文内容，这样比较灵活，可以附加图片地址，调整格式等 with open ( 'Python-logging.md' , 'r' ) as f : content = f . read () #设置html格式参数 part1 = MIMEText ( content , 'html' , 'utf-8' ) #将内容附加到邮件主体中 message . attach ( part1 ) #登录并发送 try : # SMTP服务器设置(地址,端口): server = smtplib . SMTP_SSL ( mail_host , 465 ) # server.set_debuglevel(1) # 连接SMTP服务器(发件人地址, 客户端授权密码) server . login ( mail_user , mail_pass ) server . sendmail ( sender , receivers , message . as_string ()) print ( 'Send success!' ) except smtplib . SMTPException as e : print ( 'Error' , e ) finally : # 退出SMTP服务器 server . quit () #========================================================================= # 加密SMTP # # 使用标准的25端口连接SMTP服务器时，使用的是明文传输，发送邮件的整个过程可能会被窃听。要更安全地发送邮件，可以加密SMTP会话，实际上就是先创建SSL安全连接，然后再使用SMTP协议发送邮件。 #========================================================================= from email import encoders from email.header import Header from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.utils import parseaddr , formataddr , formatdate import smtplib # return Alias_name <xxxx@example.com> def _format_addr ( s ): name , addr = parseaddr ( s ) return formataddr (( Header ( name , 'utf-8' ) . encode (), addr )) # 接收参数: 发件人地址 from_addr = '你的邮箱地址' # 接收参数: 客户端授权密码 passwd = '你的客户端授权密码' # 接收参数: 收件人地址,可多个 to_addrs = [ 'ex@qq.com' , 'ex@163.com' , 'ex@gmail.com' ] # 接收参数: SMTP服务器(注意:是发件人的smtp服务器) smtp_server = 'smtp.126.com' # 接收参数: 邮件主题 subject = '人生苦短' # 接收参数: 邮件正文 plain = '我用python!' # 带附件邮件 # 指定subtype为alternative，同时支持html和plain格式 msg = MIMEMultipart ( 'alternative' ) # 邮件正文中显示图片，同时附件的图片将不再显示 # plain = 'Hello world and hello me!' msg . attach ( MIMEText ( str ( plain ), 'plain' , 'utf-8' )) # 纯文本 # html = '<html><body><h1>Hello</h1><p><img src=\"cid:0\"></p></body></html>' # msg.attach(MIMEText(html, 'html', 'utf-8')) # HTML # 添加附件：即关联一个MIMEBase，图片为本地读取 with open ( '/home/uxeix/Pictures/icon/favicon (Jianshu).ico' , 'rb' ) as f : # 设置附件中的MIME和文件名 mime = MIMEBase ( 'image' , 'jpeg' , filename = 'hot.jpg' ) # 加上必要的头信息 mime . add_header ( 'Content-Disposition' , 'attachment' , filename = 'hot.jpg' ) mime . add_header ( 'Content-ID' , '<0>' ) mime . add_header ( 'X-Attachment-Id' , '0' ) # 把附件的内容读进来 mime . set_payload ( f . read ()) # 用Base64编码 encoders . encode_base64 ( mime ) # 添加到MIMEMultipart msg . attach ( mime ) # 未指定用户别名，则客户端会自动提取邮件地址中的名称作为邮件的用户别名 msg [ 'From' ] = _format_addr ( from_addr ) # msg['To'] = _format_addr(to_addrs) msg [ 'To' ] = ' %s ' % ',' . join ([ _format_addr ( '< %s >' % to_addr ) for to_addr in to_addrs ]) msg [ 'Subject' ] = Header ( str ( subject ), 'utf-8' ) . encode () msg [ 'Date' ] = formatdate () #========================================================================= # 发送邮件 #========================================================================= try : # SMTP服务器设置(地址,端口): server = smtplib . SMTP_SSL ( smtp_server , 465 ) # server.set_debuglevel(1) # 连接SMTP服务器(发件人地址, 客户端授权密码) server . login ( from_addr , passwd ) # 发送邮件 server . sendmail ( from_addr , to_addrs , msg . as_string ()) print ( '邮件发送成功' ) except smtplib . SMTPException as e : print ( e ) print ( '邮件发送失败' ) finally : # 退出SMTP服务器 server . quit ()","tags":"Programming","url":"articles/Python-email.html","loc":"articles/Python-email.html"},{"title":"Python logging","text":"logging总结 Logger：记录器，暴露函数给应用程序，基于日志记录器和过滤器级别决定哪些日志有效。 Handler ：处理器, 将(日志记录器产生的)日志记录发送至合适的目的地。 Filter ：过滤器, 提供了更好的粒度控制,它可以决定输出哪些日志记录。 Formatter：格式化器, 指明了最终输出中日志记录的布局。 import logging from logging import handlers import sys if True : # 1. 创建记录器 logger = logging . getLogger () logger . setLevel ( logging . INFO ) # log日志总开关，默认WARNING级 # 2. 创建handler # 2.1. 输出到终端 console = logging . StreamHandler ( sys . stdout ) # 配置日志输出到控制台 console . setLevel ( logging . WARNING ) # 设置输出到控制台的最低日志级别 # 2.2. 输出到文件 file_logging = logging . FileHandler ( \"example.log\" ) # 配置日志输出到文件 file_logging . setLevel ( logging . INFO ) # 2.3. 和上面的FIleHandler差不多，只是handler对象可以管理文件大小，当文件大于指定的大小后，会自动将当前文件改名，然后重新创建一个新的同名文件继续输出 # file_rotating_file = handlers.RotatingFileHandler(\"cat.log\",maxBytes=1024,backupCount=3) # file_rotating_file.setLevel(logging.INFO) # 2.4. 和上面的handler有点类似，不过，它是通过判断文件大小来决定何时重新创建日志文件，而是间隔一定的时候自动创建日志文件。代表每7天备份文件 # file_time_rotating = handlers.TimedRotatingFileHandler(\"app.log\",when=\"s\",interval=10,backupCount=5) # file_time_rotating.setLevel(logging.INFO) # 2.5. 输出到邮件 # STMPHandler = logging.handlers.SMTPHandler(mailhost=('smtp.163.com', 25), # fromaddr='jerrylsu@163.com', # toaddrs=['sa517301@mail.ustc.edu.cn'], # subject='Data - Issues', # credentials=('jerrylsu','xinyu102')) # STMPHandler.setLevel(logging.WARNING) # 3. 创建格式化器 formatter = logging . Formatter ( fmt = \" %(asctime)s %(filename)s [line: %(lineno)d ] %(levelname)s - %(message)s \" , datefmt = \"%m/ %d /%Y %I:%M:%S %p\" ) # 创建一个格式化对象 console . setFormatter ( formatter ) # 设置格式 file_logging . setFormatter ( formatter ) # file_rotating_file.setFormatter(formatter) # file_time_rotating.setFormatter(formatter) # STMPHandler.setFormatter(FORMATTER) # 4. 添加处理器 logger . addHandler ( console ) logger . addHandler ( file_logging ) # logger.addHandler(file_rotating_file) # logger.addHandler(file_time_rotating) # logger.addHandler(STMPHandler) # 5. 用户使用 logger . debug ( \"debug\" ) logger . info ( \"info\" ) logger . warning ( \"warning\" ) logger . error ( \"error\" ) logger . critical ( \"critical message\" ) log输出到文件除了上面的方式，还有一种便捷的方式：基于 StreamHandler 和重定位符 > 。 $ python3 script.py --argv > log_path 输出到终端的log数据，重定位到log文件中。","tags":"Programming","url":"articles/Python-logging.html","loc":"articles/Python-logging.html"},{"title":"Spark SQL Join","text":"Join in Hive Common Join Map Join Skewed Join Bucket Join Join in Spark SQL Map Join Broadcast Map Join Join in Hive Common Join 在Hive查询的性能调优期间，需要注意的一个方面是执行期间的join的类型。 Common Join是Hive中的默认join类型，也称为Shuffle Join, Distributed Join, Sort Merged Join 。 在join期间，两个表中的所有行都将根据join key分发到所有节点，来自相同join key的值最终在同一节点上。 1. In the map stage, mappers reads the tables and output the join-column value as the key. The key-value pairs are written into an intermediate file. 2. In the shuffle stage, these pairs are sorts and merged. All rows from the same key will be sent to the same reducer instance. 3. In the reduce stage, reducer gets the sorted data and performs the join. 优点：适用于任何大小的表 缺点：1. shuffle操作代价高，消耗网络资源。 2. 存在典型数据倾斜问题。如果join key数据分布不均匀，则相关的reducers会数据过载，导致多数reducers已经完成join操作，而小部分reducers仍在执行join操作。整体的运行时间取决于小部分reducers。 Common Join Map Join Broadcast join is called Map Join in Hive. Common join数据shuffle代价比较高。为了加速Hive查询，可以使用Map Join。 Map Join使用准则：如果join操作中，存在可以装入内存的小表即可。 在join期间，两个表中的所有行都将根据join key分发到所有节点，来自相同join key的值最终在同一节点上。 1. Map Join的第一步是在原始Map Reduce任务之前创建Map Reduce本地任务,此map/reduce任务从HDFS读取小表的数据并将其保存到内存中的哈希表中,然后保存到哈希表文件中。 2. 当原始join Map Reduce任务启动时，它会将哈希表文件移动到Hadoop分布式缓存(这将把哈希表文件填充到每个mapper的本地磁盘,即广播broadcast)。 对于具有大表A和小表B的连接，对于表A的每个映射器，完全读取表B。当较小的表被加载到内存中然后在MapReduce作业的map阶段中执行join时，不需要reducer并且跳过reduce阶段。Map Join比常规默认join执行得更快。 There are two ways to enable MapJoin in Hive. /*+ MAPJOIN(aliasname), MAPJOIN(anothertable) */ 类似于C语言注释，紧跟着放在 SELECT 之后，指示Hive将aliasname表加载如内存。 使用提示使用Map Join指定查询。下面的示例显示较小的表 b 是放在提示中的表，并强制手动缓存表B。 Select /*+ MAPJOIN(b) */ a.key, a.value from a join b on a.key = b.key You can force BroadcastHashJoin using SQL 's BROADCAST hint. Supported hints include BROADCAST , BROADCASTJOIN or MAPJOIN . va q = \"\"\" select /* + broadcast (I) */ C.* from dw_cmc_instnc_chdu I join dw_cmc_cntct C where C.dt >= '20150101' \"\"\" val qBroadcastRight = \"\"\" SELECT /*+ MAPJOIN (rt) */ * FROM range(100) lf inner join range(1000) rt WHERE lf.id = rt.id \"\"\" Map Join Skewed Join Skewed Join Bucket Join Bucket Join Join in Spark SQL SparkSQL支持三种Join算法: - shuffle map join - broadcast map join - sort merge join Map Join Map Join in Hive Map Join时间复杂度O(m + n), 笛卡尔集运算O(m * n) Broadcast Hash Join & Shuffle Hash Join Broadcast Map Join broadcast-join-with-spark import org.apache.spark.sql.functions.broadcast def broadcast [ T ]( df : Dataset [ T ]): Dataset [ T ] Marks a DataFrame as small enough for use in broadcast joins . The following example marks the right DataFrame for broadcast hash join using joinKey . // left and right are DataFrames left_large_dataframe . join ( broadcast ( right_small_dataframe ), \"joinKey\" ) optimizing-apache-spark-sql-joins Broadcast Hash Join","tags":"Programming","url":"articles/Spark SQL Join.html","loc":"articles/Spark SQL Join.html"},{"title":"Spark: Shuffling and Partitioning","text":"Shuffling org.apache.spark.rdd. RDD [(String, Int)]= ShuffledRDD[366] Grouping and Reducing, Example Reminder: Latency Grouping and Reducing, Example - Optimized groupByKey and reduceByKey Running Times Shuffling Partitioning Partitions Hash partitioning Hash Partitioning: Example Range partitioning Range Partitioning: Example Partitioning Data Partitioning Data: partitionBy Shuffling org.apache.spark.rdd. RDD [(String, Int)]= ShuffledRDD[366] Think again what happens when you have to do a groupBy or a groupByKey. Remember our data is distributed! Did you notice anything odd? val pairs = sc . parallelize ( List (( 1 , \"one\" ), ( 2 , \"two\" ), ( 3 , \"three\" ))) pairs . groupByKey () // res2 : org . apache . spark . rdd . RDD [ (Int, Iterable[String ] ) ] // = ShuffledRDD [ 16 ] at groupByKey at < console > : 37 We typically have to move data from one node to another to be \"grouped with\" its key. Doing this is called \"shuffling\". Shuffles Happen Shuffles can be an enormous hit to because it means that Spark must send data from one node to another. Why? Latency! Grouping and Reducing, Example Let's start with an example. Given: case class CFFPurchase(customerid: Int, destination: String, price: Double) 假设我们有瑞士火车公司（ CFF ）移动应用程序用户在过去一个月内购买的数据集RDD。 val purchasesRdd: RDD[CFFPurchaseJ = sc.textFile( ... ) 目标：每个客户在这个月内的旅行次数和花费金额。 val purchasesRdd: RDD[CFFPurchaseJ = sc.textFile( ... ) // Returns: Array[(Int, (Int, Double))] val purchasesPerMonth = purchasesRdd.map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() // groupByKey returns RDD[(K, Iterable[VJ )J .map(p => (p._1, (p._2.size, p._2.sum))) .collect() An example dataset: val purchases = List(CFFPurchase(100, \"Geneva\", 22.25), CFFPurchase (300, ''Zurich'', 42. 10), CFFPurchase(100, \"Fribourg\", 12.40), CFFPurchase (200, ''St. Gallen'', 8. 20), CFFPurchase(100, ''Lucerne'', 31.60), CFFPurchase (300, ''Basel'', 16. 20)) 注意： groupByKey 会为每个 Key 生成一个键值对。 且单个键值对不能跨越多个 worker 节点。 Reminder: Latency 如果不是绝对必要，我们不希望通过网络发送所有数据。 太多的网络通信会导致性能下降。 如何优化？或许我们没有必要通过网络发送所有的键值对。也许我们可以在shuffle之前减少。 这可以大大减少我们必须通过网络发送的数据量。 Grouping and Reducing, Example - Optimized 优化：使用 reduceByKey . 从概念上讲， reduceByKey 可以被认为是： 1. 首先执行 groupByKey 2. 然后 reduce 每个键分组的所有值的组合 然而， reduceByKey 比单独使用 groupByKey 和 reduce 更有效。 Signature: def reduceByKey(func: (V, V) => V): RDD[(K, V)] val purchasesRdd: RDD[CFFPurchaseJ = sc.textFile( ... ) val purchasesPerMonth = purchasesRdd.map(p => (p.customerld, (1, p.price))) // Pair ROD .reduceByKey( ... ) //? 注意：传递给 map 的函数变为 p => (p.customerld, (1, p.price)) 传递给 reduceByKey 怎样的函数可以返回这样形式的结果 (customerid, (numTrips, totalSpent)) ？ val purchasesPerMonth = purchasesRdd.map(p => (p.customerld, (1, p.price))) // Pair ROD .reduceByKey((v1, v2) => (v1 ._1 + v2._1, v1 ._2 + v2._2)) .collect() 1. map 2. reduceByKey reduce on mapper side first ! 从而减少了用于 shuffle 的 key-value pairs 数据量，如下图所示： 3. reduce again after shuffle reduceByKey 方法有什么好处？ 通过首先减少数据集，在 shuffle 期间通过网络发送的数据量大大减少。这可能会导致性能上的重大改进！ groupByKey and reduceByKey Running Times 在真实集群上进行基准测试： Shuffling 回想一下使用 groupByKey 的示例： val purchasesPerCust = purchasesRdd.map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() Grouping all values of key-value pairs with the same key requires collecting all key-value pairs with the same key on the same machine. Shuffling 产生的原因是：将与 Key 相关的所有 Value 移到同一台机器上，从而导致数据在网络中 Shuffle 。 但是Spark怎么知道哪个 Key 放在哪台机器上呢？ - 默认情况下，Spark使用 hash partitioning 来确定哪个 Key 应该将对发送到哪台机器。 Partitioning Partitions RDD中的数据被分成若干个分区。 分区属性： - 分区永远不会跨越多台机器，即同一分区中的元组保证在同一台机器上。 - 群集中的每台计算机都包含一个或多个分区。 - 要使用的分区数是可配置的。 默认情况下，它等于所有 executor 节点上的内核总数。 Spark中提供两种分区： - 散列分区Hash - 范围分区Range 注意：只能在Pair RDD上自定义分区。 Hash partitioning Given a Pair RDD that should be grouped: val purchasesPerCust = purchasesRdd.map(p => (p.customerld, p.price)) // Pair RDD .groupByKey() groupByKey 首先计算RDD对中每个元组的分区p: p = k.hashCode() % numPartitions 然后同一分区中的元组将被发送到托管该分区的计算机 直觉：散列分区尝试根据 Key 在分区之间均匀地分布数据。 Hash Partitioning: Example 考虑一 Pair RDD ，其中 Keys 为 [8,96,240,400,401,800] 和 所需分区数为 4 。 此外，假设 hashCode() 是标识（ n.hashCode() == n ）。 在这种情况下，散列分区在分区之间按如下方式分配 Keys ： p = key % 4 - partition 0: [8, 96, 240, 400, 800] - partition 1: [401] - partition 2: [ ] - partition 3: [ ] 结果是非常不平衡的分布，这会损害性能。 散列分区的目标是尝试均匀地分散 Keys ，在这种情况下， Job 基本上只是在一个节点上展开，并非真正并行计算。 在这种情况下，因为知道散列分区实际上是倾斜的，并且 Keys 是有序且非负的。 我们可以使用范围分区来改进分区，并使其显著均匀。 Range partitioning Pair RDDs may contain keys that have an ordering defined . - Examples: Int, Char, String, … For such RDDs, range partitioning may be more efficient. Using a range partitioner, keys are partitioned according to: 1. an ordering for keys 2. a set of sorted ranges of keys 属性：具有相同范围 Keys 的元组出现在同一台机器上。 Range Partitioning: Example 使用范围分区可以显着改善分布： - 假设：（a） Keys 非负，（b）800是RDD中最大的 Key - 范围集：[1,200]，[201,400]，[401,600]，[601,800] 在这种情况下，范围分区在分区之间按如下方式分配 Keys ： - partition 0: [8, 96] - partition 1: [240, 400] - partition 2: [ 401] - partition 3: [800] 生成的分区更加平衡。 Partitioning Data 如何为数据设置分区？ 有两种方法可以创建具有特定分区的 RDDs ： 1. 在 RDD 上调用 partitionBy ，提供显式的分区程序。 2. Using transformations that return RDDs with specific partitioners. Partitioning Data: partitionBy 调用 partitionBy 会使用指定的分区程序创建 RDD 。 Example: val pairs = purchasesRdd.map(p => (p.customerld, p.price)) val tunedPartitioner = new RangePartitioner(8, pairs) val partitioned = pairs.partitionBy(tunedPartitioner).persist() 创建RangePartitioner需要： 1. 指定所需的分区数。 2. 提供带有序 Keys 的 Pair RDD 。 对该 RDD 进行采样以创建一组合适的排序范围。 重要：partitionBy的结果应始终persist()。 否则，每次使用RDD是都会重复分区操作，而分区又会涉及Shuffle！","tags":"Programming","url":"articles/Spark:-Shuffling-and-Partitioning.html","loc":"articles/Spark:-Shuffling-and-Partitioning.html"},{"title":"Scala with Gradle in Intellij","text":"Create a directory (e.g., demo_proj) for your project. Run gradle init --type scala-library in terminal in the above directory. Import the directory as a Gradle project in IntelliJ. IntelliJ如何导入Gradle项目 Apache Spark setup with Gradle, Scala and IntelliJ","tags":"Programming","url":"articles/Scala with Gradle in Intellij.html","loc":"articles/Scala with Gradle in Intellij.html"},{"title":"Debugging Spark Application","text":"select * from table1 as A join table as B on A.item_id = B.item_id where A.id in (1139426, 1139436) and A.date >= '2018-12-01' yarn logs -applicationId <app ID> > output_file 2019 - 02 - 21 19 : 23 : 41 ERROR ApplicationMaster : 91 - User class threw exception : org . apache . spark . sql . AnalysisException : Found duplicate column ( s ) when inserting into hdfs :// apollo - phx - nn - ha / user / lsu1 / result . parquet : `item_id` ; ...... * ---> A . *","tags":"Programming","url":"articles/Debugging-Spark-Application.html","loc":"articles/Debugging-Spark-Application.html"},{"title":"Linux","text":"1. Redirection of standard output > 2. Redirected output >> 3. Pipes | 4. cat 5. grep & awk 6. chmod 7. ps 8. unzip 9. tar 10. windows ‘\\r\\n' to linux ‘\\n' 11. rsync 12. SSH SSH远程执行命令 无密钥登陆 Auto disconnect 13. pip 14. 设置环境变量 15. lsof 16. siege 17. supervisor 18. brew 19. iTerm2 rzsz 20. lsof 22. hostname 23. kill 24. iTerm2 + powerline + oh-my-zsh 25. iconv 26. /dev ln 1. Redirection of standard output > The default standard output is the screen. > is output redirection symbol and syntax is: $ command > output.file.name https://bash.cyberciti.biz/guide/Standard_output 2. Redirected output >> Appending the output to the same file using >> operator. 3. Pipes | https://bash.cyberciti.biz/guide/Chapter_7:_Pipes_and_Filters 4. cat Displaying The Contents of Files $ cat filename $ cat file > newfile Use a pipe to filter data : $ cat file | less Concatenate files $ cat file1 file2 $ cat file1 file2 > newcombinedfile Create new text files note that if a file already exists, it will be overwritten. $ cat > filename append the output to the same file using >> operator: $ cat >> filename Copy file $ cat oldfile > newfile https://www.cyberciti.biz/faq/howto-use-cat-command-in-unix-linux-shell-script/ 5. grep & awk grep 行处理 awk 列处理 cat jupyter_notebook_config.json | grep password | awk -F '\"' '{print $4}' PWD=$(cat jupyter_notebook_config.json | grep password | awk -F '\"' '{print $4}') 6. chmod $ chmod +x tarbackup.sh $ ./tarbackup.sh 7. ps ps -aux 显示所有包含其他使用者的行程 ps -aux | grep ssh 与 grep 结合，查找特定进程 ps -ef | grep PID , 找到其父进程，然后 kill -9 父进程ID kill -9 pid 8. unzip sudo apt-get install unzip unzip file.zip -d destination_folde unzip 9. tar tar -czvf ***.tar.gz directory tar -xzvf ***.tar.gz https://www.cnblogs.com/52linux/archive/2012/03/04/2379738.html 10. windows ‘\\r\\n' to linux ‘\\n' cat -A filename vim filename && set ff=unix dos2unix tool 11. rsync upload: rsync -avh --progress --delete --exclude=analysis/ src_dir/ host:dest_dir download: rsync -avh --progress host:dest_dir ./ -a, --archive # 归档模式，表示以递归方式传输文件，并保持所有文件属性，等价于 -rlptgoD -r, --recursive # 对子目录以递归模式处理 -l, --links # 保持符号链接文件 -H, --hard-links # 保持硬链接文件 -p, --perms # 保持文件权限 -t, --times # 保持文件时间信息 -g, --group # 保持文件属组信息 -o, --owner # 保持文件属主信息 (super-user only) -D # 保持设备文件和特殊文件 (super-user only) -v, --verbose # 详细输出模式 -h, --human-readable # 输出文件大小使用易读的单位（如，K，M等） --exclude=PATTERN # 指定排除一个不需要传输的文件匹配模式 -exclude=\"*.iso\", --exclude={'file1.txt','dir1/*'} --exclude-from # 指定一个本地文件，里面是需要排除的文件模式，每个模式一行。 --delete # 删除那些接收端还有而发送端已经不存在的文件。注意：如果--exclude传输排除的文件，--delete也不会删除接受端的这个文件。如果要删除，需要--delete-excluded指定。例如：rsync --delete exclude='.git' 接受端不会因--delete而删除发射端排除在外没同步的.git文件，即保留接受端原有的.git文件; 除非指定--delete-excluded='.git'，如rsync --delete exclude='.git' --delete-excluded='.git' 。 --progress # 在传输时显示传输过程 -n参数或--dry-run参数模拟将要执行的操作，而并不真的执行。配合-v参数使用，可以看到哪些内容会被同步过去。 http://www.ruanyifeng.com/blog/2020/08/rsync.html https://download.samba.org/pub/rsync/rsync.html https://rsync.samba.org/how-rsync-works.html 12. SSH SSH远程执行命令 远程执行命令： ssh lsu1@xxx.xxx.xxx.xxx \"df -h\" ssh lsu1@xxx.xxx.xxx.xxx \"df -h; ls\" 远程执行本地脚本： ssh lsu1@xxx.xxx.xxx.xxx < test.sh ssh lsu1@xxx.xxx.xxx.xxx 'bash -s' < test.sh helloworld # 带参数 执行远程服务器上脚本： ssh lsu1@xxx.xxx.xxx.xxx \"/home/lsu1/test.sh\" # 需要绝对路径 无密钥登陆 SSH Client : 192.168.0.12 SSH Remote Host : 192.168.0.11 Create Authentication SSH -Kegen Keys on – 192.168.0.12 ssh-keygen -t rsa ssh-keygen -t rsa -b 4096 -m pem Create .ssh Directory on – 192.168.0.11 ssh 192.168.0.11 mkdir -p .ssh Upload Generated Public Keys to – 192.168.0.11 cat .ssh/id_rsa.pub | ssh 192.168.0.11 'cat >> .ssh/authorized_keys' best way: ssh-copy-id -i ~/.ssh/id_rsa.pub lsu1@139.224.58.222 Set Permissions on – 192.168.0.11 ssh 192.168.0.11 \"chmod 700 .ssh; chmod 640 .ssh/authorized_keys\" Set StrictModes sudo vim /etc/ssh/sshd_config ===> StrictModes no sudo service ssh restart https://www.tecmint.com/ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps Auto disconnect /etc/ssh/sshd_config ClientAliveInterval 60 ClientAliveCountMax 3 restart sshd： Linux: service sshd restart MAC : service: command not found MAC : brew services start sshd 13. pip cd ~ && mkdir .pip && vim .pip/pip.conf [global] index-url = https://pypi.doubanio.com/simple/ timeout = 1000 [install] use-mirrors = true mirrors = https://pypi.doubanio.com// 14. 设置环境变量 Shell临时设置, 终端关闭后失效 export PATH=/cygdrive/c/Users/YCKJ2939/anaconda3:$PATH ~/.bashrc永久设置，只对本用户可见 echo 'export PATH=/cygdrive/c/Users/YCKJ2939/anaconda3:$PATH' >> ~/.bashrc source ~./bashrc /etc/profile永久设置，对所有用户可见 How To Read and Set Environmental and Shell Variables on a Linux VPS 15. lsof lsof -i 16. siege An HTTP / HTTPS stress tester siege http://localhost:8000/?q=pants -c10 -t10s 17. supervisor Supervisor process control system for UNIX 18. brew MAC install brew 19. iTerm2 rzsz https://juejin.cn/post/6844904176707698695 20. lsof 端口号占用情况 lsof -i:port 22. hostname hostname -i 23. kill ps -ef|grep nginx|grep -v grep|cut -c 9-15|xargs kill -9 https://www.joshua317.com/article/36 24. iTerm2 + powerline + oh-my-zsh 25. iconv iconv -c -f gbk -t utf8 file_name -o new_file_name 26. /dev lsu1 @ lsu1 : ~ # lsblk NAME MAJ : MIN RM SIZE RO TYPE MOUNTPOINT vda 252 : 0 0 40 G 0 disk └─ vda1 252 : 1 0 40 G 0 part / lsu1 @ lsu1 : ~ # df -h Filesystem Size Used Avail Use % Mounted on udev 975 M 0 975 M 0 % / dev tmpfs 200 M 6.5 M 193 M 4 % / run / dev / vda1 40 G 28 G 11 G 73 % / tmpfs 997 M 0 997 M 0 % / dev / shm tmpfs 5.0 M 0 5.0 M 0 % / run / lock tmpfs 997 M 0 997 M 0 % / sys / fs / cgroup overlay 40 G 28 G 11 G 73 % / var / lib / docker / overlay2 / 2263075e71 fe3ecd347c2a33571546c607048e740de7da2eac72eec365c6b429 / merged tmpfs 200 M 0 200 M 0 % / run / user / 0 lsblk 查看可用可设备， df -h 块设备 vda1 块设备挂在到目录 / ， mount 与 umount 挂在和卸载挂载点。 mount /dev/nvme3n1 /data 挂载 nvme3n1 硬盘到 /data 目录 echo \"/dev/nvme3n1 /data ext4 defaults 0 0\" >> /etc/fstab # 保证重启生效 ln ln -s /home/data /data :创建目录 /home/data 的软连接 /data rm -rf /data ：删除软连接，一定不能 rm -rf /data/ linux默认会不全 / ，这样删除会把 /home/data/ 目标目录下的内容全部删除！！！","tags":"Tools","url":"articles/Linux.html","loc":"articles/Linux.html"},{"title":"Scala","text":"前言 Functional Programming Part-1: Elements of Functional Programming Pure functions and Side effects Referential Transparency First Class Function & Higher Order Functions Anonymous Functions Immutability Functions Functions Literals Syntax Where do we use function literals? Placeholder Syntax Higher Order Functions VarArgs, Named Arguments, Default Value Partially Applied Functions Part-2: Elements of Functional Programming Strict and Lazy Evaluations Pattern Matching Scala Closures Scala Basics 前言 Functional Programming Part-1: Elements of Functional Programming 什么是函数式编程？(Functional Programming) 函数式编程是一种仅使用纯函数(pure functions)和不可变值(immutable values)编写软件应用程序的方法。 为什么需要函数式编程？ 1. Pure functions and Side effects 2. Referential Transparency 3. First Class Functions & Higher Order Functions 4. Anonymous Functions 5. Immutability 6. Recursion & Tail Recursion 7. Statements 8. Strict and Non-Strict (Lazy) evaluation 9. Pattern Matching 10. Closures Pure functions and Side effects 什么是纯函数？ 首先，什么是函数？ A function relates an input to an output. It is like a machine that has an input and output. And the output is related somehow to the input. There are three main part: - The input - The relationship - The output Math.sqrt(4.0) sqrt就是relationship The Input solely determines the output. 无论在哪里或者多少次调用这个函数，只要输入参数相同，就会得到相同的输出。 The function doestn't change its input. The function doest't do anything else except computing the output. 这个函数不会从文件或者终端读取任何数据，也不会在终端打印或向文件写任何数据。也不会读写任何全局变量或者函数以外的任何。实际上，它不会有IO操作。纯函数类似于专用机器，获取输入，计算输出并返回，没有其他工作。 如果它做了任何影响外界或外界可见的事情，我们称之为 函数的副作用(Side effect) 。副作用就像做主要目的以外的事情. 如果函数没有副作用，它就是纯函数。 如何验证函数是纯函数？ 检测函数的 引用透明性 (Referential Transparency) Referential Transparency 什么是函数的引用透明性？ 如果我们可以用相同的值替换它而不改变程序的行为，则称函数是引用透明性。 即，只要输入相同，总能得到相同的返回值。 e.g. 1: Replace Math . sqrt ( 4 . 0 ) with 2 . 0 # Can you do this ? # Yes . Because Math . sqrt ( 4 . 0 ) will always return 2 . 0 as long as input is 4 . 0 . So sqrt 是纯函数。 e.g. 2: // 全局变量 var g = 10 def rt ( i : Int ): Int = { g = i + g return g } val v1 = rt ( 5 ) // v1 = 15 val v2 = rt ( 5 ) // v2 = 20 函数rt不是纯函数，因为： - 不满足第一条：依赖于全局变量g，输出不是仅仅依赖于输入参数。 - 不满足第三条：修改了外部变量，所以是有副作用的。 至此，理解了 纯函数 ， 副作用 ， 引用透明性 。 总结： 纯函数遵循一下三条规则: 1. 输出只依赖于输入参数值 2. 函数不会修改输入参数值 3. 函数没有副作用 - 如果为相同的参数提供相同的值，则该函数是引用透明的. - 检测函数的引用透明性来判断函数是否为纯函数。 为什么需要纯函数？ 1. 安全的编程方式。纯函数对于代码重用是small, precise, simple, safe and easy. 2. 可组合或者模块化。 3. 容易测试 only asserting return value 4. 可记忆的。caching确定函数，即函数是纯函数，可以缓存输出later use。 5. 惰性 lazy 。 First Class Function & Higher Order Functions 什么是一级函数？ 把函数当做值，即一级函数。 1. 可以赋值给变量 2. 可以作为参数，传递给其他函数 3. 可以作为返回值，从其他函数返回 在Scala中所有函数都是一级函数，一级公民。 什么是高阶函数？ 至少满足一点，即高阶函数： 1. 将一个或多个函数作为参数 2. 返回一个函数作为结果 Anonymous Functions 什么是匿名函数？ 标准函数： def double ( i : Int ) : Int = { return i * 2 } 匿名函数： ( i : Int ) => { i * 2 } : Int function literal syntax 函数文本 匿名函数如何调用？ 赋值变量： val d = (i:Int) => {i * 2} :Int 通过变量调用： d(3) 匿名函数的目的是什么？ 内联函数，只使用一次的函数，提供名字是没有意义的。 e.g. 创建函数返回另一个函数：a little tricky // 返回值省略 def getOps ( c : Int ) = ( i : Int ) => { // your code here val doubler = ( x : Int ) => { x * 2 } val tripler = ( x : Int ) => { x * 3 } if ( c > 0 ) doubler ( i ) else tripler ( i ) } Immutability 什么是不变性？ Immutability = program using constants 优点： 1. 有助于采用数学方法和构建纯函数。 2. 有助于避免各种问题，如不可变对象是线程安全的，易于并发编程。 How can we program without a variable? 方法一： def iFactorial ( n : Int ): Int = { var i = n var f = 1 while ( i > 0 ){ f = f * i i = i - 1 } return f } iFactorial ( 5 ) // = 120 iFactorial函数是纯函数吗？ 可以不用变量实现iFactorial函数吗？ 递归。迭代转递归 convert loops to recursion and avoid mutation def iFactorial ( n : Int ) : Int = { if ( n <= 0 ) return 1 else return n * iFactorial ( n - 1 ) } 方法二： 可变性可能有其明显的优势. Functions ; 是可选的。一行多条语句时，可以用 ; 间隔 return 关键字是可选的。函数总是返回最后一次执行表达式的值 单行函数体的花括号 { } 是可选的 返回值类型是可选的，只要编译器可以推断返回值类型 不要忘记函数体前的 = 无参函数 ( ) 是可选的 A simple example: def myMax ( x : Int , y : Int ) : Int = { if (x > y) return x ; else return y ; } 省略 ; 和 return ： def myMax ( x : Int , y : Int ) : Int = { if ( x > y ) x else y } 一行函数体： def myMax ( x : Int , y : Int ) : Int = { if ( x > y ) x else y } 单行函数体的花括号 { } 是可选的： def myMax ( x : Int , y : Int ) : Int = if ( x > y ) x else y 返回值类型是可选的： def myMax ( x : Int , y : Int ) = if ( x > y ) x else y 这种单行 functions 和 methods 在Scala中常见。同样Scala中多行函数也很常见，但是函数体需要一对花括号 { } ： def myMax ( x : Int , y : Int ) = { if ( x > y ) x else y } 无参函数 ( ) 是可选的： 保留 ( ) ：当函数有副作用时使用 def hWorld() = println(\"Hello World!\") val h = hWorld() val h = hWorld 省略 ( ) ：当函数没有副作用时使用，即纯函数不使用 ( ) def hWorld = println(\"Hello World!\") val h = hWorld 惯例：无参函数没有 ( ) 表明副作用 Functions Literals Syntax 不同的名字，相同的意义： - Anonymous functions - Function literals - Lambda expression 我比较倾向于Function literals，因为函数式编程的基本思想之一是函数被当作 first class citizens , 可以象操作 values 一样操作它： 1. 可以赋值给变量 2. 可以作为参数，传递给其他函数 3. 可以作为返回值，从高阶函数返回 可以用 literal 创建 values ： val s = \"Hello World!\" // create a string value using a string literal val l = 5 | val l:Long = 5 | val l = 5:Long // create a integer value using a integer literal Can we create a function value using a literal? ( parameterName : type , ...) => { function body return [ expr ] } : [ return type ] 无函数名 = 变成 => 返回值类型移到末尾 val f = (x:Int) => { x + 5 } val f = (x:Int) => { x + 5 }:Int 如果把返回值类型移到前面，必须包含输入类型和返回类型。不推荐这种写法，Scala会自动推断： // also like val l:Long = 5 val f:Int=> Int = (x:Int) => { x + 5 } e.g. 创建一个函数值接收一个整型和一个字符串，连接后返回一个字符串。 myFun(5, \"-\") 返回 -5- val myFun:(Int, String)=>String = (x:Int, s:Sring) => { s + x + s }:String val myFun = (x:Int, s:Sring) => { s + x + s } // 推荐写法 Where do we use function literals? Pass it to a higher order functions Return it from higher order functions e.g. val data= List(-250, 57, 54, -33, 43) data.map( (x:Int) => x + 10 ) >>> [[-240, 67, 64, -23, 53]] 当只有一个参数时，参数类型和 ( ) 可省略： data.map( x => x + 10 ) Placeholder Syntax Higher Order Functions VarArgs, Named Arguments, Default Value Partially Applied Functions Part-2: Elements of Functional Programming Strict and Lazy Evaluations Pattern Matching Scala Closures Scala Basics","tags":"Programming","url":"articles/Scala.html","loc":"articles/Scala.html"},{"title":"Spark RDD","text":"核心概念和抽象 RDD RDD Operations Basics Understanding closures Action RDD Persistence Resiliency 高级主题 Execution & Scheduling Caching & Persistence Shared Variables Broadcast Variables Accumulators References Spark RDD要点总结： Spark RDD弹性分布式数据集 1. RDD简介 - RDD的概述 - RDD的属性 2. RDD的创建方式 - 从文件系统中加载数据创建RDD - 通过并行集合创建RDD 3. RDD的处理过程 - RDD的整体处理流程 - Transformation算子 - Action算子 - 编写WordCount词频统计案例 4. RDD的依赖关系 5. RDD机制 - 持久化机制 - 容错机制 6. Spark的任务调度 - DAG的概念 - 任务调度流程 核心概念和抽象 RDD RDD 这是一个核心抽象，既能实现计算的高效执行，又能灵活方便的形式化定义计算 为什么需要一个新的抽象？ MapReduce中的迭代计算： 1. 后续jobs之间的关系仅为用户代码所知，而不是框架。所以，框架无法优化整个计算。 2. 框架必须可靠地持久保存中间数据，从而产生过多的IO Spark将数据保存在内存中，有效地消除了中间磁盘持久性，从而改善了完成时间 RDD Operations RDDs支持两种类型操作： Transformations ：从已的数据集中创建一个新的数据集。 Actions ：在数据集上执行完一个计算后，向 driver program 返回一个值。 例如： map ：是一个 transformation ，通过一个函数传递每个数据集元素，并返回一个表示结果的新RDD。 reduce ：是一个 action ,使用某个函数聚合RDD的所有元素，并将最终结果返回给 driver program 。 Spark中所有的 transformations 都是 惰性的 ，因为他们不会立即计算他们的结果。相反，他们只记得应用于某些基础数据集的转换。 transformations 仅在 actions 需要将结果返回到 driver program 时计算。这种设计使Spark能够更有效地运行。 默认情况下，当每一次在转换后的RDD上执行一个 action 时，它都会重新计算。但是，也可以使用持久化（或缓存）的方法在内存中保留RDD，在这种情况下，Spark会在群集上保留元素，以便在下次查询时更快地访问。 当然也是支持在磁盘上保留RDD，或在多个节点之间复制的。 Basics 1 ： lines = sc . textFile ( \"data.txt\" ) 2 ： lineLengths = lines . map ( lambda s : len ( s )) 3 ： to talLength = lineLengths . reduce ( lambda a , b : a + b ) 1: 从外部文件定义基础RDD。 此数据集未加载到内存中或以其他方式操作： lines 仅仅是指向文件的指针。 2: 将 lineLengths 定义为 map 转换的结果。由于惰性 lineLengths 不会立即计算。 3: 最终运行 reduce ，是一个 action 。此时Spark将计算分解为在不同机器上运行的 Tasks ，并且每台机器都运行其 map 的部分和本地的 reduce ，仅返回其对 driver program 的结果。 如果稍后需要再次使用 lineLengths ，可以用 lineLengths.persist() 在 reduce 之前，将 lineLengths 在第一次计算之后保存在内存中。 Understanding closures Spark的一个难点是在跨集群执行代码时理解变量和方法的范围和生命周期。 修改其范围之外的变量的RDD操作可能经常引起混淆。 函数式编程：理解闭包和延迟计算 1. Example 根据是否在运行在同一JVM中可能表现不同 Spark应用程序运行：本地模式 vs. 集群模式 使用 foreach() 增加 counter ： counter = 0 rdd = sc . parallelize ( data ) # Wrong : Don 't do this!! def increment_counter(x): global counter counter += x rdd.foreach(increment_counter) print(\"Counter value: \", counter) 2. Local vs. cluster modes - 上述代码的行为是未定义的，并且不同模式下运行情况不同。为了执行 Job ，Spark将RDD操作的处理分解为 Tasks ，每个 Task 由 Executor 执行。在执行之前，Spark会计算 Task 的闭包。闭包是 Executor 在RDD上进行计算的时候必须可见的那些变量和方法（在这种情况下是 foreach() ）。闭包会被序列化并发送给每个 Executor 。 发送给每个 Executor 的闭包中的变量是副本，因此，当 foreach() 函数内引用 counter 时，它不再是 driver 节点上的 counter 。 driver 节点的内存中仍有一个 counter ，但该变量是对 Executor 不可见的！ Executor 只能看到序列化闭包的副本。因此， counter 的最终值仍然为0，因为 counter 上的所有操作都引用了序列化闭包内的值。 在本地模式，在某些情况下，该 foreach() 函数实际上将在与 driver 相同的JVM内执行，并且会引用相同的原始 counter ，这样是可能实际更新它。 Spark中的 Accumulator 专门用于提供一种机制，用于在集群中的工作节点之间执行拆分时安全地更新变量。 Action RDD Persistence Resiliency Spark如何实现弹性计算？（尽管集群出现机器故障，但仍可以继续计算操作） 1. tracking lineage 2. assuming deterministic & side-effect free execution of transformations(including closures) 3. assuming idempotency for actions 4. increasing durability of a data set 提高数据集的持久性 It is important to keep in mind that all the closures pass to Spark, must be deterministic and side effect free. Actions require a stronger property, idempotency. 所有的闭包都传递给Spark，必须是确定性的，并且没有副作用。 What is the lineage? 分区依赖关系图 ，包含计算中涉及数据源的所有分区。 What happens if the dependencies of a failed partition fails as well ? 重新启动计算。首先重新计算这个分区的依赖，然后再计算这个分区。 Key Questions: 1. 数据集必须具备哪些功能才能实现为RDD？ partitions, iterator and dependencies 高级主题 Execution & Scheduling 当在Spark上运行我们的应用程序 ( 调用一个Action ) 时会发生什么？ 1. SparkContext ：是应用的核心 - 告诉应用如何访问集群 - 协调集群上的进程集来运行我们的应用 - 在同一应用程序内，调度多个并发作业 - 在不同应用程序间，控制动态资源分配 Cluster Manager : 用于获取群集资源的一个外部服务。例如： YARN , Mesos or a standalone Spark cluster.(资源经纪人) Jobs, stages, tasks Job ：在响应 Spark action 时而产生的活动 stages ：将 Job 分解成更小的 tasks 集合，叫做 stages Tasks ： job scheduler 为所有的 job stage 创建 Tasks Task ：由 Executor 执行的一个单位工作 Action -> Job -> Job stages -> Tasks 最终， Tasks 被分发给 Executors ，执行实际的工作。 Example : 图 1 . Invoking an action Job stages 和 Tasks的区别？ - Job stage ： 是跨越物化边界的流水线计算。定义在RDD level，不是可立即可执行的。a pipelined computation spanning between materialization boundaries. not immediately executable. Task ： 是绑定到特定分区的job stage，是可立即可执行的。a job stage bound to particular partitions. immediately executable. The idea behind the job stages is to pipeline computation as much as possible, avoiding the unnecessary data materializations. Transformations with narrow dependencies allow pipelining For example, if you applied two filter transformations in a row, it is not necessary to serialize and deserialize data in between. You can simply pass the data through the next predicate. Data materialization occurs in a few places. when reading, shuffling, or passing data to an action. This is where the distinction between narrow and wide dependencies comes up. - Materialization happens when reading, shuffling or passing data to an action. Narrow dependencies allow pipelining. Wide dependencies forbid it. Caching & Persistence 1. Spark如何管理中间数据？ 2. 如何向Spark提示我们的访问模式以获得更好的弹性和性能？ Shared Variables Spark中的第二个抽象是可以在并行操作中使用的 共享变量 。 默认情况下，当Spark并行运行一个函数作为不同节点上的一组 Tasks 时，它会将函数中使用的每个 变量的副本 发送给每个 Task 。即Spark实际上操作的是这个函数所用变量的一个独立副本。这些变量会被复制到每台机器上，并且这些变量在远程机器上的所有更新都不会传递回 driver program 。 有时，变量需要跨任务共享，或者在任务和驱动程序之间共享。通常跨任务的读写共享变量是低效的，但是，Spark还是为两种常见的使用模式提供了两种有限的共享变量： 广播变量 ( Broadcast Variables ) 和累加器 ( Accumulator ) 。 Broadcast Variables 1. 为什么需要广播变量？ 如果我们要在分布式计算里面分发大的对象，例如：字典，模型等，这个都会由 Driver 端进行分发。一般来讲，如果这个变量不是广播变量，那么每个 Task 就会分发一份，这在 Task 数目十分多的情况下 Driver 的带宽会成为系统的瓶颈，而且会大量消耗 Task 服务器上的资源，如果将这个变量声明为 广播变量 ，那么只是每个 Executor 拥有一份，由这个 Executor 启动的 Task 会共享这个变量，节省了通信的成本和服务器的资源。 注： - 广播变量是只读的共享变量 - 用于共享字典和模型 - 能不能将一个RDD使用广播变量广播出去？ 不能，因为RDD是不存储数据的。可以将RDD的结果广播出去 - 广播变量只能在 Driver 端定义和修改，不能在 Executor 端定义和修改 - 如果 Executor 端用到了 Driver 的变量，如果不使用 广播变量 在 Executor 有多少 Task 就有多少 Driver 端的变量副本 - 如果 Executor 端用到了 Driver 的变量，如果使用 广播变量 在每个 Executor 中只有一份 Driver 端的变量副本 2. 使用广播变量 通过调用 SparkContext.broadcast(v) 从变量 v 创建广播变量。 广播变量是 v 的包装器，可以通过调用 value 方法访问其值： >> broadcastVar = sc.broadcast([1, 2, 3]) <pyspark.broadcast.Broadcast object at 0x102789f10> >> broadcastVar.value [1, 2, 3] 创建广播变量后，应该在群集上运行的任何函数中使用它而不是值v，这样v不会多次传送到节点。 另外，在广播之后不应修改对象v，以确保所有节点获得广播变量的相同值（例如，如果稍后将变量发送到新节点）。 Accumulators 在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个 Accumulator ，那么它将在被改变时不会再 driver 端进行全局汇总，即在分布式运行时每个 Task 运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为 Accumulator 后，该变量就会有分布式计数的功能。 1. 使用累加器 - Driver 端创建： SparkContext.accumulator(v) - Executor 端更新：集群上运行的 Task 更新： add 和 += - Driver 端读取： value >> accum = sc.accumulator(0) >> accum Accumulator<id=0, value=0> >> sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x)) ... 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s >> accum.value 10 注： 累加器不会改变Spark的惰性模型。 如果在RDD上的操作中更新它们，则只有在RDD作为 action 部分计算时才更新它的值。 因此，在像 map() 这样的惰性 transformation 中进行累积器更新时，不能保证执行累加器更新。 accum = sc . accumulator ( 0 ) def g ( x ) : accum . add ( x ) return f ( x ) data . map ( g ) # Here, accum is still 0 because no actions have caused the `map` to be computed. References [1] Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing [2] RDD Programming Guide [3] Coursera: Spark RDD [4] StackOverflow: Internal Work of Spark [5] Advanced Apache Spark- Sameer Farooqui (Databricks) [6] A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks) [7] Introduction to AmpLab Spark Internals","tags":"Programming","url":"articles/Spark-RDD.html","loc":"articles/Spark-RDD.html"},{"title":"Adaboost","text":"Boosting AdaBoost算法 AdaBoost缺点 算法原理 AdaBoost重点 引用 引：理解CART分类回归树 Boosting Boosting , short for Adaptive Boosting, is a machine learning ensemble meta-algorithm for primarily reducing bias , and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. AdaBoost算法 在算法初始化阶段，为每一个样本赋予一个相等的权重,同等概率分布（样本数的倒数），即每个样本在开始都是一样重要的。接下来，每一次训练后得到的模型，对数据点的估计会有所差异，所以在每一步结束后，我们需要对权重进行处理，而处理的方式就是通过增加错分类的样本点的权重，同时减少分类正确的样本点的权重。这样能够确保，如果某些点经常被分错，那么就会被\"严重关注\"，也就会被赋予一个很高的权重。然后等进行了N次迭代（迭代次数由用户指定），将会得到N个简单的弱学习器，最后将它们组合起来，可以对它们进行加权（错误率越大的弱学习器其权重值越小，错误率越小的弱分类器权重值越大）或者让它们进行投票等得到一个最终的模型，即带权加法模型。 AdaBoost缺点 AdaBoost对噪声和离群点敏感 可扩展性方面，由于提升的时序性，不能进行并行处理。 算法原理 初始权重，均匀分布： $$D_i=(w_{11}, ... ,w_{1i}, ... ,w_{1N}), w_{1i}=\\frac{1}{N}$$ m=1,2,…,M迭代： 2.1 由带权数据集学习弱分类器： $$G_m(x)$$ 2.2 弱分类器 \\(G_m(x)\\) 在加权数据集上的分类误差：是被 \\(G_m(x)\\) 误分类样本的权重之和。 $$e_m=\\sum_{i=1}&#94;N w_{mi}I(G_m(x_i) \\neq y_i)=\\sum_{G_m \\neq y_i}w_{mi}$$ 2.3 The weight of weak learners: $$\\alpha_m=\\frac{1}{2}\\log\\frac{1-e_m}{e_m}$$ 2.4 Re-Weighting: $$w_{m+1,i}=\\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i)),$$ $$Z_m=\\sum_{i=1}&#94;Nw_{mi}exp(-\\alpha_my_iG_m(x_i))$$ 弱分类器的线性组合： $$f(x)=\\sum_{m=1}&#94;M\\alpha_mG_m(x)$$ Strong learner: $$G(x)=sign(f(x))$$ AdaBoost重点 AdaBoost怎样实现更加关注误分类的数据？ 通过给数据样本增加权重 Weighted data ，分错的数据在下一轮学习中提高权重，反之减小权重。 \"re-weighting\" 为什么通过调高误分类数据的权重可以使下一轮的弱分类器更加关注？(数据权重的思想) 选择决策点与带权分类误差 弱分类器的分类误差由误分类权重累加，当寻找最佳划分点时，总是寻找误差最小的，则权重高的数据会尽可能分对，才能使误差尽可能的小，由此实现更加关注误分类的点。 数据权重主要用于弱分类器寻找其分类误差最小的决策点 数据权重和弱分类器权重 AdaBoost主要为减少偏差 Adaboost使用的是自适应的方法，其中概率分布式变化的，关注的是难分类的样本。 引用 PPT - A Gentle Introduction to Gradient Boosting Scikit-learn - GRADIENT BOOSTING","tags":"ML","url":"articles/Adaboost.html","loc":"articles/Adaboost.html"},{"title":"CART","text":"ID3 CART CART算法 引子：一些机器学习算法（如线性回归）创建的模型需要拟合所有的样本数据。 问题： 1：数据特征众多且其间关系繁杂而密切，构建全局模型比较困难。 2：真实世界的数据多数是非线性的，无法用全局线性模型拟合。 解决方法： 将数据集划分为多个易于建模的子数据集，子数据集可以用各种技术建模。依然难以建模的子数据集可以继续划分。在这种划分方式下， 树结构 和 回归法 相当有用。 ID3 概述： ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优。在ID3中，每次根据\"最大信息熵增益\"选取当前最佳的特征来分割数据，并按照该特征的所有取值来切分，一旦按某特征切分后，该特征在之后的算法执行中，将不再起作用。ID3算法核心是根据\"最大信息熵增益\"原则选择划分当前数据集的最好特征。在建立决策树的过程中，根据特征属性划分数据，使得原本\"混乱\"的数据的熵(混乱度)减少，按照不同特征划分数据熵减少的程度会不一样。在ID3中选择熵减少程度最大的特征来划分数据（贪心），也就是\"最大信息熵增益\"原则。 ID3算法缺点： ID3不能直接处理连续型特征。 数据集切分过于迅速。 信息增益作为特征选择标准的缺陷：信息增益在特征值多的特征上计算结果大于特征值少的特征，这会导致 ID3 偏向选择有较多分枝的特征，而该特征不一定是最优的选择。 CART CART是分类回归树。 决策树模型是将输入空间划分成若干个区域。CART是二叉树，每一次将区域二分，子区域递归。 CART树的损失函数： \\(Loss = \\min_{j, s}[\\min_{c_1}L(y&#94;{(i)}, c_1) + \\min_{c_2}L(y&#94;{(i)}, c_2)]\\) \\(c_m\\) 是子区域上的均值： \\(c_m = \\frac{1}{N_m}\\sum_{x_i \\in R_m(j, s))}y_i, \\ \\ \\ x \\in R_m, \\ \\ m = 1, 2\\) \\(x&#94;(j)\\) : 选择第j个特征作为切分变量 s ：选择该特征的特征值s作为切分点 \\(R_m\\) ：划分出的子区域 所以在CART模型中，我们需要遍历输入特征 \\(x&#94;{(j)}\\) ，找到最佳划分点 s ，即损失函数值最小。 CART算法 对于分类问题，采用基尼系数选择最优特征，同时决定该特征的最优划分点。 对于回归问题，常选用 L2 距离作为损失函数 \\(L(y&#94;{(i)}, c)\\) ，这种回归树称为 最小二乘回归树 。 选择 (j, s) $$Loss = \\min_{j, s}[\\min_{c_1} \\sum_{x_i \\in R_1(j, s))}(y_i - c_1)&#94;2 + \\min_{c_2} \\sum_{x_i \\in R_2(j, s))}(y_i - c_2)&#94;2]$$ 用选定的 (j, s) 划分区域并输出相应区域的均值 $$R_1(j, s) = {x | x&#94;{(j) \\leq s}}, \\ \\ R_2(j, s) = {x | x&#94;{(j) > s}}$$ $$c_m = \\frac{1}{N_m}\\sum_{x_i \\in R_m(j, s))}y_i, \\ \\ \\ x \\in R_m, \\ \\ m = 1, 2$$ 在两个子区域上递归1， 2步骤直到满足终止条件 将输入空间划分为 M 个区域 \\(R_1, R_2 ,..., R_M\\) ，生成决策树","tags":"ML","url":"articles/CART.html","loc":"articles/CART.html"},{"title":"Information Theory","text":"Information Theory 信息\\(X\\) 信息量\\(H(X)\\) Information Entropy 信息熵的计算 Information Gain Information Theory 信息 \\(X\\) 信息 \\(X\\) 即信源 \\(X\\) ：表示一段信息，如文本、语音等等。 信源的不确定性 ：信源发出的消息不确定性越大，收信者获取的信息量就越大。如果信源发送的消息是确切的，则对收信者来说没有任何价值（没有信息量）。衡量不确定性的方法就是考察信源 \\(X\\) 的概率空间。X包含的状态越多，状态 \\(X_i\\) 的概率 \\(P_i\\) 越小，则不确定性越大，所含有的信息量越大。 信息量 \\(H(X)\\) 如何衡量信息 \\(X\\) 的大小，如何衡量信息 \\(X\\) 所包含的信息量？ \\(H(X_1) > H(X_2); H(X_1) = H(X_2);H(X_1) < H(X_2)\\) 自信息量H(X) ：一个事件(消息)本身所包含的信息量，由事件的不确定性决定的。 如何用数学模型表示 \\(X\\) 的信息量？ 1. \\(H(X) <=> \\frac{1}{P(X)}\\) 单调性 ： 信息量 \\(H(X)\\) 与信息 \\(X\\) 出现的概率 \\(P(X)\\) 成反比，即信息 \\(X\\) 出现的概率越大，则 \\(X\\) 的信息量越小。 2. \\(H(X_1, X_2) <=> H(X_1) + H(X_2)\\) 可加性 ：信息 \\(X_1\\) 与 \\(X_2\\) 是独立随机变量可加(暂且简单认为独立，不独立有条件熵) 3. \\(H(X)\\geq0\\) 非负性 寻找一个函数 \\(H\\) 同时满足以上三点，即： 随机事件 \\(X_i\\) 发生概率为 \\(P(X_i)\\) ，则 信息量函数 定义为： $$H(X)=\\log\\frac{1}{P(X)}=-\\log{P(X)}$$ 可加性证明： \\(H(X_1,X_2)=\\log\\frac{1}{P(X_1,X_2)}=\\log\\frac{1}{P(X_1)P(X_2)}= \\log\\frac{1}{P(X_1)}+\\log\\frac{1}{P(X_2)}\\) ， \\(X_1\\) , \\(X_2\\) 相互独立 Information Entropy 定义：信息量 \\(H(X)\\) 在 \\(P(X)\\) 分布下的数学期望： $$Entropy(X)=E_x[H(X)]=-\\sum_xp(x)\\log{p(x)}$$ 热力学第二定律 薛定谔.《生命是什么》 （第六章 有序，无序和熵） 基本思想：一个正常的人若要维持高序的状态，则必须要吸收负熵来维持高序稳定的状态，否则我们的熵会趋于增大而变的无序。所以人需要吃食物，食物是高序稳定的，经过吸收变得无序产生负熵来维持我们高序稳定状态。 信息熵可以描述数据的混合程度。 熵越大，混合度越高，数据纯度越低。 信息熵的计算 数据集 \\(D\\) ： X X X | O O O X X X O O - \\(ori\\_entropy(X)\\) ：最初整个系统（数据集 \\(D\\) ）的固定熵，即 经验熵 （李航，统计学习方法） $$ori\\_entropy(D)=-\\frac{1}{2}\\log\\frac{1}{2}-\\frac{1}{2}\\log\\frac{1}{2}=1$$ - 根据某个特征将数据集 \\(D\\) 划分为 \\(D_-\\) (X X X)和 \\(D_+\\) (O O O X X X O O)： $$entropy(D_-)=0$$ $$entropy(D_+)=-\\frac{2}{7}\\log\\frac{2}{7}-\\frac{5}{7}\\log\\frac{5}{7}$$ 即数据集划分后两个子数据集的信息熵。 由这样划分数据集之后，整个系统（数据集 \\(D\\) ）的信息熵有何变化呢？ 由此引入了 信息增益(Information Gain) 。 def cal_entropy ( dataset ) : '''Calculate the datasets' entropy . : param dataset : The dataset needs to calculated . ''' dataset_size = len(dataset) label_count = {} for data in dataset: label = data[-1] if label not in label_count: label_count[label] = 0 label_count[label] += 1 entropy = -sum([(count / dataset_size) * log((count / dataset_size), 2) for count in label_count.values()]) return entropy def cond_entropy(dataset, feature_idx): ''' Calculate the weighted entropy of several sub datasets . : param dataset : The raw dataset : param feature_idx : The index of feature splited the dataset . ''' dataset_size = len(dataset) sub_datasets = {} for data in dataset: feature_value = data[feature_idx] if feature_value not in sub_datasets: sub_datasets[feature_value] = [] sub_datasets[feature_value].append(data) # Sub dataset' s weighted entropy cond_entropy = sum ( [ (len(sub_dataset) / dataset_size) * cal_entropy(sub_dataset) for sub_dataset in sub_datasets.values() ] ) return cond_entropy Information Gain 信息增益：原始数据集 \\(D\\) 的熵 减去 按特征 \\(A\\) 划分若干个子数据集 \\(D_i\\) 的加权熵 信息增益：由于熵的减小，而增加信息的获得是多少。 一个特征的信息增益就是由于使用这个特征分割数据集而导致的期望熵降低。在信息增益中，衡量标准是看特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。对一个特征而言，系统有它和没它时信息量将发生变化，而前后信息量的差值就是这个特征给系统带来的信息量（系统熵降低，则信息量减小，则获取更多的信息量，即信息增益）。 $$IG=ori\\_entropy-\\sum_i w_i \\cdot entropy(D_i)=ori\\_entropy-\\sum_i \\frac{|D_i|}{|D|} \\cdot entropy(D_i)$$ 信息增益越大越好，还是越小越好？ 信息增益是：原始数据集 \\(D\\) 的熵减去按特征 \\(A\\) 划分若干个子数据集 \\(D_i\\) 的加权熵。我们的目的是使每一个子集的熵最小（最小代表每个子集都是一类数据，高度有序的状态，高纯度），即加权熵尽量小，则IG越大。 根据IG准则的特征选择方法是什么？ 对训练数据集（或子集） \\(D\\) ，计算其每一个特征的信息增益，选择信息增益最大的特征。 def info_gain ( entropy , cond_entropy ) : return entropy - cond_entropy","tags":"ML","url":"articles/Information-Theory.html","loc":"articles/Information-Theory.html"},{"title":"Data Structure and Algorithm in Python","text":"Sort Quick Sort 引例：荷兰国旗问题🇳🇱 Heap Sort Heap 堆的定义 堆的特点与优势 大根堆的实现 建立大根堆 堆化 堆排序 BFS & DFS BFS DFS Recursion Template Example: Divide and Conquer Template Example: Dynamic Programming Example: Sort Quick Sort 引例：荷兰国旗问题🇳🇱 Heap Sort https://docs.python.org/3/library/heapq.html#heapq.heapify 时间复杂度O(N*logN)，额外空间复杂度O(1)。 堆结构非常重要 堆结构的 heapInsert 和 heapify 堆结构的增大和减少 如果只是建堆的过程，时间复杂度为O(N) 优先级队列结构，就是堆结构 Heap 堆的定义 堆结构 就是一个 完全二叉树 ，数组的结构实现，通过约定下标规则。 对于任意下标为 i 的结点， 左孩子： 2*i + 1 右孩子： 2*i + 2 父节点： (i-1) // 2 大根堆 ：在一棵完全二叉树中，任何一棵子树的最大值都是这棵子树的根，所形成的结构叫大根堆。小根堆类似。 堆的特点与优势 大/小根堆有一个很重要的属性：它的最大/小元素始终是根节点 heap[0] 。 堆的调整代价只和 层数 有关，所以 入堆 和 出堆 的代价只有 O(lgN) 。 大根堆的实现 This implementation uses arrays for which heap[i] > heap[2*i+1] and heap[i] > heap[2*i+2] for all i , counting elements from zero. 给定数组，都可根据约定 视其为堆 ，但其不是大根堆， 如何将数组调整为大根堆 ？ 建立大根堆 heapInsert : 经历一个新结点加入一个已经调整好的堆中，同时往上调整的过程。调整停止条件：当加入结点值不大于其父节点时， 调整停止。 堆在数组上可伸缩 def insertHeap ( arr , index ) : par_i = ( index - 1 ) // 2 while par_i >= 0 and arr [ index ] > arr [ par_i ] : # 插入结点值比父结点大时 ， 往上调整 arr [ index ] , arr [ par_i ] = arr [ par_i ] , arr [ index ] # 与父结点交换 index = par_i # 插入结点来到父节点的位置 par_i = ( index - 1 ) // 2 def createHeap ( arr ) : if arr == None or len ( arr ) < 2 : return arr for i in range ( len ( arr )) : # 依次将结点加入堆中 ， 最终将数组 （ 堆 ） 调整为大根堆 insertHeap ( arr , i ) return arr 时间复杂度分析： O(N) 当第 i 个结点加入堆中时， 0 ~ i-1 已经调整为大根堆，其高度为 O(log(i-1)) ，即调整代价为 O(log(i-1)) 。(沿其父结点依次向上> 比较调整) 所以, N 个结点的调整代价为： O(lg1) + O(lg2) + ... + O(lgN) 收敛于 O(N) 堆化 def heapify ( arr , index , heapSize ) : left = 2 * index + 1 # 左孩子未越界 ， 在堆上 ， 继续循环判断是否下沉 while left < heapSize : # 1. 求左右孩子最大的下标 largest = left + 1 if ( left + 1 ) < heapSize and arr [ left+1 ] > arr [ left ] else left # 2. 最大孩子和本结点最大的下标 largest = index if arr [ index ] >= arr [ largest ] else largest # 3. 如果最大的结点就是自身 ， heapify完成跳出 if largest == index : break # 4. 否则 ， 交换下沉 arr [ largest ] , arr [ index ] = arr [ index ] , arr [ largest ] index = largest left = 2 * index + 1 堆排序 堆大小：heapSize = len(arr) 数组最后一个数下标：heapSize -= 1 把数组 arr 创建为大根堆 堆顶 arr[0] 与数组最后一个数 arr[heapSize] 交换 将堆的大小缩小 heapSize -= 1 0~heapSize 做 heapify 至 2 循环，直到堆大小减到0，数组有序 def heapSort ( arr ) : createHeap ( arr ) heapSize = len ( arr ) while heapSize > 1 : heapSize -= 1 arr [ 0 ] , arr [ heapSize ] = arr [ heapSize ] , arr [ 0 ] heapify ( arr , 0 , heapSize ) BFS & DFS BFS def bfs ( graph , start , end ) : queue = [] queue . append ( [ start ] ) visted . add ( start ) while queue : node = queue . pop () # 标记已被访问 visted . add ( node ) process ( node ) # 1. 寻找后继结点 2. 检查后继结点是否被访问 nodes = generate_related_nodes ( node ) queue . push ( nodes ) # other process work .... DFS # Recursion visted = set () def dfs ( node , visted ) : visted . add ( node ) # process current node here ... for next_node in node . children () : if next_node not in visted : dfs ( next_node , visted ) # Non - Recursion def dfs ( tree ) : if not tree . root : return None visted , stack = [], [ tree . root ] while stack : node = stack . pop () visted . add ( node ) process ( node ) nodes = generate_related_node ( node ) stack . push ( nodes ) # other processing work ... Recursion 用相同的方法解决规模不同的相同问题。 相同的方法：函数 问题的规模： 函数参数控制 递归是一种特殊的循环，通过 函数体 循环。 factorial(6): 6 * factorial(5) 6 * (5 * factorial(4)) 6 * (5 * (4 * factorial(3))) 6 * (5 * (4 * (3 * factorial(2)))) 6 * (5 * (4 * (3 * ( 2 * factorial(1))))) 6 * (5 * (4 * (3 * ( 2 * 1)))) 6 * (5 * (4 * (3 * 2))) 6 * (5 * (4 * 6)) 6 * (5 * 24) 6 * 120 720 把问题转化为规模缩小了的同类子问题 有明确的不需要继续进行递归的条件（base case） Template def recursion ( level , param1 , param2 , ... ) : # recursion terminator if level > MAX_LEVEL : print_result return # process logic in current level process_data ( level , data ... ) # drill down self . recursion ( level + 1 , p1 , p2 , ... ) # reverse the current level status if needed reverse_state ( level ) Example: Divide and Conquer 递归的高阶算法应用：分治 Template def divide_conquer ( problem , param1 , param2 , ... ) : # recursion terminator if problem is None : print_result return # prepare data data = prepare_data ( problem ) subproblem = split_problem ( problem , data ) # conquer subproblem subresult1 = self . divide_conquer ( problem [ 0 ], p1 , p2 , ... ) subresult2 = self . divide_conquer ( problem [ 1 ], p1 , p2 , ... ) subresult3 = self . divide_conquer ( problem [ 2 ], p1 , p2 , ... ) ... # process and generate the final result result = process_result ( subresult1 , subresult2 , subresult3 , ... ) Example: 23. Merge k Sorted Lists Dynamic Programming 所有的动态规划都是由暴力递归优化而来 动态规划是一种分阶段求解决策问题的数学思想。 三个重要的概念： 状态 （最优子结构）、 递推方程 、 边界 。 动态规划利用 自底向上的递推 方式，实现时间和空间上的最优化。 递归展开过程中存在重复状态，即重叠子问题 重复状态无后效性：与到达这个状态的路径无关，即只要这个状态参数确定，则返回值确定。 暴力递归到动态规划的解法： 1. 找出需要求解的 状态位置 2. 回到Base case中设置不被依赖的 边界状态 3. 分析 普遍状态 如何依赖 Example: One-dimensional DP 70. Climbing Stairs Two-dimensional DP 64. Minimum Path Sum 120. Triangle 更新中…","tags":"Algorithms","url":"articles/Data-Structure-and-Algorithm-in-Python.html","loc":"articles/Data-Structure-and-Algorithm-in-Python.html"},{"title":"Hexo blog sync and migration","text":"Overview Deploy env files to github Env building in a new computer Sync operation on two computers Overview HEXO ├──.deploy_git/ ├──node_modules/ ├──public/ ├──scaffolds/ ├──source/ ├──themes/ ├──_config.yml ├──.gitignore ├──db.json ├──debug.log └──package.json .deploy_git which same as public/ : hexo g -d —-> username.github.io static files : deploy github env files : local Deploy env files to github The env files is deployed to github and does not affect the hosting of static files . 1. Create a new hexo branch on gituhb web, and set it as the default branch to store the env files . 2. git clone hexo-branch 3. cd username.github.io & remove all files except ‘.git/' 4. git add -A 5. git commit -m \"comment\" 6. git push origin hexo (hexo branch have been cleared.) 7. copy .git/ to Hexo/ ( Now, the hexo project has become a local repository associated with the remote hexo branch. ) git add . & git commit -m \"some description\" & git push origin hexo : push env files to hexo branch. hexo g -d : deploy web & push static files to master branch. Env building in a new computer install hexo: npm install -g hexo-cli git clone git@github.com:username/username.github.io.git npm install hexo g & hexo s http://localhost:4000/ Sync operation on two computers git pull origin hexo When have written blog, first commit env files, then deploy blog. - git add . - git commit -m \"comment\" - git push origin hexo - hexo g -d","tags":"Tools","url":"articles/Hexo-blog-sync-and-migration.html","loc":"articles/Hexo-blog-sync-and-migration.html"},{"title":"Leetcode Index","text":"1. Two Sum 2. Add Two Numbers 4. Median of Two Sorted Arrays 5. Longest Palindromic Substring 8. String to Integer (atoi) 13. Roman to Integer 15. 3Sum 20. Valid Parentheses 21. Merge Two Sorted Lists 23. Merge k Sorted Lists 24. Swap Nodes in Pairs 25. Reverse Nodes in k-Group 26. Remove Duplicates from Sorted Array 28. Implement strStr() 33. Search in Rotated Sorted Array 46. Permutations 47. Permutations II 48. Rotate Image 53. Maximum Subarray 54. Spiral Matrix 55. Jump Game 56. Merge Intervals 71. Simplify Path 73. Set Matrix Zeroes 75. Sort Colors 79. Word Search 88. Merge Sorted Array 91. Decode Ways 94. Binary Tree Inorder Traversal 98. Validate Binary Search Tree 101. Symmetric Tree 102. Binary Tree Level Order Traversal 103. Binary Tree Zigzag Level Order Traversal 106. Construct Binary Tree from Inorder and Postorder Traversal 112. Path Sum 114. Flatten Binary Tree to Linked List 116. Populating Next Right Pointers in Each Node 117. Populating Next Right Pointers in Each Node II 121. Best Time to Buy and Sell Stock 124. Binary Tree Maximum Path Sum 125. Valid Palindrome 138. Copy List with Random Pointer 141. Linked List Cycle 146. LRU Cache 151. Reverse Words in a String 153. Find Minimum in Rotated Sorted Array 160. Intersection of Two Linked Lists 162. Find Peak Element 165. Compare Version Numbers 168. Excel Sheet Column Title 171. Excel Sheet Column Number 173. Binary Search Tree Iterator 174. Dungeon Game 186. Reverse Words in a String II 189. Rotate Array 191. Number of 1 Bits 200. Number of Islands 204. Count Primes 206. Reverse Linked List 208. Implement Trie (Prefix Tree) 212. Word Search II 213. House Robber II 215. Kth Largest Element in an Array 218. The Skyline Problem 232. Implement Queue using Stacks 235. Lowest Common Ancestor of a Binary Search Tree 236. Lowest Common Ancestor of a Binary Tree 237. Delete Node in a Linked List 238. Product of Array Except Self 258. Add Digits 268. Missing Number 270. Closest Binary Search Tree Value 273. Integer to English Words 285. Inorder Successor in BST 297. Serialize and Deserialize Binary Tree 300. Longest Increasing Subsequence 333. Largest BST Subtree 348. Design Tic-Tac-Toe 365. Water and Jug Problem 387. First Unique Character in a String 419. Battleships in a Board 445. Add Two Numbers II 452. Minimum Number of Arrows to Burst Balloons 513. Find Bottom Left Tree Value 567. Permutation in String 591. Tag Validator 631. Design Excel Sum Formula 642. Design Search Autocomplete System 650. 2 Keys Keyboard 651. 4 Keys Keyboard 654. Maximum Binary Tree 672. Bulb Switcher II","tags":"Algorithms","url":"articles/Leetcode-Index.html","loc":"articles/Leetcode-Index.html"},{"title":"Vim","text":"Install vim ~/.vimrc Vim的设计哲学 windows ‘\\r\\n' to linux ‘\\n' 替换 Install vim apt install libncurses5 - dev git clone git @github . com : vim / vim . git cd vim # YouCompleteMe unavailable : requires Vim compiled with Python ( 3.6.0 + ) support . . / configure -- enable - pythoninterp = yes -- enable - cscope -- enable - fontset -- with - python3 - config - dir =/ opt / conda / envs / blog / lib / python3 .8 / config - 3.8 - x86_64 - linux - gnu -- enable - python3interp = yes -- with - python3 - command = python3 .8 make make install ~/.vimrc https://github.com/jerrylsu/vimrc Install Pluge curl -fLo ~/.vim/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim vim :PlugeInstall Uninstall Pluge \" 注释 .vimrc中的插件，:source ~/.vimrc :PlugeClean Vim的设计哲学 operation 操作: d, delete c, change y motion 动作: w, word b, back f, find查找光标后的字符(:冒号，字母a)并跳转， f: , fa 常用组合： cw, change word 删除一个词（删除光标以后的词），并进入写模式。 ciw, change in word 删除一个词（光标在词中），并进入写模式。 i 是插入，如果做了映射 noremap f i ，则 cfw ci\", change in \" 删除某种符号（如双引号\"）之间的所有内容，并进入写模式。 dw, diw, di\", yw, yiw, yi\" 同理 df:, cf:, yf: windows ‘\\r\\n' to linux ‘\\n' vim filename && set ff=unix 替换 :s/from/to/g https://www.cnblogs.com/wind-wang/p/5768000.html","tags":"Tools","url":"articles/Vim.html","loc":"articles/Vim.html"},{"title":"Building Python Package with Pybuilder","text":"Building Python Package Building Python Package git + venv + pybuilder [1] virtualenv user guide [2] 廖雪峰-virtualenv [3] pybuilder tutorial [4] Managing Your Python Project with Git and PyBuilder [5] examples How to add non-python file into a distribution? [1] Install additional files [2] doc 使用 project.install_file(target, source) 安装非python文件。 target path ：可以是绝对路径，也可以是相对于安装前缀( /usr/ on most linux systems)。 source path ：必须是 distribution directory 目录。因为默认情况下non-python文件未复制到分发目录，因此必须使用 copy_resources 插件来包含它们。 use_plugin ( \"copy_resources\" ) @init def initialize ( project ) : project . get_property ( \"copy_resources_glob\" ). append ( \"src/main/resources/my-config.yaml\" ) project . set_property ( \"copy_resources_target\" , \"$dir_dist\" ) project . install_file ( \"/etc/defaults\" , \"src/main/resources/my-config.yaml\" )","tags":"Programming","url":"articles/Building-Python-Package-with-Pybuilder.html","loc":"articles/Building-Python-Package-with-Pybuilder.html"},{"title":"Git","text":"1. 仓库管理 Create Repository Add File to Local Repository Associated Remote Repo(GitHub) Push GitHub Repository 2. 分支管理 创建分支 切换分支 创建并切换分支 查看所有本地分支 合并分支 删除本地分支 删除远程分支 远程新建分支 Remove files/dir from git repo 拉取远程分支 3. 在提交树上的移动 分离HEAD指针 相对引用 强制修改分支位置 5. 撤销变更 6. 整理提交分支 4. 恢复本地commit提交 可视化Learn Git Branching 1. 仓库管理 Create Repository $ mkdir myrepo $ cd myrepo $ git init Initialized empty Git repository in /Users/jerrylsu/myrepo/.git/ Add File to Local Repository $ git status $ git diff filename Check working directory status, and view changes made to the filemname. $ git add filename $ git commit -m \"write commend\" $ git commit --amend 修改最后一次提交，避免新开一个commit提交到history！ Associated Remote Repo(GitHub) $ git remote add origin [SSH address] Note : origin is an alias for the remote repository address. Push GitHub Repository $ git push origin branch_name Now, we can push the latest changes to our local branch to the remote GitHub repository, which is associated above! 2. 分支管理 创建分支 git branch branch_name 切换分支 git checkout branch_name 创建并切换分支 git checkout -b branch_name 查看所有本地分支 $ git branch 合并分支 将其他分支branch_name合并到当前分支 git merge branch_name 将当前分支合并到其他分支branch_name,线性的串接到分支branch_name后 git rebase branch_name 删除本地分支 git branch -d branch_name 删除远程分支 git branch -r -d origin/branch_name git push origin:branch_name ??? 远程新建分支 将本地分支推送到远程不存在的分支，实现远程分支新建 git push origin local_branch:remote_branch Remove files/dir from git repo $ git rm --cached file.txt $ git rm -r --cached dir $ git commit -m \"remove file.txt\" And to push changes to remote repo $ git push origin branch_name 拉取远程分支 先拉远程master分支： git clone ... 拉取远程分支： git fetch origin remote_branch_name:local_branch_name 查看拉取到本地的分支： git checkout local_branch_name 3. 在提交树上的移动 HEAD 总是指向当前分支上最近一次提交记录。大多数修改提交树的 Git 命令都是从改变 HEAD 的指向开始的。 HEAD 通常情况下是指向分支名的。在你提交时，改变了分支的状态，这一变化通过 HEAD 变得可见。 分离HEAD指针 可以用切换分支的命令来切换到特定一次的提交，提交名是哈希值commit_hash。 git checkout commit_hash(fed2) HEAD分离并指向这个特定的提交 git log 获取提交的哈希值，例如fed2da64c0efc5293610bdd892f82a58e8cbc5d8，如果可以唯一标识，取前缀即可fed2。 相对引用 相对引用更方便,使用相对引用可以从一个易于记忆的地方（比如 bugFix 分支或 HEAD ）开始计算 使用 &#94; 后退1个提交记录，如 git checkout main&#94; 或 git checkout main&#94;&#94; ，HEAD指向main分支的父节点和父父节点 使用 ~<num> 后退num步提交记录，如 ~3 。 ~ 不接数字等价于 &#94; git checkout fed2 # 分离HEAD指向fed2这次提交 git checkout HEAD&#94; # HEAD上移 git checkout HEAD&#94; # HEAD继续上移 强制修改分支位置 git branch -f main HEAD&#94;3/commit_hash 5. 撤销变更 git reset commit_hash 6. 整理提交分支 将commit_hash1与commit_hash2两次提交串接到当前分支下，当前分支指针移动到串接头部commit_hash2。类似于 git rebase git cherry-pick commit_hash1 commit_hash2 4. 恢复本地commit提交 获取commit id： git log 恢复本地commit提交： git reset --hard commit_id git常用","tags":"Tools","url":"articles/Git.html","loc":"articles/Git.html"},{"title":"Keeping a fork synced with the origin repo","text":"In the future, if you will modify code, you should pull the lastest upstream repo first ! (starting from step 5 above) 1. for k upstream_repo_url 2. git clone or igin_repo_url 3. git remote add upstream upstream_url 4. git remote -v 5. git pull upstream dev - git fetch upstream - git checkout dev - git merge upstream / dev 6. git checkout - b dev_01 ( modify code ... ) 7. git push or igin dev_01 8. pull request master: release dev: develop self-built branch: (as dev_01) In the future, if you will modify code, you should pull the lastest upstream repo first ! (starting from step 5 above)","tags":"Tools","url":"articles/Keeping-a-fork-synced-with-the-origin-repo.html","loc":"articles/Keeping-a-fork-synced-with-the-origin-repo.html"}]};