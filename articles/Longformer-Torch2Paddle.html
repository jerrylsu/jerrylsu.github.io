
<!DOCTYPE html>
<html lang="en">

<!-- Head -->
<head>

        <!-- Required metadata tags -->
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="HandheldFriendly" content="True" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

        <!-- Default metadata -->
    <meta name="author" content="Jerry Su" />
    <meta name="description" content="Reason is the light and the light of life." />
    <meta name="keywords" content="Paddle, Pytorch">
<meta property="og:site_name" content="JERRYLSU" />
<meta property="og:title" content="Longformer Torch2Paddle" />
<meta property="og:description" content="Reason is the light and the light of life." />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="../articles/Longformer-Torch2Paddle.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-10 11:17:17+08:00" />
<meta property="article:modified_time" content="" />
<meta property="article:author" content="../author/jerry-su.html">
<meta property="article:section" content="NLP" />
	<meta property="article:tag" content="Paddle" />
	<meta property="article:tag" content="Pytorch" />
	<meta property="og:image" content="../jerry.jpg">

        <!-- Site Claim -->


        <!-- Title -->
        <title>
    Longformer Torch2Paddle &ndash; JERRYLSU
        </title>
        
        <!-- Icon -->
        <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon">
        <link rel="icon" href="../favicon.ico" type="image/x-icon">

        <!-- Search engine -->
            <meta name="robots" content="" />

        <!-- Feeds -->








        <!-- Styles -->
        <!--
        <link rel="stylesheet" href="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/css/bootstrap.min.css">
        -->
        <link rel="stylesheet" href="../theme/bootstrap/bootstrap.min.css">
        <!--
        <link rel="stylesheet" href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css">
        -->
            <link rel="stylesheet" href="../theme/extra/bootstrap-toc.min.css">
        <link rel="stylesheet" href="../theme/pygment/friendly.min.css">
        <!--
        <link rel="stylesheet" href="../theme/extra/admonition.min.css">
        -->
        <link rel="stylesheet" href="../theme/style.css">

        <!-- Google Analytics -->
<script>
(function(i, s, o, g, r, a, m) { i['GoogleAnalyticsObject'] = r;
    i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments) }, i[r].l = 1 * new Date();
    a = s.createElement(o), m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    m.parentNode.insertBefore(a, m) })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
ga('create', 'UA-42618265-2', 'auto');
ga('send', 'pageview');
</script>
        <!-- Google Global Site Tag -->

        <!-- Google Tag Manager -->

        <!-- Google Adsense -->

        <!-- Heap Analytic -->

        <!-- Piwik Tracking -->

        <!-- Matomo Tracking -->

</head>

<!-- Body -->
<body class="d-flex flex-column" data-spy="scroll" data-target="#toc" data-offset="0" style="position: relative;">
    <!-- Top anchor -->
    <a href="#" id="backToTop" style="display: none; z-index: 1;" title="Back to top"><span></span></a>

    <!-- Google tag manager -->

    <!-- Navigation -->
    <nav class="flex-shrink-0 navbar navbar-expand-md navbar-expand-lg navbar-dark bg-dark text-light shadow-sm">
        <!-- Logo -->
        <a class="navbar-brand" href="..">JERRYLSU.NET</a>

        <!-- Collapse button -->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMenu" aria-controls="navbarMenu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon small"></span>
        </button>

        <!-- Collapsible content -->
        <div class="collapse navbar-collapse" id="navbarMenu">

            <!-- i18n subsites -->

            <!-- Page links -->
            <ul class="navbar-nav mr-auto text-center">
                <li class="nav-item ">                           
                    <a class="nav-link" href="..">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M21 13v10h-6v-6h-6v6h-6v-10h-3l12-12 12 12h-3zm-1-5.907v-5.093h-3v2.093l3 3z" fill="currentColor"></path>
                        </svg>
                        Home <span class="sr-only">(current)</span>
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="../categories.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M16 6h-8v-6h8v6zm-8 12h-8v6h8v-6zm16 0h-8v6h8v-6zm-11-7v-3h-2v3h-8v5h2v-3h14v3h2v-5h-8z" fill="currentColor"></path>
                        </svg>
                        Categories
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="../tags.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M10.605 0h-10.605v10.609l13.391 13.391 10.609-10.604-13.395-13.396zm-4.191 6.414c-.781.781-2.046.781-2.829.001-.781-.783-.781-2.048 0-2.829.782-.782 2.048-.781 2.829-.001.782.782.781 2.047 0 2.829z" fill="currentColor"></path>
                        </svg>
                        Tags
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="../archives.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M1.8 9l-.8-4h22l-.8 4h-2.029l.39-2h-17.122l.414 2h-2.053zm18.575-6l.604-2h-17.979l.688 2h16.687zm3.625 8l-2 13h-20l-2-13h24zm-8 4c0-.552-.447-1-1-1h-6c-.553 0-1 .448-1 1s.447 1 1 1h6c.553 0 1-.448 1-1z" fill="currentColor"></path>
                        </svg>
                        Archives
                    </a>
                </li>
                <li class="nav-item ">
                    <a class="nav-link" href="../pages/about.html">
                        <svg class="nav-icon" xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24">
                            <path d="M20.822 18.096c-3.439-.794-6.64-1.49-5.09-4.418 4.72-8.912 1.251-13.678-3.732-13.678-5.082 0-8.464 4.949-3.732 13.678 1.597 2.945-1.725 3.641-5.09 4.418-3.073.71-3.188 2.236-3.178 4.904l.004 1h23.99l.004-.969c.012-2.688-.092-4.222-3.176-4.935z" fill="currentColor"></path>
                        </svg>
                        About
                    </a>
                </li>
            </ul>

            <!-- Search form -->
            <form class="form-inline text-center" action="../search.html">
                <input class="form-control w-100 bg-dark text-light text-center border-0 p-2" type="text" name="q" pattern=".{3,}" title="At least 3 characters" required="" placeholder="Type here to search" aria-label="Search">
            </form>

            <!-- Social links -->
            <ul class="navbar-nav text-center">
                <li class="nav-item">
                    <a class="nav-link" href="">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Facebook</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm3 8h-1.35c-.538 0-.65.221-.65.778v1.222h2l-.209 2h-1.791v7h-3v-7h-2v-2h2v-2.308c0-1.769.931-2.692 3.029-2.692h1.971v3z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://github.com/jerrylsu">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Github</title>
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/jerrylsu">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Linkedin</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm-2 16h-2v-6h2v6zm-1-6.891c-.607 0-1.1-.496-1.1-1.109 0-.612.492-1.109 1.1-1.109s1.1.497 1.1 1.109c0 .613-.493 1.109-1.1 1.109zm8 6.891h-1.998v-2.861c0-1.881-2.002-1.722-2.002 0v2.861h-2v-6h2v1.093c.872-1.616 4-1.736 4 1.548v3.359z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="https://twitter.com/Jerrylsu666">
                        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24">
                            <title>Twitter</title>
                            <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z" fill="currentColor"></path>
                        </svg>
                    </a>
                </li>
            </ul>
        </div>
    </nav>

    <!-- Full page -->
    <div class="flex-shrink-0 flex-grow-1">

        <!-- Header -->
        <header class="bg-dark text-light shadow-sm pt-3 pb-2">
	<div class="container">
		<h3 id="Longformer-Torch2Paddle">Longformer&nbsp;Torch2Paddle</h3>
		<p style="font-size:larger;">Reason is the light and the light of&nbsp;life.</p>
        <div class="row mx-auto mt-3">
            <div class="col-xs-12 col-sm-12 col-md-6 text-left" style="padding: 0">
                <a href="../author/jerry-su.html" class="card-link">Jerry Su</a>
                <span class="card-link text-success">
                    <span class="post-date" title="Post date">May 10, 2022</span>
                    <span class="text-info modified-date" title="Updated date">
                            May 10, 2022
                    </span>
                </span>
                    <span class="card-link text-secondary" title="~481 words">2 mins</span>
            </div>
            <div class="col-xs-12 col-sm-12 col-md-6 text-right" style="padding: 0">
                <a class="badge badge-success" href="../category/nlp.html">nlp</a>
                    <a class="badge badge-info" href="../tag/paddle.html">paddle</a>
                    <a class="badge badge-info" href="../tag/pytorch.html">pytorch</a>
            </div>
        </div>
	</div>
        </header>

        <!-- Main -->
        <main class="py-3">
                <div class="container">
                    <!-- Sharing -->

                    <!-- Content -->
    <!-- 2 columns layout -->
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-10">

                <!-- Sharing -->
                    <div class="text-right mb-2 small" style="height: 26px">
                        <div class="addthis_inline_share_toolbox"></div>
                    </div>

                <!-- Article -->
                <p><a href="https://huggingface.co/Jerry666/paddlepaddle-longformer-base-4096/tree/main">PaddlePaddle-Longformer-model-base-4096</a></p>
<table>
<thead>
<tr>
<th align="left">PyTorch</th>
<th align="left">Shape</th>
<th align="left">Paddle</th>
<th align="left">Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">embeddings.word_embeddings.weight</td>
<td align="left">[50265, 768]</td>
<td align="left">embeddings.word_embeddings.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">embeddings.position_embeddings.weight</td>
<td align="left">[4098, 768]</td>
<td align="left">embeddings.position_embeddings.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">embeddings.token_type_embeddings.weight</td>
<td align="left">[1, 768]</td>
<td align="left">embeddings.token_type_embeddings.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">embeddings.LayerNorm.weight</td>
<td align="left">[768]</td>
<td align="left">embeddings.layer_norm.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">embeddings.LayerNorm.bias</td>
<td align="left">[768]</td>
<td align="left">embeddings.layer_norm.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.query.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.query.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.query.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.query.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.key.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.key.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.key.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.key.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.value.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.value.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.value.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.value.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.query_global.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.query_global.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.query_global.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.query_global.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.key_global.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.key_global.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.key_global.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.key_global.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.value_global.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.value_global.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.self.value_global.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.value_global.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.output.dense.weight</td>
<td align="left">[768, 768]</td>
<td align="left">encoder.layers.0.self_attn.out.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.output.dense.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.self_attn.out.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.output.LayerNorm.weight</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.norm1.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.attention.output.LayerNorm.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.norm1.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.intermediate.dense.weight</td>
<td align="left">[3072, 768]</td>
<td align="left">encoder.layers.0.linear1.weight</td>
<td align="left">T [768, 3072]</td>
</tr>
<tr>
<td align="left">encoder.layer.0.intermediate.dense.bias</td>
<td align="left">[3072]</td>
<td align="left">encoder.layers.0.linear1.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.output.dense.weight</td>
<td align="left">[768, 3072]</td>
<td align="left">encoder.layers.0.linear2.weight</td>
<td align="left">T [3072, 768]</td>
</tr>
<tr>
<td align="left">encoder.layer.0.output.dense.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.linear2.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.output.LayerNorm.weight</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.norm2.weight</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">encoder.layer.0.output.LayerNorm.bias</td>
<td align="left">[768]</td>
<td align="left">encoder.layers.0.norm2.bias</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">pooler.dense.weight</td>
<td align="left">[768, 768]</td>
<td align="left">pooler.dense.weight</td>
<td align="left">T</td>
</tr>
<tr>
<td align="left">pooler.dense.bias</td>
<td align="left">[768]</td>
<td align="left">pooler.dense.bias</td>
<td align="left"></td>
</tr>
</tbody>
</table>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">LongformerModel</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">LongformerModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;allenai/longformer-base-4096&quot;</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="nv">Some</span><span class="w"> </span><span class="nv">weights</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">checkpoint</span><span class="w"> </span><span class="nv">at</span><span class="w"> </span><span class="nv">allenai</span><span class="o">/</span><span class="nv">longformer</span><span class="o">-</span><span class="nv">base</span><span class="o">-</span><span class="mi">4096</span><span class="w"> </span><span class="nv">were</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">used</span><span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="nv">initializing</span><span class="w"> </span><span class="nv">LongformerModel</span>:<span class="w"> </span>[<span class="s1">&#39;lm_head.dense.weight&#39;</span>,<span class="w"> </span><span class="s1">&#39;lm_head.layer_norm.bias&#39;</span>,<span class="w"> </span><span class="s1">&#39;lm_head.dense.bias&#39;</span>,<span class="w"> </span><span class="s1">&#39;lm_head.bias&#39;</span>,<span class="w"> </span><span class="s1">&#39;lm_head.decoder.weight&#39;</span>,<span class="w"> </span><span class="s1">&#39;lm_head.layer_norm.weight&#39;</span>]<span class="w"></span>
<span class="o">-</span><span class="w"> </span><span class="nv">This</span><span class="w"> </span><span class="nv">IS</span><span class="w"> </span><span class="nv">expected</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">initializing</span><span class="w"> </span><span class="nv">LongformerModel</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">checkpoint</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">trained</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">another</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">another</span><span class="w"> </span><span class="nv">architecture</span><span class="w"> </span><span class="ss">(</span><span class="nv">e</span>.<span class="nv">g</span>.<span class="w"> </span><span class="nv">initializing</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">BertForSequenceClassification</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">BertForPreTraining</span><span class="w"> </span><span class="nv">model</span><span class="ss">)</span>.<span class="w"></span>
<span class="o">-</span><span class="w"> </span><span class="nv">This</span><span class="w"> </span><span class="nv">IS</span><span class="w"> </span><span class="nv">NOT</span><span class="w"> </span><span class="nv">expected</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">initializing</span><span class="w"> </span><span class="nv">LongformerModel</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">checkpoint</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">expect</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">exactly</span><span class="w"> </span><span class="nv">identical</span><span class="w"> </span><span class="ss">(</span><span class="nv">initializing</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">BertForSequenceClassification</span><span class="w"> </span><span class="nv">model</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">BertForSequenceClassification</span><span class="w"> </span><span class="nv">model</span><span class="ss">)</span>.<span class="w"></span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">state_dict</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>odict_keys([&#39;embeddings.position_ids&#39;, &#39;embeddings.word_embeddings.weight&#39;, &#39;embeddings.position_embeddings.weight&#39;, &#39;embeddings.token_type_embeddings.weight&#39;, &#39;embeddings.LayerNorm.weight&#39;, &#39;embeddings.LayerNorm.bias&#39;, &#39;encoder.layer.0.attention.self.query.weight&#39;, &#39;encoder.layer.0.attention.self.query.bias&#39;, &#39;encoder.layer.0.attention.self.key.weight&#39;, &#39;encoder.layer.0.attention.self.key.bias&#39;, &#39;encoder.layer.0.attention.self.value.weight&#39;, &#39;encoder.layer.0.attention.self.value.bias&#39;, &#39;encoder.layer.0.attention.self.query_global.weight&#39;, &#39;encoder.layer.0.attention.self.query_global.bias&#39;, &#39;encoder.layer.0.attention.self.key_global.weight&#39;, &#39;encoder.layer.0.attention.self.key_global.bias&#39;, &#39;encoder.layer.0.attention.self.value_global.weight&#39;, &#39;encoder.layer.0.attention.self.value_global.bias&#39;, &#39;encoder.layer.0.attention.output.dense.weight&#39;, &#39;encoder.layer.0.attention.output.dense.bias&#39;, &#39;encoder.layer.0.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.0.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.0.intermediate.dense.weight&#39;, &#39;encoder.layer.0.intermediate.dense.bias&#39;, &#39;encoder.layer.0.output.dense.weight&#39;, &#39;encoder.layer.0.output.dense.bias&#39;, &#39;encoder.layer.0.output.LayerNorm.weight&#39;, &#39;encoder.layer.0.output.LayerNorm.bias&#39;, &#39;encoder.layer.1.attention.self.query.weight&#39;, &#39;encoder.layer.1.attention.self.query.bias&#39;, &#39;encoder.layer.1.attention.self.key.weight&#39;, &#39;encoder.layer.1.attention.self.key.bias&#39;, &#39;encoder.layer.1.attention.self.value.weight&#39;, &#39;encoder.layer.1.attention.self.value.bias&#39;, &#39;encoder.layer.1.attention.self.query_global.weight&#39;, &#39;encoder.layer.1.attention.self.query_global.bias&#39;, &#39;encoder.layer.1.attention.self.key_global.weight&#39;, &#39;encoder.layer.1.attention.self.key_global.bias&#39;, &#39;encoder.layer.1.attention.self.value_global.weight&#39;, &#39;encoder.layer.1.attention.self.value_global.bias&#39;, &#39;encoder.layer.1.attention.output.dense.weight&#39;, &#39;encoder.layer.1.attention.output.dense.bias&#39;, &#39;encoder.layer.1.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.1.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.1.intermediate.dense.weight&#39;, &#39;encoder.layer.1.intermediate.dense.bias&#39;, &#39;encoder.layer.1.output.dense.weight&#39;, &#39;encoder.layer.1.output.dense.bias&#39;, &#39;encoder.layer.1.output.LayerNorm.weight&#39;, &#39;encoder.layer.1.output.LayerNorm.bias&#39;, &#39;encoder.layer.2.attention.self.query.weight&#39;, &#39;encoder.layer.2.attention.self.query.bias&#39;, &#39;encoder.layer.2.attention.self.key.weight&#39;, &#39;encoder.layer.2.attention.self.key.bias&#39;, &#39;encoder.layer.2.attention.self.value.weight&#39;, &#39;encoder.layer.2.attention.self.value.bias&#39;, &#39;encoder.layer.2.attention.self.query_global.weight&#39;, &#39;encoder.layer.2.attention.self.query_global.bias&#39;, &#39;encoder.layer.2.attention.self.key_global.weight&#39;, &#39;encoder.layer.2.attention.self.key_global.bias&#39;, &#39;encoder.layer.2.attention.self.value_global.weight&#39;, &#39;encoder.layer.2.attention.self.value_global.bias&#39;, &#39;encoder.layer.2.attention.output.dense.weight&#39;, &#39;encoder.layer.2.attention.output.dense.bias&#39;, &#39;encoder.layer.2.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.2.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.2.intermediate.dense.weight&#39;, &#39;encoder.layer.2.intermediate.dense.bias&#39;, &#39;encoder.layer.2.output.dense.weight&#39;, &#39;encoder.layer.2.output.dense.bias&#39;, &#39;encoder.layer.2.output.LayerNorm.weight&#39;, &#39;encoder.layer.2.output.LayerNorm.bias&#39;, &#39;encoder.layer.3.attention.self.query.weight&#39;, &#39;encoder.layer.3.attention.self.query.bias&#39;, &#39;encoder.layer.3.attention.self.key.weight&#39;, &#39;encoder.layer.3.attention.self.key.bias&#39;, &#39;encoder.layer.3.attention.self.value.weight&#39;, &#39;encoder.layer.3.attention.self.value.bias&#39;, &#39;encoder.layer.3.attention.self.query_global.weight&#39;, &#39;encoder.layer.3.attention.self.query_global.bias&#39;, &#39;encoder.layer.3.attention.self.key_global.weight&#39;, &#39;encoder.layer.3.attention.self.key_global.bias&#39;, &#39;encoder.layer.3.attention.self.value_global.weight&#39;, &#39;encoder.layer.3.attention.self.value_global.bias&#39;, &#39;encoder.layer.3.attention.output.dense.weight&#39;, &#39;encoder.layer.3.attention.output.dense.bias&#39;, &#39;encoder.layer.3.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.3.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.3.intermediate.dense.weight&#39;, &#39;encoder.layer.3.intermediate.dense.bias&#39;, &#39;encoder.layer.3.output.dense.weight&#39;, &#39;encoder.layer.3.output.dense.bias&#39;, &#39;encoder.layer.3.output.LayerNorm.weight&#39;, &#39;encoder.layer.3.output.LayerNorm.bias&#39;, &#39;encoder.layer.4.attention.self.query.weight&#39;, &#39;encoder.layer.4.attention.self.query.bias&#39;, &#39;encoder.layer.4.attention.self.key.weight&#39;, &#39;encoder.layer.4.attention.self.key.bias&#39;, &#39;encoder.layer.4.attention.self.value.weight&#39;, &#39;encoder.layer.4.attention.self.value.bias&#39;, &#39;encoder.layer.4.attention.self.query_global.weight&#39;, &#39;encoder.layer.4.attention.self.query_global.bias&#39;, &#39;encoder.layer.4.attention.self.key_global.weight&#39;, &#39;encoder.layer.4.attention.self.key_global.bias&#39;, &#39;encoder.layer.4.attention.self.value_global.weight&#39;, &#39;encoder.layer.4.attention.self.value_global.bias&#39;, &#39;encoder.layer.4.attention.output.dense.weight&#39;, &#39;encoder.layer.4.attention.output.dense.bias&#39;, &#39;encoder.layer.4.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.4.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.4.intermediate.dense.weight&#39;, &#39;encoder.layer.4.intermediate.dense.bias&#39;, &#39;encoder.layer.4.output.dense.weight&#39;, &#39;encoder.layer.4.output.dense.bias&#39;, &#39;encoder.layer.4.output.LayerNorm.weight&#39;, &#39;encoder.layer.4.output.LayerNorm.bias&#39;, &#39;encoder.layer.5.attention.self.query.weight&#39;, &#39;encoder.layer.5.attention.self.query.bias&#39;, &#39;encoder.layer.5.attention.self.key.weight&#39;, &#39;encoder.layer.5.attention.self.key.bias&#39;, &#39;encoder.layer.5.attention.self.value.weight&#39;, &#39;encoder.layer.5.attention.self.value.bias&#39;, &#39;encoder.layer.5.attention.self.query_global.weight&#39;, &#39;encoder.layer.5.attention.self.query_global.bias&#39;, &#39;encoder.layer.5.attention.self.key_global.weight&#39;, &#39;encoder.layer.5.attention.self.key_global.bias&#39;, &#39;encoder.layer.5.attention.self.value_global.weight&#39;, &#39;encoder.layer.5.attention.self.value_global.bias&#39;, &#39;encoder.layer.5.attention.output.dense.weight&#39;, &#39;encoder.layer.5.attention.output.dense.bias&#39;, &#39;encoder.layer.5.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.5.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.5.intermediate.dense.weight&#39;, &#39;encoder.layer.5.intermediate.dense.bias&#39;, &#39;encoder.layer.5.output.dense.weight&#39;, &#39;encoder.layer.5.output.dense.bias&#39;, &#39;encoder.layer.5.output.LayerNorm.weight&#39;, &#39;encoder.layer.5.output.LayerNorm.bias&#39;, &#39;encoder.layer.6.attention.self.query.weight&#39;, &#39;encoder.layer.6.attention.self.query.bias&#39;, &#39;encoder.layer.6.attention.self.key.weight&#39;, &#39;encoder.layer.6.attention.self.key.bias&#39;, &#39;encoder.layer.6.attention.self.value.weight&#39;, &#39;encoder.layer.6.attention.self.value.bias&#39;, &#39;encoder.layer.6.attention.self.query_global.weight&#39;, &#39;encoder.layer.6.attention.self.query_global.bias&#39;, &#39;encoder.layer.6.attention.self.key_global.weight&#39;, &#39;encoder.layer.6.attention.self.key_global.bias&#39;, &#39;encoder.layer.6.attention.self.value_global.weight&#39;, &#39;encoder.layer.6.attention.self.value_global.bias&#39;, &#39;encoder.layer.6.attention.output.dense.weight&#39;, &#39;encoder.layer.6.attention.output.dense.bias&#39;, &#39;encoder.layer.6.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.6.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.6.intermediate.dense.weight&#39;, &#39;encoder.layer.6.intermediate.dense.bias&#39;, &#39;encoder.layer.6.output.dense.weight&#39;, &#39;encoder.layer.6.output.dense.bias&#39;, &#39;encoder.layer.6.output.LayerNorm.weight&#39;, &#39;encoder.layer.6.output.LayerNorm.bias&#39;, &#39;encoder.layer.7.attention.self.query.weight&#39;, &#39;encoder.layer.7.attention.self.query.bias&#39;, &#39;encoder.layer.7.attention.self.key.weight&#39;, &#39;encoder.layer.7.attention.self.key.bias&#39;, &#39;encoder.layer.7.attention.self.value.weight&#39;, &#39;encoder.layer.7.attention.self.value.bias&#39;, &#39;encoder.layer.7.attention.self.query_global.weight&#39;, &#39;encoder.layer.7.attention.self.query_global.bias&#39;, &#39;encoder.layer.7.attention.self.key_global.weight&#39;, &#39;encoder.layer.7.attention.self.key_global.bias&#39;, &#39;encoder.layer.7.attention.self.value_global.weight&#39;, &#39;encoder.layer.7.attention.self.value_global.bias&#39;, &#39;encoder.layer.7.attention.output.dense.weight&#39;, &#39;encoder.layer.7.attention.output.dense.bias&#39;, &#39;encoder.layer.7.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.7.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.7.intermediate.dense.weight&#39;, &#39;encoder.layer.7.intermediate.dense.bias&#39;, &#39;encoder.layer.7.output.dense.weight&#39;, &#39;encoder.layer.7.output.dense.bias&#39;, &#39;encoder.layer.7.output.LayerNorm.weight&#39;, &#39;encoder.layer.7.output.LayerNorm.bias&#39;, &#39;encoder.layer.8.attention.self.query.weight&#39;, &#39;encoder.layer.8.attention.self.query.bias&#39;, &#39;encoder.layer.8.attention.self.key.weight&#39;, &#39;encoder.layer.8.attention.self.key.bias&#39;, &#39;encoder.layer.8.attention.self.value.weight&#39;, &#39;encoder.layer.8.attention.self.value.bias&#39;, &#39;encoder.layer.8.attention.self.query_global.weight&#39;, &#39;encoder.layer.8.attention.self.query_global.bias&#39;, &#39;encoder.layer.8.attention.self.key_global.weight&#39;, &#39;encoder.layer.8.attention.self.key_global.bias&#39;, &#39;encoder.layer.8.attention.self.value_global.weight&#39;, &#39;encoder.layer.8.attention.self.value_global.bias&#39;, &#39;encoder.layer.8.attention.output.dense.weight&#39;, &#39;encoder.layer.8.attention.output.dense.bias&#39;, &#39;encoder.layer.8.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.8.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.8.intermediate.dense.weight&#39;, &#39;encoder.layer.8.intermediate.dense.bias&#39;, &#39;encoder.layer.8.output.dense.weight&#39;, &#39;encoder.layer.8.output.dense.bias&#39;, &#39;encoder.layer.8.output.LayerNorm.weight&#39;, &#39;encoder.layer.8.output.LayerNorm.bias&#39;, &#39;encoder.layer.9.attention.self.query.weight&#39;, &#39;encoder.layer.9.attention.self.query.bias&#39;, &#39;encoder.layer.9.attention.self.key.weight&#39;, &#39;encoder.layer.9.attention.self.key.bias&#39;, &#39;encoder.layer.9.attention.self.value.weight&#39;, &#39;encoder.layer.9.attention.self.value.bias&#39;, &#39;encoder.layer.9.attention.self.query_global.weight&#39;, &#39;encoder.layer.9.attention.self.query_global.bias&#39;, &#39;encoder.layer.9.attention.self.key_global.weight&#39;, &#39;encoder.layer.9.attention.self.key_global.bias&#39;, &#39;encoder.layer.9.attention.self.value_global.weight&#39;, &#39;encoder.layer.9.attention.self.value_global.bias&#39;, &#39;encoder.layer.9.attention.output.dense.weight&#39;, &#39;encoder.layer.9.attention.output.dense.bias&#39;, &#39;encoder.layer.9.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.9.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.9.intermediate.dense.weight&#39;, &#39;encoder.layer.9.intermediate.dense.bias&#39;, &#39;encoder.layer.9.output.dense.weight&#39;, &#39;encoder.layer.9.output.dense.bias&#39;, &#39;encoder.layer.9.output.LayerNorm.weight&#39;, &#39;encoder.layer.9.output.LayerNorm.bias&#39;, &#39;encoder.layer.10.attention.self.query.weight&#39;, &#39;encoder.layer.10.attention.self.query.bias&#39;, &#39;encoder.layer.10.attention.self.key.weight&#39;, &#39;encoder.layer.10.attention.self.key.bias&#39;, &#39;encoder.layer.10.attention.self.value.weight&#39;, &#39;encoder.layer.10.attention.self.value.bias&#39;, &#39;encoder.layer.10.attention.self.query_global.weight&#39;, &#39;encoder.layer.10.attention.self.query_global.bias&#39;, &#39;encoder.layer.10.attention.self.key_global.weight&#39;, &#39;encoder.layer.10.attention.self.key_global.bias&#39;, &#39;encoder.layer.10.attention.self.value_global.weight&#39;, &#39;encoder.layer.10.attention.self.value_global.bias&#39;, &#39;encoder.layer.10.attention.output.dense.weight&#39;, &#39;encoder.layer.10.attention.output.dense.bias&#39;, &#39;encoder.layer.10.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.10.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.10.intermediate.dense.weight&#39;, &#39;encoder.layer.10.intermediate.dense.bias&#39;, &#39;encoder.layer.10.output.dense.weight&#39;, &#39;encoder.layer.10.output.dense.bias&#39;, &#39;encoder.layer.10.output.LayerNorm.weight&#39;, &#39;encoder.layer.10.output.LayerNorm.bias&#39;, &#39;encoder.layer.11.attention.self.query.weight&#39;, &#39;encoder.layer.11.attention.self.query.bias&#39;, &#39;encoder.layer.11.attention.self.key.weight&#39;, &#39;encoder.layer.11.attention.self.key.bias&#39;, &#39;encoder.layer.11.attention.self.value.weight&#39;, &#39;encoder.layer.11.attention.self.value.bias&#39;, &#39;encoder.layer.11.attention.self.query_global.weight&#39;, &#39;encoder.layer.11.attention.self.query_global.bias&#39;, &#39;encoder.layer.11.attention.self.key_global.weight&#39;, &#39;encoder.layer.11.attention.self.key_global.bias&#39;, &#39;encoder.layer.11.attention.self.value_global.weight&#39;, &#39;encoder.layer.11.attention.self.value_global.bias&#39;, &#39;encoder.layer.11.attention.output.dense.weight&#39;, &#39;encoder.layer.11.attention.output.dense.bias&#39;, &#39;encoder.layer.11.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.11.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.11.intermediate.dense.weight&#39;, &#39;encoder.layer.11.intermediate.dense.bias&#39;, &#39;encoder.layer.11.output.dense.weight&#39;, &#39;encoder.layer.11.output.dense.bias&#39;, &#39;encoder.layer.11.output.LayerNorm.weight&#39;, &#39;encoder.layer.11.output.LayerNorm.bias&#39;, &#39;pooler.dense.weight&#39;, &#39;pooler.dense.bias&#39;])
</code></pre></div>

<div class="highlight"><pre><span></span><code>
</code></pre></div>

                <!-- Neighbors -->
                    <br>
                    <b>Read more:</b><br>
<div class="pagination">
    <a class="w-50" href="../articles/Base64-Encode-Decode.html">
&ll; Base64-encode-decode
    </a>
    <a class="w-50 text-right" href="../articles/FastAPI-for-CPU-Bound-Task.html">
        FastAPI for <span class="caps">CPU</span>-Bound&nbsp;Task &gg;    </a>
</div>
                <!-- Google Adsense -->
            </div>

            <!-- Sidebar -->
            <div class="col-md-2 d-none d-md-block small">
                <div class="sticky-top">
                    <!-- ToC -->
                    <nav id="toc" data-toggle="toc" ></nav>

                    <!-- Share post -->

                    <!-- Google Adsense -->
                </div>
            </div>
        </div>

    <!-- Releated posts -->
        <hr>
        <div>
            <h5>Related posts:</h5>
            <ul>
                <li><a href="../articles/Pytorch-View-vs-Reshape.html">Pytorch View vs&nbsp;Reshape</a></li>
                <li><a href="../articles/pytorch.nn.functional.pad.html">pytorch.nn.functional.pad</a></li>
                <li><a href="../articles/Tokenizer-offset-mapping.html">Tokenizer offerset&nbsp;mapping</a></li>
                <li><a href="../articles/paddle implements torch.repeat_interleave/K.repeat_elements using paddle.reshape &paddle.tile.html">paddle implements torch.repeat_interleave/K.repeat_elements using paddle.reshape <span class="amp">&amp;</span>&nbsp;paddle.tile</a></li>
                <li><a href="../articles/Paddle-Adversarial-Train.html">Paddle adversarial&nbsp;train</a></li>
            </ul>
        </div>

    <!-- Comments -->
        <hr>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'jerrylsu-github-io';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>                </div>
        </main>

    </div>

    <!-- Footer -->
    <footer class="flex-shrink-0 bg-dark text-light small py-1">
        <div class="container text-center">
            &copy; 2023 <a href="..">JERRYLSU</a> by <a href="../pages/about.html">JERRY</a>. Powered by <a href="http://getpelican.com">Pelican</a>, <a href="http://python.org">Python</a>, <a href="https://getbootstrap.com">Bootstrap 4</a><br>
            <!-- Do not remove below license sentence -->
            License: <a href="https://spdx.org/licenses/CC-BY-4.0.html">CC-BY-4.0</a>, based on <a href="https://github.com/vuquangtrong/simplify-theme">Simplify Bootstrap Theme</a>
        </div>
    </footer>

    <!-- Scripts -->
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.4.1.min.js"></script>
    -->
    <script type="text/javascript" src="../theme/jquery/jquery-3.4.1.min.js"></script>
    <!--
    <script src="https://ajax.aspnetcdn.com/ajax/bootstrap/4.3.1/bootstrap.min.js"></script>
    -->
    <script type="text/javascript" src="../theme/bootstrap/bootstrap.min.js"></script>
    <!--
    <script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>
    -->
        <script type="text/javascript" src="../theme/extra/bootstrap-toc.min.js"></script>
    <script type="text/javascript" src="../theme/style.js"></script>

    <!-- Sharing -->
        <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5d9ffca0db80069e"></script>

    <!-- JSON LD -->
<script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "name": "Longformer Torch2Paddle",
    "headline": "Longformer Torch2Paddle",
    "datePublished": "2022-05-10 11:17:17+08:00",
    "dateModified": "",
    "author": {
        "@type": "Person",
        "name": "Jerry Su",
        "url": "../author/jerry-su.html"
    },
    "image": "../jerry.jpg",
    "url": "../articles/Longformer-Torch2Paddle.html",
    "description": "Reason is the light and the light of life."
}
</script>
    <!-- Disqus count -->
</body>

</html>