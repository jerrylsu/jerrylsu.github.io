<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../../../theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../../../theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Jerry Su" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Deep Learning, Pytorch, Programming, " />

<meta property="og:title" content="tensor.detach/torch.detach_/torch.data "/>
<meta property="og:url" content="../../../articles/tensor.detach/torch.detach_/torch.data.html" />
<meta property="og:description" content="Reason is the light and the light of life." />
<meta property="og:site_name" content="JERRYLSU" />
<meta property="og:article:author" content="Jerry Su" />
<meta property="og:article:published_time" content="2021-01-18T12:17:17+08:00" />
<meta name="twitter:title" content="tensor.detach/torch.detach_/torch.data ">
<meta name="twitter:description" content="Reason is the light and the light of life.">

        <title>tensor.detach/torch.detach_/torch.data  · JERRYLSU
</title>



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="../../../"><span class=site-name>JERRYLSU</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       ../../..
                                    >Home</a>
                                </li>
                                <li ><a href="../../../pages/about.html">About</a></li>
                                <li ><a href="../../../categories.html">Categories</a></li>
                                <li ><a href="../../../tags.html">Tags</a></li>
                                <li ><a href="../../../archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="../../../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="../../../articles/tensor.detach/torch.detach_/torch.data.html">
                tensor.detach/torch.detach_/torch.data
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        show
        </nav>
    </div>
    <div class="span8 article-content">
            
            <p>https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
</code></pre></div>

<p>当我们再训练网络的时候可能希望保持一部分的网络参数不变，只对其中一部分的参数进行调整；或者值训练部分分支网络，并不让其梯度对主网络的梯度造成影响，这时候我们就需要使用detach()函数来切断一些分支的反向传播</p>
<h2 id="tensordetach">tensor.detach()</h2>
<p>返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。</p>
<p>即使之后重新将它的requires_grad置为true,它也不会具有梯度grad</p>
<p>这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的tensor就会停止，不能再继续向前进行传播</p>
<p>注意：</p>
<h2 id="detachtensortensor"><strong>使用detach返回的tensor和原始的tensor共同一个内存，即一个修改另一个也会跟着改变。</strong></h2>
<h3 id="1">1.&nbsp;正常</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># e.g.1</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">After backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Before backward():
a.grad: None

After backward():
a.grad: tensor([0.1779, 0.0313, 0.0037])
</code></pre></div>

<h3 id="2-detachtensorbackward">2.&nbsp;当使用detach()分离且不修改tensor时，不会影响backward()</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># e.g.2</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;out.grad: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 分离out tensor, c.requires_read变为False</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 并未修改分离出的tensor c，所以不影响backward()。</span>
<span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">After backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># 从上可见tensor  c是由out分离得到的，但是没有去改变这个c，这个时候依然对原来的out求导是不会有错误的，</span>
<span class="c1"># 即c,out之间的区别是c是没有梯度的，out是有梯度的,但是需要注意的是下面两种情况是会报错的.</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>Before backward():
a.grad: None
out.grad: None
c: tensor([0.7685, 0.9677, 0.9963])

After backward():
a.grad: tensor([0.1779, 0.0313, 0.0037])


&lt;ipython-input-28-2b96ae582a0b&gt;:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#39;t be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  print(f&quot;out.grad: {out.grad}&quot;)
</code></pre></div>

<h3 id="3-detachtensortensorbackward">3.&nbsp;当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，会影响backward()，会出现错误</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># e.g.3</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;out.grad: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 分离out tensor, c.requires_read变为False</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 使用分离出来的c，反向传播。</span>
<span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">Before</span><span class="w"> </span><span class="n">backward</span><span class="p">():</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>
<span class="n">out</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>
<span class="n">c</span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.7685</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9677</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9963</span><span class="p">])</span>


<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="mi">30</span><span class="o">-</span><span class="n">c3f4f3622562</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">6</span><span class="p">:</span><span class="w"> </span><span class="n">UserWarning</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="n">attribute</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">leaf</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">being</span><span class="w"> </span><span class="n">accessed</span><span class="o">.</span><span class="w"> </span><span class="n">Its</span><span class="w"> </span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="n">attribute</span><span class="w"> </span><span class="n">won</span><span class="s1">&#39;t be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.</span>
<span class="w">  </span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;out.grad: {out.grad}&quot;</span><span class="p">)</span>



<span class="o">---------------------------------------------------------------------------</span>

<span class="n">RuntimeError</span><span class="w">                              </span><span class="n">Traceback</span><span class="w"> </span><span class="p">(</span><span class="n">most</span><span class="w"> </span><span class="n">recent</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">last</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="mi">30</span><span class="o">-</span><span class="n">c3f4f3622562</span><span class="o">&gt;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="w">     </span><span class="mi">11</span><span class="w"> </span>
<span class="w">     </span><span class="mi">12</span><span class="w"> </span><span class="c1"># 使用分离出来的c，反向传播。</span>
<span class="o">---&gt;</span><span class="w"> </span><span class="mi">13</span><span class="w"> </span><span class="n">c</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="w">     </span><span class="mi">14</span><span class="w"> </span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;a.grad: {a.grad}&quot;</span><span class="p">)</span>


<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">conda</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">tensor</span><span class="o">.</span><span class="n">py</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span>
<span class="w">    </span><span class="mi">219</span><span class="w">                 </span><span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
<span class="w">    </span><span class="mi">220</span><span class="w">                 </span><span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
<span class="o">--&gt;</span><span class="w"> </span><span class="mi">221</span><span class="w">         </span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span>
<span class="w">    </span><span class="mi">222</span><span class="w"> </span>
<span class="w">    </span><span class="mi">223</span><span class="w">     </span><span class="n">def</span><span class="w"> </span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">hook</span><span class="p">):</span>


<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">conda</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">autograd</span><span class="o">/</span><span class="n">__init__</span><span class="o">.</span><span class="n">py</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">grad_tensors</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">,</span><span class="w"> </span><span class="n">grad_variables</span><span class="p">)</span>
<span class="w">    </span><span class="mi">128</span><span class="w">         </span><span class="n">retain_graph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">create_graph</span>
<span class="w">    </span><span class="mi">129</span><span class="w"> </span>
<span class="o">--&gt;</span><span class="w"> </span><span class="mi">130</span><span class="w">     </span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">run_backward</span><span class="p">(</span>
<span class="w">    </span><span class="mi">131</span><span class="w">         </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">grad_tensors_</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">,</span>
<span class="w">    </span><span class="mi">132</span><span class="w">         </span><span class="n">allow_unreachable</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="w">  </span><span class="c1"># allow_unreachable flag</span>


<span class="n">RuntimeError</span><span class="p">:</span><span class="w"> </span><span class="n">element</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tensors</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">require</span><span class="w"> </span><span class="n">grad</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">grad_fn</span>
</code></pre></div>

<h3 id="4-detachtensortensoroutbackward">4.&nbsp;当使用detach()分离tensor并且更改这个tensor时，即使再对原来的out求导数，会影响backward()，会出现错误</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># e.g.4</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Before backward():&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;a.grad: </span><span class="si">{</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;out.grad: </span><span class="si">{</span><span class="n">out</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 分离out tensor, c.requires_read变为False</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">c</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span> <span class="c1"># 修改c</span>

<span class="c1"># c和out均改变</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;c: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;out: </span><span class="si">{</span><span class="n">out</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1">#这时候对c进行更改，所以会影响backward()，这时候就不能进行backward()，会报错</span>
<span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">Before</span><span class="w"> </span><span class="n">backward</span><span class="p">():</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>
<span class="n">out</span><span class="o">.</span><span class="n">grad</span><span class="p">:</span><span class="w"> </span><span class="n">None</span>
<span class="n">c</span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.7685</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9677</span><span class="p">,</span><span class="w"> </span><span class="mf">0.9963</span><span class="p">])</span>
<span class="n">c</span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="p">])</span>
<span class="n">out</span><span class="p">:</span><span class="w"> </span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="p">,</span><span class="w"> </span><span class="mf">0.</span><span class="p">],</span><span class="w"> </span><span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SigmoidBackward</span><span class="o">&gt;</span><span class="p">)</span>


<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="mi">36</span><span class="o">-</span><span class="mi">08</span><span class="n">ba9844c2b4</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">6</span><span class="p">:</span><span class="w"> </span><span class="n">UserWarning</span><span class="p">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="n">attribute</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">leaf</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">being</span><span class="w"> </span><span class="n">accessed</span><span class="o">.</span><span class="w"> </span><span class="n">Its</span><span class="w"> </span><span class="o">.</span><span class="n">grad</span><span class="w"> </span><span class="n">attribute</span><span class="w"> </span><span class="n">won</span><span class="s1">&#39;t be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.</span>
<span class="w">  </span><span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;out.grad: {out.grad}&quot;</span><span class="p">)</span>



<span class="o">---------------------------------------------------------------------------</span>

<span class="n">RuntimeError</span><span class="w">                              </span><span class="n">Traceback</span><span class="w"> </span><span class="p">(</span><span class="n">most</span><span class="w"> </span><span class="n">recent</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">last</span><span class="p">)</span>

<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="n">input</span><span class="o">-</span><span class="mi">36</span><span class="o">-</span><span class="mi">08</span><span class="n">ba9844c2b4</span><span class="o">&gt;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="w">     </span><span class="mi">17</span><span class="w"> </span>
<span class="w">     </span><span class="mi">18</span><span class="w"> </span><span class="c1">#这时候对c进行更改，所以会影响backward()，这时候就不能进行backward()，会报错</span>
<span class="o">---&gt;</span><span class="w"> </span><span class="mi">19</span><span class="w"> </span><span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>


<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">conda</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">tensor</span><span class="o">.</span><span class="n">py</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span>
<span class="w">    </span><span class="mi">219</span><span class="w">                 </span><span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
<span class="w">    </span><span class="mi">220</span><span class="w">                 </span><span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
<span class="o">--&gt;</span><span class="w"> </span><span class="mi">221</span><span class="w">         </span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span>
<span class="w">    </span><span class="mi">222</span><span class="w"> </span>
<span class="w">    </span><span class="mi">223</span><span class="w">     </span><span class="n">def</span><span class="w"> </span><span class="n">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">hook</span><span class="p">):</span>


<span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">conda</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">blog</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">torch</span><span class="o">/</span><span class="n">autograd</span><span class="o">/</span><span class="n">__init__</span><span class="o">.</span><span class="n">py</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">backward</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">grad_tensors</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">,</span><span class="w"> </span><span class="n">grad_variables</span><span class="p">)</span>
<span class="w">    </span><span class="mi">128</span><span class="w">         </span><span class="n">retain_graph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">create_graph</span>
<span class="w">    </span><span class="mi">129</span><span class="w"> </span>
<span class="o">--&gt;</span><span class="w"> </span><span class="mi">130</span><span class="w">     </span><span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">run_backward</span><span class="p">(</span>
<span class="w">    </span><span class="mi">131</span><span class="w">         </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">grad_tensors_</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">,</span>
<span class="w">    </span><span class="mi">132</span><span class="w">         </span><span class="n">allow_unreachable</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="w">  </span><span class="c1"># allow_unreachable flag</span>


<span class="n">RuntimeError</span><span class="p">:</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">variables</span><span class="w"> </span><span class="n">needed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">computation</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">modified</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">inplace</span><span class="w"> </span><span class="n">operation</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="w"> </span><span class="p">[</span><span class="mi">3</span><span class="p">]],</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">SigmoidBackward</span><span class="p">,</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="n">expected</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">instead</span><span class="o">.</span><span class="w"> </span><span class="n">Hint</span><span class="p">:</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">anomaly</span><span class="w"> </span><span class="n">detection</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">operation</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">compute</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="n">True</span><span class="p">)</span><span class="o">.</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>
</code></pre></div>


             
 
                <p id="post-share-links">
    Share on:
      <a href="https://twitter.com/intent/tweet?text=tensor.detach/torch.detach_/torch.data&url=http%3A//www.jerrylsu.net/articles/tensor.detach/torch.detach_/torch.data.html&hashtags=deep-learning,pytorch" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 ❄       <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A//www.jerrylsu.net/articles/tensor.detach/torch.detach_/torch.data.html" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
 ❄       <a href="mailto:?subject=tensor.detach/torch.detach_/torch.data&amp;body=http%3A//www.jerrylsu.net/articles/tensor.detach/torch.detach_/torch.data.html" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>

            
            







            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="../../../articles/BertForQuestionAnswering.html" title="BertForQuestionAnswering">BertForQuestionAnswering</a></li>
<li><a href="../../../articles/torch.lt/torch.gt/torch.le/torch.ge/torch.ne.html" title="torch.lt/torch.gt/torch.le/torch.ge/torch.ne">torch.lt/torch.gt/torch.le/torch.ge/torch.ne</a></li>
<li><a href="../../../articles/Conditional-Random-Field.html" title="ConditionalRandomField">ConditionalRandomField</a></li>
<li><a href="../../../articles/scatter_nd.html" title="Scatter_nd">Scatter_nd</a></li>
<li><a href="../../../articles/paddle implements torch.repeat_interleave/K.repeat_elements using paddle.reshape &paddle.tile.html" title="paddle implements torch.repeat_interleave/K.repeat_elements using paddle.reshape &amp; paddle.tile">paddle implements torch.repeat_interleave/K.repeat_elements using paddle.reshape & paddle.tile</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="../../../articles/torch.lt/torch.gt/torch.le/torch.ge/torch.ne.html" title="Previous: torch.lt/torch.gt/torch.le/torch.ge/torch.ne">torch.lt/torch.gt/torch.le/torch.ge/torch.ne</a></li>
                <li class="next-article"><a href="../../../articles/Conditional-Random-Field.html" title="Next: ConditionalRandomField">ConditionalRandomField</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2021-01-18T12:17:17+08:00">Jan 18, 2021</time>
            <h4>Category</h4>
            <a class="category-link" href="../../../categories.html#programming-ref">Programming</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../../../tags.html#deep-learning-ref">Deep Learning
                    <span class="superscript">34</span>
</a></li>
                <li><a href="../../../tags.html#pytorch-ref">Pytorch
                    <span class="superscript">19</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="mailto:[email protected]" title="sa517301@mail.ustc.edu.cn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Mail" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#328cff"/><path d="m250 186c-46 0-69 35-69 74 0 44 29 72 68 72 43 0 73-32 73-75 0-44-34-71-72-71zm-1-37c30 0 57 13 77 33 0-22 35-22 35 1v150c-1 10 10 16 16 9 25-25 54-128-14-187-64-56-149-47-195-15-48 33-79 107-49 175 33 76 126 99 182 76 28-12 41 26 12 39-45 19-168 17-225-82-38-68-36-185 67-248 78-46 182-33 244 32 66 69 62 197-2 246-28 23-71 1-71-32v-11c-20 20-47 32-77 32-57 0-108-51-108-108 0-58 51-110 108-110" fill="#fff"/></svg>
    </a>
    <a href="https://github.com/jerrylsu" title="Jerry Github" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.youtube.com/@jerrysu780" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="YouTube" role="img" viewBox="0 0 512 512" fill="#ed1d24"><rect width="512" height="512" rx="15%"/><path d="m427 169c-4-15-17-27-32-31-34-9-239-10-278 0-15 4-28 16-32 31-9 38-10 135 0 174 4 15 17 27 32 31 36 10 241 10 278 0 15-4 28-16 32-31 9-36 9-137 0-174" fill="#fff"/><path d="m220 203v106l93-53"/></svg>
    </a>
    <a href="https://twitter.com/Jerrylsu666" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="http://www.jerrylsu.net/feeds/all.atom.xml" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="RSS" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#f80"/><circle cx="145" cy="367" r="35" fill="#fff"/><path fill="none" stroke="#fff" stroke-width="60" d="M109 241c89 0 162 73 162 162M109 127c152 0 276 124 276 276"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="../../../theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>