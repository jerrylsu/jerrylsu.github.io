<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Jerry Su" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="LLM, NLP, " />

<meta property="og:title" content="Contrastive Language-Image Pretraining "/>
<meta property="og:url" content="../articles/Contrastive-Language-Image-Pretraining.html" />
<meta property="og:description" content="CLIP: Contrastive Language-Image Pretraining 1. Data 1.1 image_processor # 1.The first processor, usually is the clip pretrained model (vit) # 224*224 # image_processor do not used in opt from transformers.models.clip.image_processing_clip import CLIPImageProcessor.preprocess image_processor = CLIPImageProcessor.from_pretrained(&#34;~/.cache/huggingface/hub/models-openai-clip-vit-large-patch14&#34;) processor = image_processor image = processor.preprocess(image, return_tensors …" />
<meta property="og:site_name" content="JERRYLSU" />
<meta property="og:article:author" content="Jerry Su" />
<meta property="og:article:published_time" content="2024-01-04T11:17:17+08:00" />
<meta name="twitter:title" content="Contrastive Language-Image Pretraining ">
<meta name="twitter:description" content="CLIP: Contrastive Language-Image Pretraining 1. Data 1.1 image_processor # 1.The first processor, usually is the clip pretrained model (vit) # 224*224 # image_processor do not used in opt from transformers.models.clip.image_processing_clip import CLIPImageProcessor.preprocess image_processor = CLIPImageProcessor.from_pretrained(&#34;~/.cache/huggingface/hub/models-openai-clip-vit-large-patch14&#34;) processor = image_processor image = processor.preprocess(image, return_tensors …">

        <title>Contrastive Language-Image Pretraining  · JERRYLSU
</title>
        <link rel="shortcut icon" href="../theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="../theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="../theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="../theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="../theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="../theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="../theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="../theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="../theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="../theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="../theme/images/apple-touch-icon-180x180.png" type="image/png" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="../"><span class=site-name>JERRYLSU</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       ..
                                    >Home</a>
                                </li>
                                <li ><a href="../categories.html">Categories</a></li>
                                <li ><a href="../tags.html">Tags</a></li>
                                <li ><a href="../archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="../articles/Contrastive-Language-Image-Pretraining.html">
                Contrastive Language-Image&nbsp;Pretraining
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        show
        </nav>
    </div>
    <div class="span8 article-content">
            
            <h3 id="clip-contrastive-language-image-pretraining"><span class="caps">CLIP</span>: Contrastive Language-Image&nbsp;Pretraining</h3>
<h2 id="1-data">1.&nbsp;Data</h2>
<h3 id="11-image_processor">1.1&nbsp;image_processor</h3>
<div class="highlight"><pre><span></span><code><span class="c1"># 1.The first processor, usually is the clip pretrained model (vit)</span>
<span class="c1"># 224*224</span>
<span class="c1"># image_processor do not used in opt</span>
<span class="kn">from</span> <span class="nn">transformers.models.clip.image_processing_clip</span> <span class="kn">import</span> <span class="n">CLIPImageProcessor</span><span class="o">.</span><span class="n">preprocess</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">CLIPImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;~/.cache/huggingface/hub/models-openai-clip-vit-large-patch14&quot;</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">image_processor</span>          
<span class="n">image</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)[</span><span class="s1">&#39;pixel_values&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># 2.The second processor_high, usually is the designed image encoder (sam/swin/cnn)</span>
<span class="c1"># 1024*1024</span>
<span class="kn">from</span> <span class="nn">vary.model.plug.transforms</span> <span class="kn">import</span> <span class="n">train_transform</span>
<span class="n">image_processor_high</span> <span class="o">=</span> <span class="n">train_transform</span>
<span class="n">processor_high</span> <span class="o">=</span> <span class="n">image_processor_high</span>
<span class="n">image_high</span> <span class="o">=</span> <span class="n">processor_high</span><span class="p">(</span><span class="n">image_high</span><span class="p">)</span>
</code></pre></div>

<h3 id="12-multimodal_processor">1.2&nbsp;multimodal_processor</h3>
<div class="highlight"><pre><span></span><code><span class="nt">&lt;image&gt;</span><span class="w"> </span>-&gt;<span class="w"> </span><span class="nt">&lt;img&gt;</span><span class="w"> </span>+<span class="w"> </span>[<span class="nt">&lt;imgpad&gt;</span>]<span class="w"> </span>*<span class="w"> </span>256<span class="w"> </span>+<span class="w"> </span><span class="nt">&lt;/img&gt;</span>
</code></pre></div>

<h3 id="13-token_processor">1.3&nbsp;token_processor</h3>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a list of sources, each is a conversation list. This transform:</span>
<span class="sd">    1. Add signal &#39;### &#39; at the beginning each sentence, with end signal &#39;\n&#39;;</span>
<span class="sd">    2. Concatenate conversations together;</span>
<span class="sd">    3. Tokenize the concatenated conversation;</span>
<span class="sd">    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">DEFAULT_IM_START_TOKEN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;</span><span class="n">img</span><span class="o">&gt;</span>
<span class="w">    </span><span class="n">DEFAULT_IM_END_TOKEN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&lt;/</span><span class="n">img</span><span class="o">&gt;</span>

<span class="w">    </span><span class="n">conversation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;human&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;&lt;img&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;imgpad&gt;&lt;/img&gt;&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;gpt&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;The man is sitting on top of piled objects or belongings loaded into the back of a pickup truck.&#39;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;human&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Is the man holding anything in his hands?&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;gpt&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Yes, the man is holding a beer in his hand while sitting on top of the objects in the back of the pickup truck.&#39;</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;human&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;What color is the pickup truck?&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;gpt&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;The pickup truck is white.&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;human&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;Is the man sitting or standing?&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;gpt&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;The man is sitting on top of the piled objects in the back of the pickup truck.&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;human&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;What could be the possible reasons for the man sitting on top of the possessions in the back of the pickup truck?&#39;</span>
<span class="w">        </span><span class="p">},</span><span class="w"> </span>
<span class="w">        </span><span class="p">{</span>
<span class="w">            </span><span class="s1">&#39;from&#39;</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;gpt&#39;</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="s1">&#39;value&#39;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;There could be several reasons for the man sitting on top of his possessions in the back of the pickup truck:</span><span class="se">\n\n</span><span class="s2">1. Moving: The man might be moving to a new location and needed to transport his items in a pickup truck, utilizing available space efficiently. By sitting on top of the belongings, he could be helping to stabilize and secure the items during the move, preventing them from falling or shifting during transportation.</span><span class="se">\n\n</span><span class="s2">2. Lack of seating: If the cab of the pickup truck is already at full capacity or there isn&#39;t enough space for him to sit inside, the man may have chosen to sit on his possessions as an alternative seating arrangement.</span><span class="se">\n\n</span><span class="s2">3. Road trip or outing: The man might be on a road trip or a casual outing with friends or family, where he is using the back of the pickup truck as an open-air seating area. By sitting on top of the loaded items, he may be enjoying the journey while savoring his beer.</span><span class="se">\n\n</span><span class="s2">4. Keeping an eye on belongings: The man could be safeguarding his possessions by staying close to them, ensuring that no items are lost, stolen or damaged during the journey.</span><span class="se">\n\n</span><span class="s2">Regardless of the specific reason, the image shows a person making the most of their situation, adding a touch of lightheartedness or adventure to an otherwise mundane scene.&quot;</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">]</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">conversation</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;&lt;/s&gt;&quot;</span>    <span class="c1"># # pseudo code</span>

<span class="n">tokenized_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">text</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;longest&quot;</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">strings</span>
<span class="p">]</span>

<span class="c1"># input_ids = labels </span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">tokenized</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">tokenized</span> <span class="ow">in</span> <span class="n">tokenized_list</span>
<span class="p">]</span>

<span class="c1"># labels.repalce(&quot;...&quot;, -100)</span>
<span class="n">IGNORE_INDEX</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
<span class="n">lables</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;imgpad&gt;&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># pseudo code</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>
</code></pre></div>

<h2 id="2model">2.Model</h2>
<h3 id="varyoptmodel-optmodel-optdecoder-optdecoderlayer">varyOPTModel -&gt; OPTModel -&gt; OPTDecoder -&gt;&nbsp;OPTDecoderLayer</h3>
<h3 id="21-varyoptmodel">2.1&nbsp;varyOPTModel</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">varyOPTModel</span><span class="p">(</span><span class="n">OPTModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">OPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">varyOPTModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vision_tower</span> <span class="o">=</span> <span class="n">build_sam_vit_b</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mm_projector</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
</code></pre></div>

<h3 id="22-optmodel">2.2&nbsp;OPTModel</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">OPTModel</span><span class="p">(</span><span class="n">OPTPreTrainedModel</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">OPTConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">OPTDecoder</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</code></pre></div>

<h3 id="23-optdecoder">2.3&nbsp;OPTDecoder</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">OPTDecoder</span><span class="p">(</span><span class="n">OPTPreTrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OPTDecoderLayer`]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">word_embed_proj_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_positions</span> <span class="o">=</span> <span class="n">OPTLearnedPositionalEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">OPTDecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)])</span>
</code></pre></div>

<h3 id="24-optdecoderlayer">2.4&nbsp;OPTDecoderLayer</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">OPTDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">OPTAttention</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">is_decoder</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">enable_bias</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">activation_function</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">self_attn_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_elementwise_affine</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">ffn_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">enable_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">ffn_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">enable_bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">final_layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">layer_norm_elementwise_affine</span><span class="p">)</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># vision tower build as below!!!</span>
<span class="bp">self</span><span class="o">.</span><span class="n">vision_tower</span> <span class="o">=</span> <span class="n">build_sam_vit_b</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">build_sam_vit_b</span><span class="p">(</span><span class="n">checkpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">_build_sam</span><span class="p">(</span>
        <span class="n">encoder_embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
        <span class="n">encoder_depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">encoder_num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
        <span class="n">encoder_global_attn_indexes</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_build_sam</span><span class="p">(</span>
    <span class="n">encoder_embed_dim</span><span class="p">,</span>
    <span class="n">encoder_depth</span><span class="p">,</span>
    <span class="n">encoder_num_heads</span><span class="p">,</span>
    <span class="n">encoder_global_attn_indexes</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">prompt_embed_dim</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">vit_patch_size</span> <span class="o">=</span> <span class="mi">16</span>

    <span class="c1"># This class of ImageEncoderViT and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa</span>

    <span class="n">image_encoder</span><span class="o">=</span><span class="n">ImageEncoderViT</span><span class="p">(</span>
            <span class="n">depth</span><span class="o">=</span><span class="n">encoder_depth</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">encoder_embed_dim</span><span class="p">,</span>
            <span class="n">img_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
            <span class="n">mlp_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">norm_layer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">encoder_num_heads</span><span class="p">,</span>
            <span class="n">patch_size</span><span class="o">=</span><span class="n">vit_patch_size</span><span class="p">,</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_rel_pos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">global_attn_indexes</span><span class="o">=</span><span class="n">encoder_global_attn_indexes</span><span class="p">,</span>
            <span class="n">window_size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
            <span class="n">out_chans</span><span class="o">=</span><span class="n">prompt_embed_dim</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">image_encoder</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="c1"># image embed + text embed</span>
<span class="n">input_embeds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                        <span class="p">(</span>
                            <span class="n">input_embeds</span><span class="p">[:</span><span class="n">image_start_token_pos</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> 
                            <span class="n">image_features</span><span class="p">,</span>    <span class="n">num_patches</span><span class="p">:</span> <span class="mi">256</span>
                            <span class="n">input_embeds</span><span class="p">[</span><span class="n">image_start_token_pos</span> <span class="o">+</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
                        <span class="p">),</span> 
                        <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
                    <span class="p">)</span>
</code></pre></div>

<p><strong>Vision Vocabulary</strong>：CLIP的视觉字典是指该模型通过对比学习从大规模图像和文本数据中学到的关于图像的表示和语义信息的集合。<br>&nbsp;在训练过程中，CLIP学会了将图像嵌入（embed）到一个高维空间中，并在该空间中通过文本描述对图像进行分类或检索。这个视觉字典的结构是通过模型的架构和训练数据来定义的。</p>


             
 
                <p id="post-share-links">
    Share on:
      <a href="https://twitter.com/intent/tweet?text=Contrastive%20Language-Image%C2%A0Pretraining&url=http%3A//www.jerrylsu.net/articles/Contrastive-Language-Image-Pretraining.html&hashtags=llm" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 ❄       <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A//www.jerrylsu.net/articles/Contrastive-Language-Image-Pretraining.html" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
 ❄       <a href="mailto:?subject=Contrastive%20Language-Image%C2%A0Pretraining&amp;body=http%3A//www.jerrylsu.net/articles/Contrastive-Language-Image-Pretraining.html" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>

            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   href="../articles/Contrastive-Language-Image-Pretraining.html#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">



                        <div class="commentbox" id="../articles/Contrastive-Language-Image-Pretraining.html"></div>
<script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
<script>
    commentBox("True", {
        onCommentCount(count) {
            const ele = document.querySelector("#comment-accordion-toggle")
            if (ele && count > 0) {
                ele.innerText = `${count} Comment${count > 1 ? 's' : ''}`
            }
        }
    });
</script>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="../articles/Self-Instruct.html" title="SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions</a></li>
<li><a href="../articles/Nucleus Sampling Top-p Sampling.html" title="Nucleus Sampling Top-p Sampling">Nucleus Sampling Top-p Sampling</a></li>
<li><a href="../articles/Universal-Chart-Structural-Multimodal-Generation-and-Extraction.html" title="Universal Chart Structural Multimodal Generation and Extraction">Universal Chart Structural Multimodal Generation and Extraction</a></li>
<li><a href="../articles/MinHash-Document-level-Deduplication.html" title="MinHash: Document-level Deduplication">MinHash: Document-level Deduplication</a></li>
<li><a href="../articles/GGUF-Model.html" title="GGUF Model">GGUF Model</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="../articles/Concurrent-http-requests-using-asyncio-and-aiohttp.html" title="Previous: Concurrent http requests using asyncio and aiohttp">Concurrent http requests using asyncio and aiohttp</a></li>
                <li class="next-article"><a href="../articles/Nucleus Sampling Top-p Sampling.html" title="Next: Nucleus Sampling Top-p Sampling">Nucleus Sampling Top-p Sampling</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2024-01-04T11:17:17+08:00">Jan 04, 2024</time>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#nlp-ref">NLP</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#llm-ref">LLM
                    <span class="superscript">7</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="mailto:[email protected]" title="sa517301@mail.ustc.edu.cn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Mail" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#328cff"/><path d="m250 186c-46 0-69 35-69 74 0 44 29 72 68 72 43 0 73-32 73-75 0-44-34-71-72-71zm-1-37c30 0 57 13 77 33 0-22 35-22 35 1v150c-1 10 10 16 16 9 25-25 54-128-14-187-64-56-149-47-195-15-48 33-79 107-49 175 33 76 126 99 182 76 28-12 41 26 12 39-45 19-168 17-225-82-38-68-36-185 67-248 78-46 182-33 244 32 66 69 62 197-2 246-28 23-71 1-71-32v-11c-20 20-47 32-77 32-57 0-108-51-108-108 0-58 51-110 108-110" fill="#fff"/></svg>
    </a>
    <a href="https://github.com/jerrylsu" title="Jerry Github" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.youtube.com/@jerrysu780" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="YouTube" role="img" viewBox="0 0 512 512" fill="#ed1d24"><rect width="512" height="512" rx="15%"/><path d="m427 169c-4-15-17-27-32-31-34-9-239-10-278 0-15 4-28 16-32 31-9 38-10 135 0 174 4 15 17 27 32 31 36 10 241 10 278 0 15-4 28-16 32-31 9-36 9-137 0-174" fill="#fff"/><path d="m220 203v106l93-53"/></svg>
    </a>
    <a href="https://twitter.com/Jerrylsu666" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="http://www.jerrylsu.net/feeds/all.atom.xml" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="RSS" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#f80"/><circle cx="145" cy="367" r="35" fill="#fff"/><path fill="none" stroke="#fff" stroke-width="60" d="M109 241c89 0 162 73 162 162M109 127c152 0 276 124 276 276"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="../theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

        <!-- 在这里添加 CommentBox 评论框代码 -->
        <div class="commentbox"></div>
        <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script> 
        <script>commentBox('5717141856714752-proj')</script>
    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>