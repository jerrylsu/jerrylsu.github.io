<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="../theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Jerry Su" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="NLP, Projects, " />

<meta property="og:title" content="Global Pointer: Multi-Head Attention without Value Operation "/>
<meta property="og:url" content="../articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html" />
<meta property="og:description" content="Background 单个字符的子序列：对于长度为 \(n\) 的序列，有 \(n\) 个可能的单个字符的子序列。 长度为\(k\)的子序列：对于任意 \(1 &lt; k \leq n\)，序列 …if (!document.getElementById(&#39;mathjaxscript_pelican_#%@#$@#&#39;)) { var align = &#34;center&#34;, indent = &#34;0em&#34;, linebreak = &#34;false&#34;; if (false) { align = (screen.width" />
<meta property="og:site_name" content="JERRYLSU" />
<meta property="og:article:author" content="Jerry Su" />
<meta property="og:article:published_time" content="2022-08-22T11:17:17+08:00" />
<meta name="twitter:title" content="Global Pointer: Multi-Head Attention without Value Operation ">
<meta name="twitter:description" content="Background 单个字符的子序列：对于长度为 \(n\) 的序列，有 \(n\) 个可能的单个字符的子序列。 长度为\(k\)的子序列：对于任意 \(1 &lt; k \leq n\)，序列 …if (!document.getElementById(&#39;mathjaxscript_pelican_#%@#$@#&#39;)) { var align = &#34;center&#34;, indent = &#34;0em&#34;, linebreak = &#34;false&#34;; if (false) { align = (screen.width">

        <title>Global Pointer: Multi-Head Attention without Value Operation  · JERRYLSU
</title>



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="../"><span class=site-name>JERRYLSU</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       ..
                                    >Home</a>
                                </li>
                                <li ><a href="../categories.html">Categories</a></li>
                                <li ><a href="../tags.html">Tags</a></li>
                                <li ><a href="../archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="../articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html">
                Global Pointer: Multi-Head Attention without Value&nbsp;Operation
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
    <div class="span2 table-of-content">
        <nav>
        <h4>Contents</h4>
        <div class="toc">
<ul>
<li><a href="#background">Background</a></li>
<li><a href="#problem-description">Problem Description</a></li>
<li><a href="#mathematical-form">Mathematical Form</a></li>
<li><a href="#global-pointer">Global Pointer</a></li>
<li><a href="#loss">Loss</a></li>
<li><a href="#gp4paddle">GP4Paddle</a></li>
</ul>
</div>
        </nav>
    </div>
    <div class="span8 article-content">
            
            
<h2 id="background">Background</h2>
<ol>
<li>
<p><strong>单个字符的子序列</strong>：对于长度为 <span class="math">\(n\)</span> 的序列，有 <span class="math">\(n\)</span> 个可能的单个字符的子序列。</p>
</li>
<li>
<p><strong>长度为<span class="math">\(k\)</span>的子序列</strong>：对于任意 <span class="math">\(1 &lt; k \leq n\)</span>，序列中可以有 <span class="math">\(n - k + 1\)</span> 个长度为 <span class="math">\(k\)</span> 的子序列，因为可以从序列的起始位置开始取长度为 <span class="math">\(k\)</span> 的子序列，也可以从序列的第二个字符开始取，依此类推，直到从序列的第 <span class="math">\(n-k+1\)</span> 个字符开始。</p>
</li>
<li>
<p><strong>所有可能的子序列总数</strong>：要得到序列中所有可能的非空子序列的总数，我们需要对所有可能的子序列长度求和，即从长度为 1 加到长度为 <span class="math">\(n\)</span> ：</p>
</li>
</ol>
<div class="math">$$\text{子序列总数} = \sum_{k=1}^{n} (n - k + 1) = n + (n-1) + (n-2) + \ldots + 1=  \frac{n(n + 1)}{2}$$</div>
<h2 id="problem-description">Problem Description</h2>
<p>从 <span class="math">\(\frac{n(n + 1)}{2}\)</span> 候选实体中选取真正实体。如果有 <span class="math">\(m\)</span> 种实体类型，即转换为 <span class="math">\(m\)</span> 个 <span class="math">\(\frac{n(n + 1)}{2}\)</span> 选 <span class="math">\(k\)</span> 的多标签分类问题。</p>
<h2 id="mathematical-form">Mathematical Form</h2>
<p><strong>Multi-Head Attention without Value Operation</strong></p>
<p>设长度为 <span class="math">\(n\)</span> 的输入 <span class="math">\(t\)</span> 经过编码后得到向量序列 <span class="math">\([h_1, h_2, \ldots, h_n]\)</span>，即<strong>Token Representation</strong>，通过变换</p>
<div class="math">$$q_{i, \alpha} = W_{q, \alpha} h_i + b_{g, \alpha}$$</div>
<p><br/>
</p>
<div class="math">$$k_{i, \alpha} = W_{k, \alpha} h_i + b_{k, \alpha}$$</div>
<p>我们可以得到序列向量序列 <span class="math">\([q_{1, \alpha}, q_{2, \alpha}, \ldots, q_{n, \alpha}]\)</span> 和 <span class="math">\([k_{1, \alpha}, k_{2, \alpha}, \ldots, k_{n, \alpha}]\)</span>，它们是识别第 <span class="math">\(\alpha\)</span> 种类型实体所用的向量序列。定义<strong>Span Prediction</strong></p>
<div class="math">$$S_{\alpha}(i, j) = q_{i, \alpha} \cdot k_{j, \alpha}$$</div>
<p>是类型为 <span class="math">\(\alpha\)</span> 的实体的打分（logits），这里的 <span class="math">\(t[i:j]\)</span> 指的是序列 <span class="math">\(t\)</span> 的第 <span class="math">\(i\)</span> 个到第 <span class="math">\(j\)</span> 个元素组成的连续子串。</p>
<p>GlobalPointer 就是简化版 Multi-Head Attention，有多少种实体就对应多少个头，相比 Multi-Head Attention 去掉了 <span class="math">\(Value\)</span> 相关的运算。</p>
<h2 id="global-pointer">Global Pointer</h2>
<p><strong>Multi-Head Attention without Value Operation</strong></p>
<p><img alt="GP" src="../images/GlobalPointer/GP.jpg"/></p>
<h2 id="loss">Loss</h2>
<h2 id="gp4paddle">GP4Paddle</h2>
<div class="highlight"><pre><span></span><code><span class="c1"># !/usr/bin/env python3</span>
<span class="c1"># -*- coding: UTF-8 -*-</span>
<span class="c1">#</span>
<span class="c1">################################################################################</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2022 All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1">################################################################################</span>
<span class="sd">"""Global Pointer.</span>
<span class="sd">"""</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">paddle</span>
<span class="kn">import</span> <span class="nn">paddle.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">paddlenlp.transformers</span> <span class="kn">import</span> <span class="n">ErniePretrainedModel</span>


<span class="k">class</span> <span class="nc">RotaryPositionEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Sinusoidal position embedding.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">10000</span> <span class="o">**</span> <span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">inv_freq</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"sin"</span><span class="p">,</span> <span class="n">freqs</span><span class="o">.</span><span class="n">sin</span><span class="p">(),</span> <span class="n">persistable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"cos"</span><span class="p">,</span> <span class="n">freqs</span><span class="o">.</span><span class="n">cos</span><span class="p">(),</span> <span class="n">persistable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""The RPE forward method, overrides the `__call__()` special method.</span>
<span class="sd">        """</span>
        <span class="n">seqlen</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">sin</span><span class="p">,</span> <span class="n">cos</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sin</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">seqlen</span><span class="p">,</span> <span class="p">:],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cos</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">seqlen</span><span class="p">,</span> <span class="p">:],</span>
        <span class="p">)</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">paddle</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x1</span> <span class="o">*</span> <span class="n">cos</span> <span class="o">-</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">sin</span><span class="p">,</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">sin</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">*</span> <span class="n">cos</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GlobalPointer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Global Pointer Header.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">heads</span><span class="p">,</span>
                 <span class="n">head_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">RoPE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">tril_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="n">head_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">RoPE</span> <span class="o">=</span> <span class="n">RoPE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tril_mask</span> <span class="o">=</span> <span class="n">tril_mask</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">head_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">heads</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">RoPE</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span> <span class="o">=</span> <span class="n">RotaryPositionEmbedding</span><span class="p">(</span><span class="n">head_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        The GPModel forward method, overrides the `__call__()` special method.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (Tensor):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. They are</span>
<span class="sd">                numerical representations of tokens that build the input sequence</span>
<span class="sd">            attention_mask (Tensor, optional):</span>
<span class="sd">                Mask used in multi-head attention to avoid performing attention to some unwanted positions,</span>
<span class="sd">                usually the paddings or the subsequent positions.</span>
<span class="sd">        """</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">qw</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
        <span class="c1"># RoPE编码</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">RoPE</span><span class="p">:</span>
            <span class="n">qw</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span><span class="p">(</span><span class="n">qw</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span><span class="p">(</span><span class="n">kw</span><span class="p">)</span>
        <span class="c1"># 计算内积</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"bmd,bnd-&gt;bmn"</span><span class="p">,</span> <span class="n">qw</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[:,</span> <span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">bias</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="c1"># 排除padding</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">-</span>
            <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">attn_mask</span> <span class="o">*</span> <span class="mf">1e12</span>
        <span class="c1"># 排除下三角</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tril_mask</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">paddle</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">paddle</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">mask</span> <span class="o">*</span> <span class="mf">1e12</span>
        <span class="k">return</span> <span class="n">logits</span>


<span class="k">class</span> <span class="nc">ErnieForGlobalPointer</span><span class="p">(</span><span class="n">ErniePretrainedModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    ERNIE Model with a global pointer header on top of the hidden-states output layer,</span>
<span class="sd">    designed for NER tasks.</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder (`ErnieModel`):</span>
<span class="sd">            An instance of `ErnieModel`.</span>
<span class="sd">        entity_size_num (int):</span>
<span class="sd">            The number of entity type.</span>
<span class="sd">        max_length (int):</span>
<span class="sd">            Max length.</span>
<span class="sd">        head_size (int):</span>
<span class="sd">            Head size.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">entity_size_num</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">head_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ErnieForGlobalPointer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"hidden_size"</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">entity_output</span> <span class="o">=</span> <span class="n">GlobalPointer</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                           <span class="n">entity_size_num</span><span class="p">,</span>
                                           <span class="n">head_size</span><span class="o">=</span><span class="n">head_size</span><span class="p">,</span>
                                           <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        The GPModel forward method, overrides the `__call__()` special method.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_ids (Tensor):</span>
<span class="sd">                Indices of input sequence tokens in the vocabulary. They are</span>
<span class="sd">                numerical representations of tokens that build the input sequence</span>
<span class="sd">            attention_mask (Tensor, optional):</span>
<span class="sd">                Mask used in multi-head attention to avoid performing attention to some unwanted positions,</span>
<span class="sd">                usually the paddings or the subsequent positions.</span>
<span class="sd">        """</span>
        <span class="c1"># input_ids, attention_mask, token_type_ids: (batch_size, seq_len)</span>
        <span class="n">context_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="c1"># last_hidden_state: (batch_size, seq_len, hidden_size)</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">context_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">entity_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">entity_output</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">entity_output</span>
</code></pre></div>
<p>Refs.</p>
<p><a href="https://arxiv.org/pdf/2208.03054">Jianlin Su, Ahmed Murtadha, et al. Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
                <p id="post-share-links">
    Share on:
      <a href="https://twitter.com/intent/tweet?text=Global%20Pointer%3A%20Multi-Head%20Attention%20without%20Value%C2%A0Operation&url=http%3A//www.jerrylsu.net/articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html&hashtags=nlp" target="_blank" rel="nofollow noopener noreferrer" title="Share on Twitter">Twitter</a>
 ❄       <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A//www.jerrylsu.net/articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html" target="_blank" rel="nofollow noopener noreferrer" title="Share on Facebook">Facebook</a>
 ❄       <a href="mailto:?subject=Global%20Pointer%3A%20Multi-Head%20Attention%20without%20Value%C2%A0Operation&amp;body=http%3A//www.jerrylsu.net/articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html" target="_blank" rel="nofollow noopener noreferrer" title="Share via Email">Email</a>

            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   href="../articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">



                        <div class="commentbox" id="../articles/Global-Pointer-Multi-Head-Attention-without-Value-Operation.html"></div>
<script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
<script>
    commentBox("True", {
        onCommentCount(count) {
            const ele = document.querySelector("#comment-accordion-toggle")
            if (ele && count > 0) {
                ele.innerText = `${count} Comment${count > 1 ? 's' : ''}`
            }
        }
    });
</script>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
<section>
    <h2>Related Posts</h2>
<ul class="related-posts-list">
<li><a href="../articles/Creating-folds-properly.html" title="Creating folds properly">Creating folds properly</a></li>
<li><a href="../articles/Ernie4Paddle.html" title="Ernie4paddle">Ernie4paddle</a></li>
<li><a href="../articles/Self-Instruct.html" title="SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions">SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions</a></li>
<li><a href="../articles/DocTuning.html" title="DocTuning">DocTuning</a></li>
<li><a href="../articles/Universal-Chart-Structural-Multimodal-Generation-and-Extraction.html" title="Universal Chart Structural Multimodal Generation and Extraction">Universal Chart Structural Multimodal Generation and Extraction</a></li>
</ul>
<hr />
</section>
            <aside>
            <nav>
            <ul class="articles-timeline">
                <li class="previous-article">« <a href="../articles/Multiprocessing-in-FastAPI.html" title="Previous: Multiprocessing in FastAPI">Multiprocessing in FastAPI</a></li>
                <li class="next-article"><a href="../articles/Ernie4Paddle.html" title="Next: Ernie4paddle">Ernie4paddle</a> »</li>
            </ul>
            </nav>
            </aside>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2022-08-22T11:17:17+08:00">Aug 22, 2022</time>
            <h4>Category</h4>
            <a class="category-link" href="../categories.html#projects-ref">Projects</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../tags.html#nlp-ref">NLP
                    <span class="superscript">32</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="mailto:[email protected]" title="sa517301@mail.ustc.edu.cn" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Mail" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#328cff"/><path d="m250 186c-46 0-69 35-69 74 0 44 29 72 68 72 43 0 73-32 73-75 0-44-34-71-72-71zm-1-37c30 0 57 13 77 33 0-22 35-22 35 1v150c-1 10 10 16 16 9 25-25 54-128-14-187-64-56-149-47-195-15-48 33-79 107-49 175 33 76 126 99 182 76 28-12 41 26 12 39-45 19-168 17-225-82-38-68-36-185 67-248 78-46 182-33 244 32 66 69 62 197-2 246-28 23-71 1-71-32v-11c-20 20-47 32-77 32-57 0-108-51-108-108 0-58 51-110 108-110" fill="#fff"/></svg>
    </a>
    <a href="https://github.com/jerrylsu" title="Jerry Github" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
    <a href="https://www.youtube.com/@jerrysu780" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="YouTube" role="img" viewBox="0 0 512 512" fill="#ed1d24"><rect width="512" height="512" rx="15%"/><path d="m427 169c-4-15-17-27-32-31-34-9-239-10-278 0-15 4-28 16-32 31-9 38-10 135 0 174 4 15 17 27 32 31 36 10 241 10 278 0 15-4 28-16 32-31 9-36 9-137 0-174" fill="#fff"/><path d="m220 203v106l93-53"/></svg>
    </a>
    <a href="https://twitter.com/Jerrylsu666" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="Twitter" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1da1f3"/><path fill="#fff" d="M437 152a72 72 0 0 1-40 12 72 72 0 0 0 32-40 72 72 0 0 1-45 17 72 72 0 0 0-122 65 200 200 0 0 1-145-74 72 72 0 0 0 22 94 72 72 0 0 1-32-7 72 72 0 0 0 56 69 72 72 0 0 1-32 1 72 72 0 0 0 67 50 200 200 0 0 1-105 29 200 200 0 0 0 309-179 200 200 0 0 0 35-37"/></svg>
    </a>
    <a href="http://www.jerrylsu.net/feeds/all.atom.xml" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="RSS" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#f80"/><circle cx="145" cy="367" r="35" fill="#fff"/><path fill="none" stroke="#fff" stroke-width="60" d="M109 241c89 0 162 73 162 162M109 127c152 0 276 124 276 276"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="../theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>